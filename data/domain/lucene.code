0	public string GeneratePreviewText(Query q, string text)\n{\n    QueryScorer scorer = new QueryScorer(q);\n    Formatter formatter = new SimpleHTMLFormatter(highlightStartTag, highlightEndTag);\n    Highlighter highlighter = new Highlighter(formatter, scorer);\n    highlighter.SetTextFragmenter(new SimpleFragmenter(fragmentLength));\n    TokenStream stream = new StandardAnalyzer().TokenStream(new StringReader(text));\n    return highlighter.GetBestFragments(stream, text, fragmentCount, fragmentSeparator);\n}\n
1	const int DocumentCount = 10 * 1000;\nconst string IndexFilePath = @"X:\Temp\tmp.idx";\n\nAnalyzer analyzer = new StandardAnalyzer();\nDirectory ramDirectory = new RAMDirectory();\n\nIndexWriter indexWriter = new IndexWriter(ramDirectory, analyzer, true);\n\nfor (int i = 0; i &lt; DocumentCount; i++)\n{\n    Document doc = new Document();\n    string text = "Value" + i;\n    doc.Add(new Field("Value", text, Field.Store.YES, Field.Index.TOKENIZED));\n    indexWriter.AddDocument(doc);\n}\n\nindexWriter.Close();\n\n//Save index\nFSDirectory fileDirectory = FSDirectory.GetDirectory(IndexFilePath, true);\nIndexWriter fileIndexWriter = new IndexWriter(fileDirectory, analyzer, true);\nfileIndexWriter.AddIndexes(new[] { ramDirectory });\nfileIndexWriter.Close();\n\n//Load index\nFSDirectory newFileDirectory = FSDirectory.GetDirectory(IndexFilePath, false);\nDirectory newRamDirectory = new RAMDirectory();\nIndexWriter newIndexWriter = new IndexWriter(newRamDirectory, analyzer, true);\nnewIndexWriter.AddIndexes(new[] { newFileDirectory });\n\nConsole.WriteLine("New index writer document count:{0}.", newIndexWriter.DocCount());\n
2	try {\n    IndexWriter indexWriter = writer.get();\n    int c = 0;\n    indexWriter.deleteUnusedFiles();\n    while (hasUnusedFiles(indexDir, commit)) {\n        indexWriter.deleteUnusedFiles();\n        LOG.info("Sleeping for 1000ms to wait for unused lucene index files to be delete-able");\n        Thread.sleep(1000);\n        c++;\n        if (c &gt;= 30) {\n            LOG.warn("IndexFetcher unable to cleanup unused lucene index files so we must do a full copy instead");\n            isFullCopyNeeded = true;\n            break;\n        }\n    }\n    if (c &gt; 0)  {\n        LOG.info("IndexFetcher slept for " + (c * 1000) + "ms for unused lucene index files to be delete-able");\n    }\n} finally {\n    writer.decref();\n}\n
3	List docIds = // doc ids for documents that matched the query, \n              // sorted in ascending order \n\nint totalFreq = 0;\nTermDocs termDocs = reader.termDocs();\ntermDocs.seek(new Term("my_field", "congress"));\nfor (int id : docIds) {\n    termDocs.skipTo(id);\n    totalFreq += termDocs.freq();\n}\n
4	private static final FieldType DOUBLE_FIELD_TYPE_STORED_SORTED = new FieldType();\nstatic {\n    DOUBLE_FIELD_TYPE_STORED_SORTED.setTokenized(true);\n    DOUBLE_FIELD_TYPE_STORED_SORTED.setOmitNorms(true);\n    DOUBLE_FIELD_TYPE_STORED_SORTED.setIndexOptions(IndexOptions.DOCS);\n    DOUBLE_FIELD_TYPE_STORED_SORTED\n        .setNumericType(FieldType.NumericType.DOUBLE);\n    DOUBLE_FIELD_TYPE_STORED_SORTED.setStored(true);\n    DOUBLE_FIELD_TYPE_STORED_SORTED.setDocValuesType(DocValuesType.NUMERIC);\n    DOUBLE_FIELD_TYPE_STORED_SORTED.freeze();\n}\n
5	private void createDocument() throws FileNotFoundException {\n    File sjp = new File(dictionaryPath);\n    BufferedReader reader = new BufferedReader(new FileReader(sjp));\n\n    String readLine = null;\n    while((readLine = reader.readLine() != null)) {\n        readLine = readLine.trim();\n        Document dictionary = new Document();\n        dictionary.add(new Field("word", readLine));\n        // toAnagram methods sorts the letters in the word. Also makes it\n        // case insensitive.\n        dictionary.add(new Field("anagram", toAnagram(readLine)));\n        indexWriter.addDocument(dictionary);\n    }\n}\n
6	Sort sort = new Sort(new SortField("title", SortField.Type.STRING));\nTopDocs docs = searcher.search(new TermQuery(new Term("title", "something")), 10, sort);\n
7	q={!boost b=recip(geodist(),1,1000,1000)}foo:bar&amp;...\n
8	 Query query = MultiFieldQueryParser.parse("development",\n        new String[]{"title", "subject"},\n        new SimpleAnalyzer());\n
9	SolrQuery query = new SolrQuery(searchTerm);\nquery.setStart((pageNum - 1) * numItemsPerPage);\nquery.setRows(numItemsPerPage);\n// execute the query on the server and get results\nQueryResponse res = solrServer.query(solrQuery);\n
10	directory = new RAMDirectory();\n            analyzer = new StandardAnalyzer(version, new Hashtable());\n            var indexWriter = new IndexWriter(directory, analyzer, true, IndexWriter.MaxFieldLength.UNLIMITED);\n            using (var session = sessionFactory.OpenStatelessSession())\n            {\n                organizations = session.CreateCriteria(typeof(Organization)).List&lt;Organization&gt;();\n                foreach (var organization in organizations)\n                {\n                    var document = new Document();\n                    document.Add(new Field("Id", organization.ID.ToString(), Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS));\n                    document.Add(new Field("FullName", organization.FullName, Field.Store.NO, Field.Index.ANALYZED_NO_NORMS));\n                    document.Add(new Field("ObjectTypeInvariantName", typeof(Organization).FullName, Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS));\n                    indexWriter.AddDocument(document);\n                }\n\n                var persistentType = typeof(Order);\n                var classMetadata = DbContext.SessionFactory.GetClassMetadata(persistentType);\n\n\n                var properties = new List&lt;PropertyInfo&gt;();\n                for (int i = 0; i &lt; classMetadata.PropertyTypes.Length; i++)\n                {\n                    var propertyType = classMetadata.PropertyTypes[i];\n                    if (propertyType.IsCollectionType || propertyType.IsEntityType) continue;\n                    properties.Add(typeof(Order).GetProperty(classMetadata.PropertyNames[i]));\n                }\n\n                orders = session.CreateCriteria(typeof(Order)).List&lt;Order&gt;();\n                var idProperty = typeof(Order).GetProperty(classMetadata.IdentifierPropertyName);\n\n                foreach (var order in orders)\n                {\n                    var document = new Document();\n                    document.Add(new Field("Id", idProperty.GetValue(order, null).ToString(), Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS));\n                    document.Add(new Field("ObjectTypeInvariantName", typeof(Order).FullName, Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS));\n                    foreach (var property in properties)\n                    {\n                        var value = property.GetValue(order, null);\n                        if (value != null)\n                        {\n\n                            document.Add(new Field(property.Name, value.ToString(), Field.Store.NO, Field.Index.ANALYZED_NO_NORMS));\n                        }\n                    }\n                    indexWriter.AddDocument(document);\n                }\n                indexWriter.Optimize(true);\n                indexWriter.Commit();\n                return indexWriter.GetReader();\n            }\n
11	$index = Zend_Search_Lucene::open(APPLICATION_PATH . '/cache/search_index');\n    $doc = new Zend_Search_Lucene_Document();\n\n    $title = "سلام سینا xxx sad";\n\n\n\n    $doc-&gt;addField(Zend_Search_Lucene_Field::Text('title', $title,"UTF8"));\n\n    $index-&gt;addDocument($doc);\n    $index-&gt;commit();\n\n    $index-&gt;optimize();\n    echo "Index contains " . $index-&gt;count() . " documents.\n\n";\n\n\n\n    var_dump($index-&gt;getDocument(9));\n\n    echo "Search";\n     $results = $index-&gt;find('سینا');\n   foreach ($results as $res) {\n\n\n        var_dump($res-&gt;title);\n    }\n\n    die(1); \n
12	CREATE TABLE item (\n    id INTEGER PRIMARY KEY ASC,\n    thing TEXT);\n\nINSERT INTO item (thing) VALUES ("thing 1");\nINSERT INTO item (thing) VALUES ("thing 2");\nINSERT INTO item (thing) VALUES ("thing 3");\n\nCREATE TEMP TABLE ordered (\n    id INTEGER PRIMARY KEY ASC,\n    item_id INTEGER);\n\nINSERT INTO ordered (item_id) VALUES (2);\nINSERT INTO ordered (item_id) VALUES (3);\nINSERT INTO ordered (item_id) VALUES (1);\n\nSELECT item.thing\nFROM item\nJOIN ordered\nON ordered.item_id = item.id\nORDER BY ordered.id;\n
13	# Linux     (Ubuntu 8.10 64-bit, Python 2.5.2, OpenJDK 1.6, setuptools 0.6c9)\nPREFIX_PYTHON=/usr\nANT=ant\nPYTHON=$(PREFIX_PYTHON)/bin/python\nJCC=$(PYTHON) -m jcc.__main__ --shared\nNUM_FILES=200\n
14	FullTextSession textSession = Search.getFullTextSession(session);\ntextSession.index(myProductSummary);\n
15	import org.apache.lucene.index.IndexReader;\nimport org.apache.lucene.index.DirectoryReader;\nimport org.apache.lucene.index.Fields;\nimport org.apache.lucene.index.DocsEnum;\nimport org.apache.lucene.document.Document;\nimport org.apache.lucene.index.TermsEnum;\nimport org.apache.lucene.index.Term;\nimport org.apache.lucene.index.MultiFields;\nimport org.apache.lucene.util.BytesRef;\nimport org.apache.lucene.util.BytesRefBuilder;\nimport org.apache.lucene.util.NumericUtils;\nimport org.apache.lucene.queryparser.classic.ParseException;\nimport org.apache.lucene.store.Directory;\nimport org.apache.lucene.store.FSDirectory;\nimport org.apache.lucene.search.DocIdSetIterator;\nimport org.apache.lucene.search.IndexSearcher;\nimport org.apache.lucene.search.TermQuery;\nimport org.apache.lucene.search.TotalHitCountCollector;\nimport org.apache.lucene.util.Bits;\nimport org.apache.lucene.index.MultiFields;\n\npublic class ReadLongTermReferenceCount {\n\n    public static void main(String[] args) throws IOException {\n\n        Directory dirIndex = FSDirectory.open('/path/to/index/');\n        IndexReader indexReader = DirectoryReader.open(dirIndex);\n        final BytesRefBuilder bytes = new BytesRefBuilder(); \n\n\n        TermsEnum termEnum = MultiFields.getTerms(indexReader, "field").iterator(null);\n\n        IndexSearcher searcher = new IndexSearcher(indexReader);\n        TotalHitCountCollector collector = new TotalHitCountCollector();\n\n        Bits liveDocs = MultiFields.getLiveDocs(indexReader);\n        final BytesRefBuilder bytes = new BytesRefBuilder(); // for reuse!\n        int maxDoc = indexReader.maxDoc();\n        int docsPassed = 0;\n        for (int i=0; i&lt;maxDoc; i++) {\n            if (docsPassed==100) {\n                break;\n            }\n            if (liveDocs != null &amp;&amp; !liveDocs.get(i))\n                continue;\n            Document doc = indexReader.document(i);\n\n            //get longTerm from this doc and convert to BytesRefBuilder\n            String longTerm = doc.get("longTerm");\n            NumericUtils.longToPrefixCoded(Long.valueOf(longTerm).longValue(),0,bytes);\n\n            //time before the first test\n            long time_start = System.nanoTime();\n\n            //look in the "field" index for longTerm and count the number of documents\n            int count = 0;\n            termEnum.seekExact(bytes.toBytesRef());\n            DocsEnum docsEnum = termEnum.docs(liveDocs, null);\n            if (docsEnum != null) {\n                int docx;\n                while ((docx = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    count++;\n                }\n            }\n\n            //mid point: test 1 done, start of test 2\n            long time_mid = System.nanoTime();\n\n            //do a search for longTerm in "field"\n            TermQuery query = new TermQuery(new Term("field", bytes.toBytesRef()));\n            searcher.search(query,collector);\n            int count = collector.getTotalHits();\n\n            //end point: test 2 done.\n            long time_end = System.nanoTime();\n\n            //write to stdout\n            System.out.println(longTerm+"\t"+(time_mid-time_start)+"\t"+(time_end-time_mid));\n\n            docsPassed++;\n        }\n        indexReader.close();\n        dirIndex.close();\n    }\n}   \n
16	    SolrInputDocument parentDoc = new SolrInputDocument();\n    parentDoc.addField("id", "parent_1");\n    parentDoc.addField("name_s", "Sarah Connor");\n    parentDoc.addField("blockJoinId", "1");\n    solrClient.add(parentDoc);\n    solrClient.commit();\n
17	// Chains together standard tokenizer, standard filter, and lowercase filter\nclass MyAnalyzer : Analyzer\n{\n    public override TokenStream TokenStream(string fieldName, System.IO.TextReader reader)\n    {\n        StandardTokenizer baseTokenizer = new StandardTokenizer(Lucene.Net.Util.Version.LUCENE_29, reader);\n        StandardFilter standardFilter = new StandardFilter(baseTokenizer);\n        LowerCaseFilter lcFilter = new LowerCaseFilter(standardFilter);\n        return lcFilter; \n    }\n}\n
18	var query = new PhraseQuery();\nquery.Add(new Term("propertyName", "tulip"));\nquery.Add(new Term("propertyName", "inn"));\nquery.Add(new Term("propertyName", "ryiadhh"));\n
19	&lt;fieldType name="location" class="solr.LatLonType" subFieldSuffix="_coordinate"/&gt;\n
20	&lt;updateLog&gt; \n  &lt;str name="dir"&gt;${solr.data.dir:}&lt;/str&gt; \n&lt;/updateLog&gt; \n
21	curl http://localhost:8983/solr/update/json?softCommit=true -H 'Content-type:application/json' -d '\n[\n  {\n    "id": "123",\n    "text" : "Bill studied The Bill of Rights last summer.",\n    "content_type": "source",\n    "_childDocuments_": [\n      {\n        "id": "123-1",\n        "content_type": "source_annotation",\n        "annotation": "William Brown",\n        "start_offset": 0,\n        "end_offset": 4\n      },\n      {\n        "id": "123-2",\n        "content_type": "source_annotation",\n        "annotation": "legal term",\n        "start_offset": 13,\n        "end_offset": 31\n      },\n      {\n        "id": "123-3",\n        "content_type": "source_annotation",\n        "annotation": "summer 2011",\n        "start_offset": 32,\n        "end_offset": 43\n      }\n    ]\n  }\n]\n
22	IList&lt;string&gt; ALL_TYPES = new[] { "article", "blog", "forum" };\nstring q = ...; // The user's search string\nIList&lt;string&gt; includeTypes = ...; // List of types to include\nQuery searchQuery = parser.Parse(q);\nQuery parentQuery = new BooleanQuery();\nparentQuery.Add(searchQuery, BooleanClause.Occur.SHOULD);\n// Invert the logic, exclude the other types\nforeach (var type in ALL_TYPES.Except(includeTypes))\n{\n    query.Add(\n        new TermQuery(new Term("type", type)),\n        BooleanClause.Occur.MUST_NOT\n    );\n}\nsearchQuery = parentQuery;\n
23	&lt;!--&lt;str name="spellcheck.maxCollationTries"&gt;3&lt;/str&gt; here is a bug, put this parameter in the actual query string instead --&gt;\n
24	#!/bin/bash\n\nexport ELASTICSEARCH_ENDPOINT="http://localhost:9200"\n\n# Index documents\ncurl -XPOST "$ELASTICSEARCH_ENDPOINT/_bulk?refresh=true" -d '\n{"index":{"_index":"play","_type":"type"}}\n{"title":"Female"}\n{"index":{"_index":"play","_type":"type"}}\n{"title":"Female specimen"}\n{"index":{"_index":"play","_type":"type"}}\n{"title":"Microscopic examination of specimen from female"}\n'\n\n# Do searches\n\n# This will match all documents.\ncurl -XPOST "$ELASTICSEARCH_ENDPOINT/_search?pretty" -d '\n{\n    "query": {\n        "prefix": {\n            "title": {\n                "prefix": "femal"\n            }\n        }\n    }\n}\n'\n\n# This matches only the two first documents.\ncurl -XPOST "$ELASTICSEARCH_ENDPOINT/_search?pretty" -d '\n{\n    "query": {\n        "span_first": {\n            "end": 1,\n            "match": {\n                "span_multi": {\n                    "match": {\n                        "prefix": {\n                            "title": {\n                                "prefix": "femal"\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n'\n\n# This matches all, but prefers the one's with a prefix match.\n# It's sufficient that either of these match, but prefer that both matches.\ncurl -XPOST "$ELASTICSEARCH_ENDPOINT/_search?pretty" -d '\n{\n    "query": {\n        "bool": {\n            "should": [\n                {\n                    "span_first": {\n                        "end": 1,\n                        "match": {\n                            "span_multi": {\n                                "match": {\n                                    "prefix": {\n                                        "title": {\n                                            "prefix": "femal"\n                                        }\n                                    }\n                                }\n                            }\n                        }\n                    }\n                },\n                {\n                    "match": {\n                        "title": {\n                            "query": "femal"\n                        }\n                    }\n                }\n            ]\n        }\n    }\n}\n'\n
25	String queryString = ...;\nString[] fields = ...;\nAnalyzer analyzer = ...;\n\nMultiFieldQueryParser queryParser = new MultiFieldQueryParser(fields, analyzer);\nqueryParser.setDefaultOperator(QueryParser.Operator.AND);\n\nQuery query = queryParser.parse(queryString);\n\n// If you're not happy with MultiFieldQueryParser's default Occur (SHOULD), you can re-configure it afterward:\nif (query instanceof BooleanQuery) {\n    BooleanClause.Occur[] flags = ...;\n    BooleanQuery booleanQuery = (BooleanQuery) query;\n    BooleanClause[] clauses = booleanQuery.getClauses();\n    for (int i = 0; i &lt; clauses.length; i++) {\n        clauses[i].setOccur(flags[i]);\n    }\n}\n
26	Map&lt;String, String&gt; explainmap = response.getExplainMap();\nString explanation = explainmap.get(id);\n
27	...    \nCharTermAttribute token = tokenStream.getAttribute(CharTermAttribute.class);\ntokenStream.reset();   // I added this \nwhile(stopFilter.incrementToken()) {\n...\n
28	Query query = new TermQuery(new Term("3","\"Fuel Tank Capacity\"@en"));\n
29	String id = "42";\nDocument doc = new Document();\nField field = new Field("id", id, Store.YES, Index.NOT_ANALYZED);\ndoc.add(field);\n\nindexWriter.updateDocument(new Term("id", id), doc);\n
30	#!/bin/bash\n\nexport ELASTICSEARCH_ENDPOINT="http://localhost:9200"\n\n# Index documents\ncurl -XPOST "$ELASTICSEARCH_ENDPOINT/_bulk?refresh=true" -d '\n{"index":{"_index":"play","_type":"type"}}\n{"word":"bar"}\n{"index":{"_index":"play","_type":"type"}}\n{"word":"barf"}\n{"index":{"_index":"play","_type":"type"}}\n{"word":"zip"}\n'\n\n# Do searches\n# This will not match barf\ncurl -XPOST "$ELASTICSEARCH_ENDPOINT/_search?pretty" -d '\n{\n    "query": {\n        "filtered": {\n            "filter": {\n                "regexp": {\n                    "word": {\n                        "value": ".{0,3}"\n                    }\n                }\n            }\n        }\n    }\n}\n'\n
31	query = monthQb\n        .range()\n            .onField( "startTS" ).ignoreFieldBridge()\n            .from( DateTools.dateToString( from, DateTools.Resolution.MILLISECOND ) )\n            .to( DateTools.dateToString( to, DateTools.Resolution.MILLISECOND ) ).excludeLimit()\n            .createQuery();\n
32	StandardAnalyzer analyzer = new StandardAnalyzer(Version.LUCENE_CURRENT);\n// city query\nQueryParser cityQP = new QueryParser(Version.LUCENE_CURRENT, "city", analyzer);\nQuery cityQuery = cityQP.parse(myCity);\n\n// title query\nQueryParser titleQP = new QueryParser(Version.LUCENE_CURRENT, "title", analyzer);\nQuery titleQuery = titleQP.parse(myQuery);\n\n// final query\nBooleanQuery finalQuery = new BooleanQuery();\nfinalQuery.add(cityQuery, Occur.MUST); // MUST implies that the keyword must occur.\nfinalQuery.add(titleQuery, Occur.MUST); // Using all "MUST" occurs is equivalent to "AND" operator.\n
33	&lt;dynamicField type="PageEditor.Unleashed.Search.DynamicFields.VisualizationField, PageEditor.Unleashed.Classes" name="_content" storageType="NO" indexType="TOKENIZED" vectorType="NO" boost="1f" /&gt;\n
34	&lt;!--\n      Default numeric field types. For faster range queries, consider the tint/tfloat/tlong/tdouble types.\n    --&gt;\n&lt;fieldType name="int" class="solr.TrieIntField" precisionStep="0" positionIncrementGap="0"/&gt;\n&lt;fieldType name="float" class="solr.TrieFloatField" precisionStep="0" positionIncrementGap="0"/&gt;\n&lt;fieldType name="long" class="solr.TrieLongField" precisionStep="0" positionIncrementGap="0"/&gt;\n&lt;fieldType name="double" class="solr.TrieDoubleField" precisionStep="0" positionIncrementGap="0"/&gt;\n&lt;!--\n     Numeric field types that index each value at various levels of precision\n     to accelerate range queries when the number of values between the range\n     endpoints is large. See the javadoc for NumericRangeQuery for internal\n     implementation details.\n\n     Smaller precisionStep values (specified in bits) will lead to more tokens\n     indexed per value, slightly larger index size, and faster range queries.\n     A precisionStep of 0 disables indexing at different precision levels.\n    --&gt;\n&lt;fieldType name="tint" class="solr.TrieIntField" precisionStep="8" positionIncrementGap="0"/&gt;\n&lt;fieldType name="tfloat" class="solr.TrieFloatField" precisionStep="8" positionIncrementGap="0"/&gt;\n&lt;fieldType name="tlong" class="solr.TrieLongField" precisionStep="8" positionIncrementGap="0"/&gt;\n&lt;fieldType name="tdouble" class="solr.TrieDoubleField" precisionStep="8" positionIncrementGap="0"/&gt;\n
35	      &lt;cores adminPath="/admin/cores" defaultCoreName="collection1"&gt;\n         &lt;core name="collection1" instanceDir="." /&gt;\n       &lt;/cores&gt;\n
36	export CLASSPATH=${CLASSPATH}:/Users/philhunter/Desktop/COM562\ Project/lucene-3.0.3/lucene-core-3.0.3.jar\n
37	&lt;schemaFactory class="ManagedIndexSchemaFactory"&gt;\n  &lt;bool name="mutable"&gt;true&lt;/bool&gt;\n  &lt;str name="managedSchemaResourceName"&gt;managed-schema&lt;/str&gt;\n&lt;/schemaFactory&gt;\n
38	        IndexReader reader = IndexReader.open(FSDirectory.open(indexDir));\n        IndexSearcher searcher = new IndexSearcher(reader);\n        Analyzer analyzer = new IKAnalyzer();\n        QueryParser parser = new QueryParser(Version.LUCENE_31, "title", analyzer);\n        Query q = null;\n        q = parser.parse("MacOS");\n        TopDocs docs = searcher.search(q, 10);\n        ScoreDoc[] filterScoreDosArray = docs.topDocs().scoreDocs;\n        for (int i = 0; i &lt; filterScoreDosArray.length; ++i) {\n            int docId = filterScoreDosArray[i].doc;\n            Document d = is.doc(docId);\n            System.out.println((i + 1) + ". " + d.get("docno")+" Score: "+ filterScoreDosArray[i].score);\n        }\n
39	...\nDirectoryReader reader = DirectoryReader.open(MMapDirectory.open( java.io.File(indexFile) );\nIndexSearcher searcher = new IndexSearcher(reader);\nTopScoreDocCollector collector = TopScoreDocCollector.create(MAX_RESULTS, true);  // MAX_RESULTS is just an int limiting the total number of hits \nint startIndex = (page -1) * hitsPerPage;  // our page is 1 based - so we need to convert to zero based\nQuery query = new QueryParser(Version.LUCENE_48, "All", analyzer).parse(searchQuery);\nsearcher.search(query, collector);\nTopDocs hits = collector.topDocs(startIndex, hitsPerPage);\n...\n
40	 public TokenStream reusableTokenStream(String fieldName, Reader reader) throws IOException {\n        TokenStream stream = (TokenStream) getPreviousTokenStream();\n\n        if (stream == null) {\n            stream = new AttachmentNameTokenizer(reader);\n            if (stemmTokens)\n                stream = new SnowballFilter(stream, name);\n            setPreviousTokenStream(stream); // ---------------&gt;  problem was here \n        } else if (stream instanceof Tokenizer) {\n            ( (Tokenizer) stream ).reset(reader); \n        }\n\n        return stream;\n    }\n
41	&lt;copyField source="title" dest="exact_title"/&gt; \n
42	Query newQuery = query.Rewrite(indexReader);\n
43	var searcher = new IndexSearcher( reader );\nvar collector = new IntegralCollector(); // my custom Collector\nsearcher.Search( query, collector );\nvar result = new Document[ collector.Docs.Count ];\nfor ( int i = 0; i &lt; collector.Docs.Count; i++ )\n    result[ i ] = searcher.Doc( collector.Docs[ i ] );\nsearcher.Close(); // this is probably not needed\nreader.Close();\n
44	double distance = DistanceUtils.getInstance().getDistanceMi(lat1,lon1,lat2,lon2);\n
45	protected void HistoryEngine_AddedEntry(object sender, HistoryAddedEventArgs e)\n{\n    Item item = e.Database.GetItem(e.Entry.ItemId);\n    //TODO: Add logic to make sure e.Entry.ItemId requires a parent/child reindex as well\n    //TODO: Make sure that logic also prevents excessive or infinite recursion since we'll be triggering the AddedEntry event again below\n    Item parent = item.Parent;\n    //RegisterItemSaved doesn't appear to do anything with its second argument\n    e.Database.Engines.HistoryEngine.RegisterItemSaved(parent, null);\n}\n
46	SimMetricsMetricUtilities.Levenstein ls = new SimMetricsMetricUtilities.Levenstein(); //compare string 1, string 2 \ndouble sim = ls.GetSimilarity(string_1, string_2); \nif(sim &gt; [some value]) \n{ \n//do something \n} \n
47	solr/select?q=*:*&amp;fl=*,customFunc:complexFunction(querySpecificValue1,querySpecificValue2)\n
48	Directory directory = new RAMDirectory();\nAnalyzer analyzer = new StandardAnalyzer(Version.LUCENE_30);\nMaxFieldLength mlf = MaxFieldLength.UNLIMITED;\nIndexWriter writer = new IndexWriter(directory, analyzer, true, mlf);\n\nDocument doc = new Document();\ndoc.add(new Field("tags", "foo bar", Field.Store.NO,\n        Field.Index.ANALYZED, Field.TermVector.YES));\n\nwriter.addDocument(doc);\nwriter.close();\n\nIndexReader reader = IndexReader.open(directory);\nfor (int i = 0; i &lt; reader.numDocs(); i++) {\n    TermFreqVector tfv = reader.getTermFreqVector(i, "tags");\n    System.out.println(tfv);\n}\n
49	&lt;add&gt;\n  &lt;doc&gt;\n    &lt;field name="id"&gt;UTF8TEST&lt;/field&gt;\n    &lt;field name="name"&gt;Test with some UTF-8 encoded characters&lt;/field&gt;\n    &lt;field name="manu"&gt;Apache Software Foundation&lt;/field&gt;\n    &lt;field name="cat"&gt;software&lt;/field&gt;\n    &lt;field name="cat"&gt;search&lt;/field&gt;\n    &lt;field name="features"&gt;No accents here&lt;/field&gt;\n    &lt;field name="price"&gt;0&lt;/field&gt;\n    &lt;!-- no popularity, get the default from schema.xml --&gt;\n    &lt;field name="inStock"&gt;true&lt;/field&gt;\n  &lt;/doc&gt;\n&lt;/add&gt;\n
50	IndexSearcher searcher = new IndexSearcher(directoryReader);\nTermQuery query = new TermQuery(new Term("field", "term"));\nTopDocs topdocs = searcher.query(query, numberToReturn);\n
51	using System.Collections.Generic;\nusing System.Linq;\nusing NUnit.Framework;\nusing Raven.Abstractions.Data;\nusing Raven.Client;\nusing Raven.Client.Document;\nusing Raven.Client.Indexes;\nusing Raven.Client.Linq;\n\nnamespace Prototype.Search.Tests\n{\n    [TestFixture]\n    public class HierarchicalFaceting\n    {\n        //\n        // Document definition\n        //\n        public class Doc\n        {\n            public Doc()\n            {\n                Categories = new List&lt;string&gt;();\n            }\n\n            public int Id { get; set; }\n            public List&lt;string&gt; Categories { get; set; }\n        }\n\n        //\n        // Data sample\n        //\n        public IEnumerable&lt;Doc&gt;  GetDocs()\n        {\n            yield return new Doc { Id = 1, Categories = new List&lt;string&gt; { "0/NonFic", "1/NonFic/Law"} };\n            yield return new Doc { Id = 2, Categories = new List&lt;string&gt; { "0/NonFic", "1/NonFic/Sci" } };\n            yield return new Doc { Id = 3, Categories = new List&lt;string&gt; { "0/NonFic", "1/NonFic/Hist", "1/NonFic/Sci", "2/NonFic/Sci/Phys" } };\n        }\n\n        //\n        // The index\n        //\n        public class DocByCategory : AbstractIndexCreationTask&lt;Doc, DocByCategory.ReduceResult&gt;\n        {\n            public class ReduceResult\n            {\n                public string Category { get; set; }\n            }\n\n            public DocByCategory()\n            {\n                Map = docs =&gt;\n                      from d in docs\n                      from c in d.Categories\n                      select new\n                                 {\n                                     Category = c\n                                 };\n            }\n        }\n\n        //\n        // FacetSetup\n        //\n        public FacetSetup GetDocFacetSetup()\n        {\n            return new FacetSetup\n                       {\n                           Id = "facets/Doc",\n                           Facets = new List&lt;Facet&gt;\n                                        {\n                                            new Facet\n                                                {\n                                                    Name = "Category"\n                                                }\n                                        }\n                       };\n        }\n\n        [SetUp]\n        public void SetupDb()\n        {\n            IDocumentStore store = new DocumentStore()\n            {\n                Url = "http://localhost:8080"\n            };\n            store.Initialize();\n            IndexCreation.CreateIndexes(typeof(HierarchicalFaceting).Assembly, store);\n\n            var session = store.OpenSession();\n            session.Store(GetDocFacetSetup());\n            session.SaveChanges();\n\n            store.Dispose();\n        }\n\n        [Test]\n        [Ignore]\n        public void DeleteAll()\n        {\n            IDocumentStore store = new DocumentStore()\n            {\n                Url = "http://localhost:8080"\n            };\n            store.Initialize();\n\n            store.DatabaseCommands.DeleteIndex("Raven/DocByCategory");\n            store.DatabaseCommands.DeleteByIndex("Raven/DocumentsByEntityName", new IndexQuery());\n\n            store.Dispose();\n        }\n\n        [Test]\n        [Ignore]\n        public void StoreDocs()\n        {\n            IDocumentStore store = new DocumentStore()\n            {\n                Url = "http://localhost:8080"\n            };\n            store.Initialize();\n\n            var session = store.OpenSession();\n\n            foreach (var doc in GetDocs())\n            {\n                session.Store(doc);\n            }\n\n            session.SaveChanges();\n            session.Dispose();\n            store.Dispose();\n        }\n\n        [Test]\n        public void QueryDocsByCategory()\n        {\n            IDocumentStore store = new DocumentStore()\n            {\n                Url = "http://localhost:8080"\n            };\n            store.Initialize();\n\n            var session = store.OpenSession();\n\n            var q = session.Query&lt;DocByCategory.ReduceResult, DocByCategory&gt;()\n                .Where(d =&gt; d.Category == "1/NonFic/Sci")\n                .As&lt;Doc&gt;();\n\n            var results = q.ToList();\n            var facetResults = q.ToFacets("facets/Doc").ToList();\n\n            session.Dispose();\n            store.Dispose();\n        }\n\n        [Test]\n        public void GetFacets()\n        {\n            IDocumentStore store = new DocumentStore()\n            {\n                Url = "http://localhost:8080"\n            };\n            store.Initialize();\n\n            var session = store.OpenSession();\n\n            var q = session.Query&lt;DocByCategory.ReduceResult, DocByCategory&gt;()\n                .Where(d =&gt; d.Category.StartsWith("1/NonFic"))\n                .As&lt;Doc&gt;();\n\n            var results = q.ToList();\n            var facetResults = q.ToFacets("facets/Doc").ToList();\n\n            session.Dispose();\n            store.Dispose();\n        }\n    }\n}\n
52	import java.net.MalformedURLException;\n\nimport org.apache.solr.client.solrj.SolrServer;\nimport org.apache.solr.client.solrj.SolrServerException;\nimport org.apache.solr.client.solrj.impl.HttpSolrServer;\nimport org.apache.solr.client.solrj.response.QueryResponse;\nimport org.apache.solr.common.params.ModifiableSolrParams;\n\npublic class SolrQuery {\n  public static void main(String[] args) throws MalformedURLException, SolrServerException {\n    SolrServer server = new HttpSolrServer("http://localhost:8080/solr");\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set("q", "1");\n\n            QueryResponse response = server.query(params);\n\n            System.out.println("response = " + response);\n\n  }\n} \n
53	&lt;fieldType name="text" class="solr.TextField" omitNorms="false"&gt;\n  &lt;analyzer type="index"&gt;\n    &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;\n    &lt;filter class="solr.StopFilterFactory" words="stopwords.txt"/&gt;\n    &lt;filter class="solr.StandardFilterFactory"/&gt;\n    &lt;filter class="solr.LowerCaseFilterFactory"/&gt;\n    &lt;filter class="solr.PorterStemFilterFactory"/&gt;\n    &lt;filter class="solr.EdgeNGramFilterFactory" minGramSize="2" maxGramSize="15" side="front"/&gt;\n  &lt;/analyzer&gt;\n  &lt;analyzer type="query"&gt;\n    &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;\n    &lt;filter class="solr.StopFilterFactory" words="stopwords.txt"/&gt;\n    &lt;filter class="solr.StandardFilterFactory"/&gt;\n    &lt;filter class="solr.LowerCaseFilterFactory"/&gt;\n    &lt;filter class="solr.PorterStemFilterFactory"/&gt;\n  &lt;/analyzer&gt;\n&lt;/fieldType&gt;\n
54	&lt;script&gt;&lt;![CDATA[\n        function addfield(row){\n            var fieldName = row.get('key') + "_s"\n            row.put(fieldName, row.get('value'));\n            return row;\n        }\n]]&gt;&lt;/script&gt;\n
55	&lt;requestHandler name="/dataimport"  class="org.apache.solr.handler.dataimport.DataImportHandler"&gt;  \n    .........\n    &lt;lst name="defaults"&gt;\n        &lt;str name="config"&gt;data-config.xml&lt;/str&gt;\n        &lt;str name="update.chain"&gt;uuid&lt;/str&gt;\n    &lt;/lst&gt;\n&lt;/requestHandler&gt;\n\n&lt;updateRequestProcessorChain name="uuid"&gt;\n  &lt;processor class="solr.UUIDUpdateProcessorFactory"&gt;\n      &lt;str name="fieldName"&gt;uuid&lt;/str&gt;\n  &lt;/processor&gt;\n  &lt;processor class="solr.RunUpdateProcessorFactory" /&gt;\n&lt;/updateRequestProcessorChain&gt;\n
56	private void GetIndexTerms(string indexFolder)\n{\n    List&lt;String&gt; termlist = new ArrayList&lt;String&gt;();\n    IndexReader reader = IndexReader.open(indexFolder);\n    TermEnum terms = reader.terms();\n    while (terms.next()) \n    {\n      Term term = terms.term();\n      String termText = term.text();\n      int frequency = reader.docFreq(term);\n      termlist.add(termText);\n    }\n    reader.close();\n}\n
57	&lt;fieldType name="text" class="solr.TextField" positionIncrementGap="100"&gt;\n  &lt;analyzer&gt;\n    &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;\n    &lt;filter class="solr.StandardFilterFactory"/&gt;\n    &lt;filter class="solr.LowerCaseFilterFactory"/&gt;\n  &lt;/analyzer&gt;\n&lt;/fieldType&gt;\n\n&lt;fieldType name="text_ngram" class="solr.TextField" positionIncrementGap="100"&gt;\n  &lt;analyzer&gt;\n    &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;\n    &lt;filter class="solr.StandardFilterFactory"/&gt;\n    &lt;filter class="solr.LowerCaseFilterFactory"/&gt;\n    &lt;filter class="solr.NGramFilterFactory" minGramSize="2" maxGramSize="15" side="front"/&gt;\n  &lt;/analyzer&gt;\n&lt;/fieldType&gt;\n\n&lt;fieldType name="text_first_letter" class="solr.TextField" positionIncrementGap="100"&gt;\n  &lt;analyzer&gt;\n    &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;\n    &lt;filter class="solr.StandardFilterFactory"/&gt;\n    &lt;filter class="solr.LowerCaseFilterFactory"/&gt;\n    &lt;filter class="solr.EdgeNGramFilterFactory" minGramSize="1" maxGramSize="1" side="front"/&gt;\n  &lt;/analyzer&gt;\n&lt;/fieldType&gt;\n
58	q={!boost b=boost1}\n
59	def results = searchableService.search(\n    "(${query} AND approved:1) OR (${query} -approved:0 -approved:1)"\n)\n
60	IndexWriter writer = new IndexWriter("MyIndexPath",analyzer, false);\n
61	index = RAMDirectory.new\nwriter = IndexWriter.new(index, StandardAnalyzer.new(Version::LUCENE_30),IndexWriter::MaxFieldLength::UNLIMITED)\nwriter.commit()\nIndexSearcher.new(index)\n
62	IndexHits&lt;Node&gt; hits = actors.get( "name", "Keanu Reeves" );\nNode reeves = hits.getSingle();\n
63	IndexWriter writer = ...\nwriter.update(new Term("id","http://somedomain.org/somedoc.htm"), doc); \n
64	Document doc = new Document();\ndoc.add(new Field("modified",\n        DateTools.timeToString(f.lastModified(), DateTools.Resolution.MINUTE),\n        Field.Store.YES, Field.Index.NOT_ANALYZED));\n
65	int numberOfHits = 200;\nString LocationOfDirectory = "C:\\dir\\index";\nTopScoreDocCollector collector = TopScoreDocCollector.create(numberOfHits, true);\nDirectory directory = new SimpleFSDirectory(new File(LocationOfDirectory));\nIndexSearcher searcher = new IndexSearcher(IndexReader.open(directory);\n\nStandardAnalyzer analyzer = new StandardAnalyzer(Version.LUCENE_35);\n\n//WildcardQuery q = new WildcardQuery(new Term("Street", "the crescent");\nQueryParser qp = new QueryParser(Version.LUCENE_35, "Street", analyzer);\nqp.setDefaultOperator(QueryParser.Operator.AND);\n\nQuery q = qp.parse("grove road");\n\nsearcher.search(q, collector);\nScoreDoc[] hits = collector.topDocs().scoreDocs;\n
66	class WhitespaceAndAtSymbolTokenizer : CharTokenizer\n{\n    public WhitespaceAndAtSymbolTokenizer(TextReader input)\n        : base(input)\n    {\n    }\n\n    protected override bool IsTokenChar(char c)\n    {\n        // Make whitespace characters and the @ symbol be indicators of new words.\n        return !(char.IsWhiteSpace(c) || c == '@');\n    }\n}\n\n\ninternal class WhitespaceAndAtSymbolAnalyzer : Analyzer\n{\n    public override TokenStream TokenStream(string fieldName, TextReader reader)\n    {\n        return new WhitespaceAndAtSymbolTokenizer(reader);\n    }\n}\n
67	cache = [1, 0, 0, 1, .. 0]\n
68	&lt;requestHandler name="/analysis/field" class="solr.FieldAnalysisRequestHandler" /&gt;\n
69	public class IndexCrawler : DatabaseCrawler\n{\n    protected override void IndexVersion(Item item, Item latestVersion, Sitecore.Search.IndexUpdateContext context)\n    {\n        if (item.Versions.Count &gt; 0 &amp;&amp; item.Version.Number != latestVersion.Version.Number)\n            return;\n\n        base.IndexVersion(item, latestVersion, context);\n    }\n}\n
70	...&amp;q=+author:John+published_from:2016-08-03\n
71	public class MunicipalitiesAndDistrictsNamesIndex : AbstractMultiMapIndexCreationTask&lt;MunicipalitiesAndDistrictsNamesIndex.Result&gt;\n{\n    public class Result\n    {\n        public string Name { get; set; }\n\n        public string Value { get; set; }\n    }\n\n    public MunicipalitiesAndDistrictsNamesIndex()\n    {\n        AddMap&lt;Municipality&gt;(municipality =&gt; from m in municipalities\n            select new\n            {\n                m.Name,\n                m.Value,\n            });\n\n        AddMap&lt;Municipality&gt;(municipality =&gt; from m in municipalities\n            from d in m.Districts\n            select new\n            {\n                d.Name,\n                d.Value,\n            });\n\n        // mark 'Name' field as analyzed which enables full text search operations\n        Index(x =&gt; x.Name, FieldIndexing.Search);\n\n        // storing fields so when projection\n        // requests only those fields\n        // then data will come from index only, not from storage\n        Store(x =&gt; x.Name, FieldStorage.Yes);\n        Store(x =&gt; x.Value, FieldStorage.Yes);\n    }\n}\n
72	fq={!tag=pt}price:[0 TO 100]&amp;facet=true&amp;facet.query={!ex=pt key=$queryOne}price:[0 TO 100]&amp;facet.query={!ex=pt key=$queryTwo}price:[100 TO *]&amp;queryOne=price:[0 TO 100]&amp;queryTwo=price:[100 TO *]\n
73	var query = context.GetQueryable&lt;SearchResultItem&gt;().Where(predicate);\nreturn query.ToList();\n
74	    BooleanQuery bQuery = new BooleanQuery();\n    Session session = persistence.currentManager();\n    FullTextSession fullTextSession = Search.getFullTextSession(session);\n    Analyzer analyzer = fullTextSession.getSearchFactory().getAnalyzer("searchtokenanalyzer");\n    QueryParser parser = new QueryParser(Version.LUCENE_35, "title", analyzer);\n    String[] tokenized=null;\n    try {\n    Query query=    parser.parse(searchString);\n    String cleanedText=query.toString("title");\n     tokenized = cleanedText.split("\\s");\n\n    } catch (ParseException e) {\n        // TODO Auto-generated catch block\n        e.printStackTrace();\n    }\n\n    QueryBuilder qBuilder = fullTextSession.getSearchFactory()\n            .buildQueryBuilder().forEntity(LearningGoal.class).get();\n    for(int i=0;i&lt;tokenized.length;i++){\n         if(i==(tokenized.length-1)){\n            Query query = qBuilder.keyword().wildcard().onField("title")\n                    .matching(tokenized[i] + "*").createQuery();\n                bQuery.add(query, BooleanClause.Occur.MUST);\n        }else{\n            Term exactTerm = new Term("title", tokenized[i]);\n            bQuery.add(new TermQuery(exactTerm), BooleanClause.Occur.MUST);\n        }\n    }\n        for (LearningGoal exGoal : existingGoals) {\n        Term omittedTerm = new Term("id", String.valueOf(exGoal.getId()));\n        bQuery.add(new TermQuery(omittedTerm), BooleanClause.Occur.MUST_NOT);\n    }\n    org.hibernate.Query hibQuery = fullTextSession.createFullTextQuery(\n            bQuery, LearningGoal.class);\n
75	DateTime d1 = Convert.ToDateTime(dr[dc.ColumnName]);\ndoc.Add(new Field("Registered_Date", DateTools.DateToString(d1, DateTools.Resolution.SECOND), Field.Store.YES, Field.Index.ANALYZED));\n
76	org.hibernate.search.FullTextQuery query = s.createFullTextQuery( luceneQuery, MyEntity.class );\norg.apache.lucene.search.Sort sort = new Sort(\n    SortField.FIELD_SCORE, \n    new SortField("id", SortField.STRING, true));\nquery.setSort(sort);\nList results = query.list();\n
77	public class GapTest {\n\n    public static void main(String[] args) throws Exception {\n        final Directory dir = new RAMDirectory();\n        final IndexWriterConfig iwConfig = new IndexWriterConfig(Version.LUCENE_4_10_1, new SimpleAnalyzer());\n        final IndexWriter iw = new IndexWriter(dir, iwConfig);\n\n        Document doc = new Document();\n        doc.add(new TextField("body", "A B C", Store.YES));\n        doc.add(new TextField("body", new PositionIncrementTokenStream(10)));\n        doc.add(new TextField("body", "D E F", Store.YES));\n\n        System.out.println(doc);\n        iw.addDocument(doc);\n        iw.close();\n\n        final IndexReader ir = DirectoryReader.open(dir);\n        IndexSearcher is = new IndexSearcher(ir);\n\n        QueryParser qp = new QueryParser("body", new SimpleAnalyzer());\n\n        for (String q : new String[] { "\"A B C\"", "\"A B C D\"",\n                "\"A B C D\"", "\"A B C D\"~10", "\"A B C D E F\"~10",\n                "\"A B C D F E\"~10", "\"A B C D F E\"~11" }) {\n            Query query = qp.parse(q);\n            TopDocs docs = is.search(query, 10);\n            System.out.println(docs.totalHits + "\t" + q);\n        }\n        ir.close();\n    }\n\n    /**\n     * A gaps-only TokenStream (uses {@link PositionIncrementAttribute}\n     * \n     * @author Christian Kohlschuetter\n     */\n    private static final class PositionIncrementTokenStream extends TokenStream {\n    private boolean first = true;\n    private PositionIncrementAttribute attribute;\n    private final int positionIncrement;\n\n    public PositionIncrementTokenStream(final int positionIncrement) {\n        super();\n        this.positionIncrement = positionIncrement;\n        attribute = addAttribute(PositionIncrementAttribute.class);\n    }\n\n    @Override\n    public boolean incrementToken() throws IOException {\n        if (first) {\n            first = false;\n            attribute.setPositionIncrement(positionIncrement);\n            return true;\n        } else {\n            return false;\n        }\n    }\n\n    @Override\n    public void reset() throws IOException {\n        super.reset();\n        first = true;\n    }\n}\n
78	TermQuery toolQuery = new TermQuery(new Term("Tool", "Nail"));\nTermQuery nailLengthQuery = new TermQuery(new Term("NailLength", "3 inches"));\n\nBooleanQuery filterQuery = new BooleanQuery();\nfilterQuery.add(toolQuery, BooleanClause.Occur.MUST);\nfilterQuery.add(nailLengthQuery, BooleanClause.Occur.MUST);\n\nFilter f = new QueryFilter(filterQuery);\n
79	q=*:*&amp;sort=view_count desc&amp;rows=1&amp;fl=view_count\n
80	String categoriesForItem = getCategories(); // get "category1, category2, cat3" from a DB call\n\nString [] categoriesForItems = categoriesForItem.split(","); \nfor(String cat : categoriesForItems) {\n    doc.add(new StringField("categories", cat , Field.Store.YES)); // doc is a Document \n}\n
81	BooleanQuery booleanQuery = new BooleanQuery.Builder()\n    .add(query1, BooleanClause.Occur.MUST)\n    .add(query2, BooleanClause.Occur.MUST)\n    .build();\n
82	&lt;lib dir="../../../dist/" regex="solr-dataimporthandler-4.3.0.jar" /&gt;\n&lt;lib dir="../../../dist/" regex="solr-dataimporthandler-extras-4.3.0.jar" /&gt;\n
83	String[] fields = {"title", "body", "subject", "author"};\nQueryParser[] parsers = new QueryParser[fields.length];      \nfor(int i = 0; i &lt; parsers.length; i++)\n{\n   parsers[i] = new QueryParser(Version.LUCENE_35, fields[i], analyzer);\n   parsers[i].setDefaultOperator(QueryParser.AND_OPERATOR);\n}\n
84	SearchResponse response = client.prepareSearch("mongoindex")\n    .setSearchType(SearchType.QUERY_AND_FETCH)\n    .setQuery(fieldQuery("name", "test name"))\n    .setFrom(0).setSize(60).setExplain(true)\n    .execute()\n    .actionGet();\nSearchHit[] results = response.getHits().getHits();\nfor (SearchHit hit : results) {\n  System.out.println(hit.getId());    //prints out the id of the document\n  Map&lt;String,Object&gt; result = hit.getSource();   //the retrieved document\n}\n
85	&lt;field name="WouldBuySameModelAgain"&gt;true&lt;/field&gt;\n
86	String[] fields = new String[] { "title", "keywords", "text" };\nHashMap&lt;String,Float&gt; boosts = new HashMap&lt;String,Float&gt;();\nboosts.put("title", 10);\nboosts.put("keywords", 5);\nMultiFieldQueryParser queryParser = new MultiFieldQueryParser(\n    fields, \n    new StandardAnalyzer(),\n    boosts\n);\n
87	public class ScaledScoreDocComparator implements ScoreDocComparator {\n\n    private int[][] values;\n    private float[] scalars;\n\n    public ScaledScoreDocComparator(IndexReader reader, String[] fields, float[] scalars) throws IOException {\n        this.scalars = scalars;\n        this.values = new int[fields.length][];\n        for (int i = 0; i &lt; values.length; i++) {\n            this.values[i] = FieldCache.DEFAULT.getInts(reader, fields[i]);\n        }\n    }\n\n    protected float score(ScoreDoc scoreDoc) {\n        int doc = scoreDoc.doc;\n\n        float score = 0;\n        for (int i = 0; i &lt; values.length; i++) {\n            int value = values[i][doc];\n            float scalar = scalars[i];\n            score += (value * scalar);\n        }\n        return score;\n    }\n\n    @Override\n    public int compare(ScoreDoc i, ScoreDoc j) {\n        float iScore = score(i);\n        float jScore = score(j);\n        return Float.compare(iScore, jScore);\n    }\n\n    @Override\n    public int sortType() {\n        return SortField.CUSTOM;\n    }\n\n    @Override\n    public Comparable&lt;?&gt; sortValue(ScoreDoc i) {\n        float score = score(i);\n        return Float.valueOf(score);\n    }\n\n}\n
88	import java.io.IOException;\nimport java.nio.file.Paths;\nimport java.util.Scanner;\nimport java.util.concurrent.*;\nimport org.apache.lucene.analysis.standard.StandardAnalyzer;\nimport org.apache.lucene.document.*;\nimport org.apache.lucene.index.*;\nimport org.apache.lucene.search.*;\nimport org.apache.lucene.store.FSDirectory;\n\npublic class LucenePeriodicCommitRefreshExample {\n    ScheduledExecutorService scheduledExecutor;\n    MyIndexer indexer;\n    MySearcher searcher;\n\n    void init() throws IOException {\n        scheduledExecutor = Executors.newScheduledThreadPool(3);\n        indexer = new MyIndexer();\n        indexer.init();\n        searcher = new MySearcher(indexer.indexWriter);\n        searcher.init();\n    }\n\n    void destroy() throws IOException {\n        searcher.destroy();\n        indexer.destroy();\n        scheduledExecutor.shutdown();\n    }\n\n    class MyIndexer {\n        IndexWriter indexWriter;\n        Future commitFuture;\n\n        void init() throws IOException {\n            indexWriter = new IndexWriter(FSDirectory.open(Paths.get("C:\\Temp\\lucene-example")), new IndexWriterConfig(new StandardAnalyzer()));\n            indexWriter.deleteAll();\n            for (int i = 1; i &lt;= 100000; i++) {\n                add(String.valueOf(i), "whatever " + i);\n            }\n            indexWriter.commit();\n            commitFuture = scheduledExecutor.scheduleWithFixedDelay(() -&gt; {\n                try {\n                    indexWriter.commit();\n                } catch (IOException e) {\n                    e.printStackTrace();\n                }\n            }, 5, 5, TimeUnit.MINUTES);\n        }\n\n        void add(String id, String text) throws IOException {\n            Document doc = new Document();\n            doc.add(new StringField("id", id, Field.Store.YES));\n            doc.add(new StringField("text", text, Field.Store.YES));\n            indexWriter.addDocument(doc);\n        }\n\n        void update(String id, String text) throws IOException {\n            indexWriter.deleteDocuments(new Term("id", id));\n            add(id, text);\n        }\n\n        void destroy() throws IOException {\n            commitFuture.cancel(false);\n            indexWriter.close();\n        }\n    }\n\n    class MySearcher {\n        IndexWriter indexWriter;\n        SearcherManager searcherManager;\n        Future maybeRefreshFuture;\n\n        public MySearcher(IndexWriter indexWriter) {\n            this.indexWriter = indexWriter;\n        }\n\n        void init() throws IOException {\n            searcherManager = new SearcherManager(indexWriter, true, null);\n            maybeRefreshFuture = scheduledExecutor.scheduleWithFixedDelay(() -&gt; {\n                try {\n                    searcherManager.maybeRefresh();\n                } catch (IOException e) {\n                    e.printStackTrace();\n                }\n            }, 0, 5, TimeUnit.SECONDS);\n        }\n\n        String findText(String id) throws IOException {\n            IndexSearcher searcher = null;\n            try {\n                searcher = searcherManager.acquire();\n                TopDocs topDocs = searcher.search(new TermQuery(new Term("id", id)), 1);\n                return searcher.doc(topDocs.scoreDocs[0].doc).getField("text").stringValue();\n            } finally {\n                if (searcher != null) {\n                    searcherManager.release(searcher);\n                }\n            }\n        }\n\n        void destroy() throws IOException {\n            maybeRefreshFuture.cancel(false);\n            searcherManager.close();\n        }\n    }\n\n    public static void main(String[] args) throws IOException {\n        LucenePeriodicCommitRefreshExample example = new LucenePeriodicCommitRefreshExample();\n        example.init();\n        Runtime.getRuntime().addShutdownHook(new Thread() {\n            @Override\n            public void run() {\n                try {\n                    example.destroy();\n                } catch (IOException e) {\n                    e.printStackTrace();\n                }\n            }\n        });\n\n        try (Scanner scanner = new Scanner(System.in)) {\n            System.out.print("Enter a document id to update (from 1 to 100000): ");\n            String id = scanner.nextLine();\n            System.out.print("Enter what you want the document text to be: ");\n            String text = scanner.nextLine();\n            example.indexer.update(id, text);\n            long startTime = System.nanoTime();\n            String foundText;\n            do {\n                foundText = example.searcher.findText(id);\n            } while (!text.equals(foundText));\n            long elapsedTimeMillis = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - startTime);\n            System.out.format("it took %d milliseconds for the searcher to see that document %s is now '%s'\n", elapsedTimeMillis, id, text);\n        } catch (Exception e) {\n            e.printStackTrace();\n        } finally {\n            System.exit(0);\n        }\n    }\n}\n
89	IndexReader reader = IndexReader.open(index);\nTermEnum terms = reader.terms();\nSet&lt;String&gt; uniqueTerms = new HashSet&lt;String&gt;();\nwhile (terms.next()) {\n        final Term term = terms.term();\n        if (term.field().equals("field_name")) {\n                uniqueTerms.add(term.text());\n        }\n}\n
90	.Or(i =&gt; i.Name.Contains(term).Boost(1))\n
91	@Override\nprotected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n    final Tokenizer source = new StandardTokenizer(matchVersion, reader);\n    TokenStream result = new StandardFilter(matchVersion, source);\n    // prior to this we get the classic behavior, standardfilter does it for us.\n    if (matchVersion.onOrAfter(Version.LUCENE_31))\n      result = new EnglishPossessiveFilter(matchVersion, result);\n    result = new LowerCaseFilter(matchVersion, result);\n    result = new StopFilter(matchVersion, result, stopwords);\n    if(!stemExclusionSet.isEmpty())\n      result = new KeywordMarkerFilter(result, stemExclusionSet);\n    result = new PorterStemFilter(result);\n    return new TokenStreamComponents(source, result);\n }\n
92	TopDocs hits = is.search(booleanQuery.build(),10);\n
93	// Find all docs whose .Text contains "hello", ordered by .CreationDate.\nvar query = new QueryParser(Lucene.Net.Util.Version.LUCENE_29, "Text", new StandardAnalyzer()).Parse("hello");\nvar indexDirectory = FSDirectory.Open(new DirectoryInfo("c:\\foo"));\nvar searcher = new IndexSearcher(indexDirectory, true);\ntry\n{\n   var sort = new Sort(new SortField("CreationDate", SortField.LONG));\n   var filter =  new QueryWrapperFilter(query);\n   var results = searcher.Search(query, , 1000, sort);\n   foreach (var hit in results.scoreDocs)\n   {\n       Document document = searcher.Doc(hit.doc);\n       Console.WriteLine("\tFound match: {0}", document.Get("Text"));\n   }\n}\nfinally\n{\n   searcher.Close();\n}\n
94	    class TVM : TermVectorMapper\n    {\n        public List&lt;string&gt; FoundTerms = new List&lt;string&gt;();\n        HashSet&lt;string&gt; _termTexts = new HashSet&lt;string&gt;();\n\n        public TVM(Query q, IndexReader r) : base()\n        {\n            List&lt;Term&gt; allTerms = new List&lt;Term&gt;();\n            q.Rewrite(r).ExtractTerms(allTerms);\n            foreach (Term t in allTerms) _termTexts.Add(t.Text());\n        }\n\n        public override void SetExpectations(string field, int numTerms, bool storeOffsets, bool storePositions)\n        {\n        }\n\n        public override void Map(string term, int frequency, TermVectorOffsetInfo[] offsets, int[] positions)\n        {\n            if (_termTexts.Contains(term)) FoundTerms.Add(term);\n        }\n    }\n\n    void TermVectorMapperTest()\n    {\n        RAMDirectory dir = new RAMDirectory();\n        IndexWriter writer = new IndexWriter(dir, new Lucene.Net.Analysis.Standard.StandardAnalyzer(), true);\n        Document d = null;\n\n        d = new Document();\n        d.Add(new Field("text", "microscope aaa", Field.Store.YES, Field.Index.ANALYZED,Field.TermVector.WITH_POSITIONS_OFFSETS));\n        writer.AddDocument(d);\n\n        d = new Document();\n        d.Add(new Field("text", "microsoft bbb", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n        writer.AddDocument(d);\n\n        writer.Close();\n\n        IndexReader reader = IndexReader.Open(dir);\n        IndexSearcher searcher = new IndexSearcher(reader);\n\n        QueryParser queryParser = new QueryParser("text", new Lucene.Net.Analysis.Standard.StandardAnalyzer());\n        queryParser.SetMultiTermRewriteMethod(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE); \n        Query query = queryParser.Parse("micro*");\n\n        TopDocs results = searcher.Search(query, 5);\n        System.Diagnostics.Debug.Assert(results.TotalHits == 2);\n\n        TVM tvm = new TVM(query, reader);\n        for (int i = 0; i &lt; results.ScoreDocs.Length; i++)\n        {\n            Console.Write("DOCID:" + results.ScoreDocs[i].Doc + " &gt; ");\n            reader.GetTermFreqVector(results.ScoreDocs[i].Doc, "text", tvm);\n            foreach (string term in tvm.FoundTerms) Console.Write(term + " ");\n            tvm.FoundTerms.Clear();\n            Console.WriteLine();\n        }\n    }\n
95	Query titleQuery, viewsQuery;\n\ntitleQuery.setBoost(0.8);\nviewsQuery.setBoost(0.2);\nBooleanQuery query = new BooleanQuery();\nquery.add(titleQuery, Occur.MUST); // or Occur.SHOULD if this clause is optional\nquery.add(viewsQuery, Occur.SHOULD); // or Occur.MUST if this clause is required\n\n// use query to search documents\n
96	Analyzer ana = new StandardAnalyzer(LUCENE_30, Collections.emptySet());\n
97	using System;\nusing System.IO;\n\nusing org.apache.pdfbox.pdmodel;\nusing org.apache.pdfbox.util;\n\nnamespace testPDF\n{\nclass Program\n{\n    static void Main()\n    {\n        PDFtoText pdf = new PDFtoText();\n\n        string pdfText = pdf.parsePDF(@"C:\Sample.pdf");\n\n        using (StreamWriter writer = new StreamWriter(@"C:\Sample.txt"))\n        { writer.Write(pdfText); }\n\n    }\n\n    class PDFtoText\n    {\n        public string parsePDF(string filepath)\n        {\n            PDDocument document = PDDocument.load(filepath);\n            PDFTextStripper stripper = new PDFTextStripper();\n            return stripper.getText(document);\n        }\n\n    }\n}\n\n}\n
98	IndexReader indexReader = IndexReader.open(path); \nTermEnum termEnum = indexReader.terms(); \nwhile (termEnum.next()) { \n    Term term = termEnum.term(); \n    System.out.println(term.text()); \n}\ntermEnum.close(); \nindexReader.close(); \n
99	BooleanQuery categoryQuery = new BooleanQuery();\nTermQuery catQuery1 = new TermQuery(new Term("category_name", "Electronics"));\nTermQuery catQuery2 = new TermQuery(new Term("category_name", "Home"));\ncategoryQuery.add(new BooleanClause(catQuery1, BooleanClause.Occur.SHOULD));\ncategoryQuery.add(new BooleanClause(catQuery2, BooleanClause.Occur.SHOULD));\nbq.add(new BooleanClause(categoryQuery, BooleanClause.Occur.MUST));\n
100	&lt;Engines.HistoryEngine.Storage&gt;\n  &lt;obj type="Sitecore.Data.$(database).$(database)HistoryStorage, Sitecore.Kernel"&gt;\n   &lt;param connectionStringName="$(id)" /&gt;\n   &lt;EntryLifeTime&gt;30.00:00:00&lt;/EntryLifeTime&gt;\n  &lt;/obj&gt;\n&lt;/Engines.HistoryEngine.Storage&gt;\n&lt;Engines.HistoryEngine.SaveDotNetCallStack&gt;false&lt;/Engines.HistoryEngine.SaveDotNetCallStack&gt;\n
101	//declare a mutilphrasequery\nMultiPhraseQuery childrenInOrder = new MultiPhraseQuery();\n\n//user fuzzytermenum to enumerate your query string\nFuzzyTermEnum fuzzyEnumeratedTerms1 = new FuzzyTermEnum(reader, new Term(searchField,"mosa"));\nFuzzyTermEnum fuzzyEnumeratedTerms2 = new FuzzyTermEnum(reader, new Term(searchField,"employee"));\nFuzzyTermEnum fuzzyEnumeratedTerms3 = new FuzzyTermEnum(reader, new Term(searchField,"appreicata"));\n\n//this basically pull out the possbile terms from the index             \nTerm termHolder1 = fuzzyEnumeratedTerms1.term();\nTerm termHolder2 = fuzzyEnumeratedTerms2.term();\nTerm termHolder3 = fuzzyEnumeratedTerms3.term();\n\n//put the possible terms into multiphrasequery\nif (termHolder1==null){\n    childrenInOrder.add(new Term(searchField,"mosa"));\n}else{\n    childrenInOrder.add(fuzzyEnumeratedTerms1.term());\n}\n\nif (termHolder2==null){\n    childrenInOrder.add(new Term(searchField,"employee"));\n}else{\n    childrenInOrder.add(fuzzyEnumeratedTerms2.term());\n}\n\nif (termHolder3==null){\n    childrenInOrder.add(new Term(searchField,"appreicata"));\n}else{\n    childrenInOrder.add(fuzzyEnumeratedTerms3.term());\n}\n\n\n//close it - it is important to close it\nfuzzyEnumeratedTerms1.close();\nfuzzyEnumeratedTerms2.close();\nfuzzyEnumeratedTerms3.close();\n
102	&lt;fields hint="raw:AddComputedIndexField"&gt;\n   &lt;field fieldName="read_roles"           returnType="stringCollection"&gt;Sitecore.ContentSearch.ComputedFields.ReadItemRoles,Sitecore.ContentSearch&lt;/field&gt;\n&lt;/fields&gt;\n
103	Analyzer analyzer = new SnowballAnalyzer(Version.LUCENE_30, "English");\n
104	SpanTermQuery spanTermQuery = new SpanTermQuery(new Term("title", "lucene"));\nSpanFirstQuery spanFirstQuery = new SpanFirstQuery(spanTermQuery, 1);\n
105	O(|Q|· |D| · |T|) = O(|D| · |T|) \n
106	Deprecated.\n  Use SnowballPorterFilterFactory with language="English" instead\n
107	curl -XPUT localhost:9200/test-idx -d '{\n    "settings": {\n        "index": {\n            "number_of_shards": 1,\n            "number_of_replicas": 0\n        }\n    },\n    "mappings": {\n        "doc": {\n            "properties": {\n                "first_name": {\n                    "type": "multi_field",\n                    "path": "just_name",\n                    "fields": {\n                        "first_name": {"type": "string", "index": "analyzed"},\n                        "name": {"type": "string","index": "analyzed"}\n                    }\n                },\n                "last_name": {\n                    "type": "multi_field",\n                    "path": "just_name",\n                    "fields": {\n                        "last_name": {"type": "string", "index": "analyzed"},\n                        "name": {"type": "string","index": "analyzed"}\n                    }\n                }\n            }\n        }\n    }\n}'\necho\ncurl -XPUT localhost:9200/test-idx/doc/1 -d '{\n    "first_name": "Sebastien",\n    "last_name": "Lorber"\n}'\necho\ncurl -XPOST localhost:9200/test-idx/_refresh\necho\ncurl "localhost:9200/test-idx/doc/_search?q=name:Sebastien"\necho\ncurl "localhost:9200/test-idx/doc/_search?q=name:Lorber"\n
108	Lucene.Net.Documents.Field fldContent = \n    new Lucene.Net.Documents.Field("content", \n        File.ReadAllText(@"Documents\100.txt"),\n    Lucene.Net.Documents.Field.Store.YES,\n    Lucene.Net.Documents.Field.Index.TOKENIZED, \n    Lucene.Net.Documents.Field.TermVector.WITH_POSITIONS_OFFSETS);\n
109	Fields fields = reader.Fields();\nif (fields != null) {\n  ...\n}\n
110	using System;\nusing System.Collections.Generic;\nusing Lucene.Net.Index;\nusing Lucene.Net.Search;\n\npublic class ScoreLimitingCollector : Collector {\n    private readonly Single _lowerInclusiveScore;\n    private readonly List&lt;Int32&gt; _docIds = new List&lt;Int32&gt;();\n    private Scorer _scorer;\n    private Int32 _docBase;\n\n    public IEnumerable&lt;Int32&gt; DocumentIds {\n        get { return _docIds; }\n    }\n\n    public ScoreLimitingCollector(Single lowerInclusiveScore) {\n        _lowerInclusiveScore = lowerInclusiveScore;\n    }\n\n    public override void SetScorer(Scorer scorer) {\n        _scorer = scorer;\n    }\n\n    public override void Collect(Int32 doc) {\n        var score = _scorer.Score();\n        if (_lowerInclusiveScore &lt;= score)\n            _docIds.Add(_docBase + doc);\n    }\n\n    public override void SetNextReader(IndexReader reader, Int32 docBase) {\n        _docBase = docBase;\n    }\n\n    public override bool AcceptsDocsOutOfOrder() {\n        return true;\n    }\n}\n
111	String termQueryString = "title:\"hello world\"";\nQuery termQuery = parser.parse(termQueryString);\n\nQuery pageQueryRange = NumericRangeQuery.newIntRange("page_count", 10, 20, true, true);\n\nQuery query = termQuery.combine(new Query[]{termQuery, pageQueryRange});\n
112	Reader reader = new StringReader("This is a test string");\nNGramTokenizer gramTokenizer = new NGramTokenizer(reader, 1, 3);\nCharTermAttribute charTermAttribute = gramTokenizer.addAttribute(CharTermAttribute.class);\ngramTokenizer.reset();\n\nwhile (gramTokenizer.incrementToken()) {\n    String token = charTermAttribute.toString();\n    //Do something\n}\ngramTokenizer.end();\ngramTokenizer.close();\n
113	TermQuery userQuery = new TermQuery(new Term("user_id", u.getId()+""));\n\nBooleanQuery orQuery = new BooleanQuery();\norQuery.add(new BooleanClause(name_query, Occur.SHOULD));\norQuery.add(new BooleanClause(desc_query, Occur.SHOULD));\n\nBooleanQuery andQuery = new BooleanQuery();\nandQuery.add(new BooleanClause(userQuery , Occur.MUST));\nandQuery.add(new BooleanClause(orQuery, Occur.MUST));\n
114	curl -XGET 'http://127.0.0.1:9200/_validate/query?pretty=1&amp;explain=true'  -d '\n{\n   "multi_match" : {\n      "operator" : "and",\n      "fields" : [\n         "firstname",\n         "lastname"\n      ],\n      "query" : "john smith"\n   }\n}\n'\n\n# {\n#    "_shards" : {\n#       "failed" : 0,\n#       "successful" : 1,\n#       "total" : 1\n#    },\n#    "explanations" : [\n#       {\n#          "index" : "test",\n#          "explanation" : "((+lastname:john +lastname:smith) | (+firstname:john +firstname:smith))",\n#          "valid" : true\n#       }\n#    ],\n#    "valid" : true\n# }\n
115	final MAX_RESULTS = 10000;\nfinal Term t = /* ... */;\nfinal TopDocs topDocs = searcher.search( new TermQuery( t ), MAX_RESULTS );\nfor ( ScoreDoc scoreDoc : topDocs.scoreDocs ) {\n    Document doc = searcher.doc( scoreDoc.doc )\n    // "FILE" is the field that recorded the original file indexed\n    File f = new File( doc.get( "FILE" ) );\n    // ...\n}\n
116	String defaultField = ...;\nAnalyzer analyzer = ...;\nQueryParser queryParser = new QueryParser(defaultField, analyzer);\n\nqueryParser.setDefaultOperator(QueryParser.Operator.AND);\n\nQuery query = queryParser.parse("Searching is fun");\n
117	public abstract class CustomComparator extends FieldComparator&lt;Double&gt; {\n    double[] scoring;\n    double bottom;\n    double topValue;\n    private FieldCache.Ints[] currentReaderValues;\n    private String[] fields;\n\n    protected abstract double getScore(int[] value);\n\n    public CustomComparator(int hitNum, String[] fields) {\n        this.fields = fields;\n        scoring = new double[hitNum];\n    }\n\n    int[] fromReaders(int doc) {\n        int[] result = new int[currentReaderValues.length];\n        for (int i = 0; i &lt; result.length; i++) {\n            result[i] = currentReaderValues[i].get(doc);\n        }\n        return result;\n    }\n\n    @Override\n    public int compare(int slot1, int slot2) {\n        return Double.compare(scoring[slot1], scoring[slot2]);\n    }\n\n    @Override\n    public void setBottom(int slot) {\n        this.bottom = scoring[slot];\n    }\n\n    @Override\n    public void setTopValue(Double top) {\n        topValue = top;\n    }\n\n    @Override\n    public int compareBottom(int doc) throws IOException {\n        double v2 = getScore(fromReaders(doc));\n        return Double.compare(bottom, v2);\n    }\n\n    @Override\n    public int compareTop(int doc) throws IOException {\n        double docValue = getScore(fromReaders(doc));\n        return Double.compare(topValue, docValue);\n    }\n\n    @Override\n    public void copy(int slot, int doc) throws IOException {\n        scoring[slot] = getScore(fromReaders(doc));\n    }\n\n    @Override\n    public FieldComparator&lt;Double&gt; setNextReader(AtomicReaderContext atomicReaderContext) throws IOException {\n        currentReaderValues = new FieldCache.Ints[fields.length];\n        for (int i = 0; i &lt; fields.length; i++) {\n            currentReaderValues[i] = FieldCache.DEFAULT.getInts(atomicReaderContext.reader(), fields[i], null, false);\n        }\n        return this;\n    }\n\n    @Override\n    public Double value(int slot) {\n        return scoring[slot];\n    }\n}\n
118	 public static void main(String[] args) {\n    try {\n      Directory directory = new RAMDirectory();  \n      Analyzer analyzer = new SimpleAnalyzer();\n      IndexWriter writer = new IndexWriter(directory, analyzer, true);\n
119	0:0:0:0:0:0:0:1%0 -  -  [21/01/2010:15:08:29 +0000] "GET /solr/select/?q=*:*&amp;qt=geo&amp;lat=45&amp;long=15&amp;radius=10 HTTP/1.1" 200 197 \n
120	Query query = parser.parse(QueryParser.escape(parsedReview));\n
121	&lt;doc&gt;\n    &lt;float name="score"&gt;0.4451987&lt;/float&gt;\n    &lt;str name="id"&gt;2&lt;/str&gt;\n    &lt;arr name="text_ws"&gt;\n        &lt;str&gt;David Letterman&lt;/str&gt;\n    &lt;/arr&gt;\n&lt;/doc&gt;\n&lt;doc&gt;\n    &lt;float name="score"&gt;0.44072422&lt;/float&gt;\n    &lt;str name="id"&gt;3&lt;/str&gt;\n    &lt;arr name="text_ws"&gt;\n        &lt;str&gt;David Hasselhoff&lt;/str&gt;\n        &lt;str&gt;David Michael Hasselhoff&lt;/str&gt;\n    &lt;/arr&gt;\n&lt;/doc&gt;\n&lt;doc&gt;\n    &lt;float name="score"&gt;0.314803&lt;/float&gt;\n    &lt;str name="id"&gt;1&lt;/str&gt;\n    &lt;arr name="text_ws"&gt;\n        &lt;str&gt;David Bowie&lt;/str&gt;\n        &lt;str&gt;David Robert Jones&lt;/str&gt;\n        &lt;str&gt;Ziggy Stardust&lt;/str&gt;\n        &lt;str&gt;Thin White Duke&lt;/str&gt;\n    &lt;/arr&gt;\n&lt;/doc&gt;\n
122	public static void main(String[] args) throws Exception {\n    // Create the in memory lucence index\n    RAMDirectory ramDir = new RAMDirectory();\n\n    // Create the analyzer (has default stop words)\n    Analyzer analyzer = new StandardAnalyzer();\n\n    // Create a set of documents to work with\n    createDocs(ramDir, analyzer);\n\n    // Query the set of documents\n    queryDocs(ramDir, analyzer);\n}\n\nprivate static void createDocs(RAMDirectory ramDir, Analyzer analyzer) \n        throws IOException {\n    // Setup the configuration for the index\n    IndexWriterConfig config = new IndexWriterConfig(analyzer);\n    config.setOpenMode(IndexWriterConfig.OpenMode.CREATE);\n\n    // IndexWriter creates and maintains the index\n    IndexWriter writer = new IndexWriter(ramDir, config);\n\n    // Create the documents\n    indexDoc(writer, "document-1", "hello planet mercury");\n    indexDoc(writer, "document-2", "hi PLANET venus");\n    indexDoc(writer, "document-3", "howdy Planet Earth");\n    indexDoc(writer, "document-4", "hey planet MARS");\n    indexDoc(writer, "document-5", "ayee Planet jupiter");\n\n    // Close down the writer\n    writer.close();\n}\n\nprivate static void indexDoc(IndexWriter writer, String name, String content) \n        throws IOException {\n    Document document = new Document();\n    document.add(new TextField("name", name, Field.Store.YES));\n    document.add(new TextField("body", content, Field.Store.YES));\n\n    writer.addDocument(document);\n}\n\nprivate static void queryDocs(RAMDirectory ramDir, Analyzer analyzer) \n        throws IOException, ParseException {\n    // IndexReader maintains access to the index\n    IndexReader reader = DirectoryReader.open(ramDir);\n\n    // IndexSearcher handles searching of an IndexReader\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    // Setup a query\n    QueryParser parser = new QueryParser("body", analyzer);\n    Query query = parser.parse("hey earth");\n\n    // Search the index\n    TopDocs foundDocs = searcher.search(query, 10);\n    System.out.println("Total Hits: " + foundDocs.totalHits);\n\n    for (ScoreDoc scoreDoc : foundDocs.scoreDocs) {\n        // Get the doc from the index by id\n        Document document = searcher.doc(scoreDoc.doc);\n        System.out.println("Name: " + document.get("name") \n                + " - Body: " + document.get("body") \n                + " - Score: " + scoreDoc.score);\n    }\n\n    // Close down the reader\n    reader.close();\n}\n
123	 IList&lt;Document&gt; luceneDocuments = new List&lt;Document&gt;();\n\n IndexReader indexReader = new IndexReader(directory);\n Searcher searcher = new IndexSearcher(indexReader);\n\n TopDocs results = searcher.Search("Your Query", null, skipRecords + takeRecords);\n ScoreDoc[] scoreDocs = results.scoreDocs;\n\n for (int i = skipRecords; i &lt; results.totalHits; i++)\n {\n      if (i &gt; (skipRecords + takeRecords) - 1)\n      {\n           break;\n      }\n\n      luceneDocuments.Add(searcher.Doc(scoreDocs[i].doc));\n }\n
124	curl 'http://localhost:9200/test-idx/_search?pretty=true' -d '{\n    "query" : {\n        "match_all" : { }\n    },\n    "script_fields": {\n        "terms" : {\n            "script": "doc[field].values",\n            "params": {\n                "field": "message"\n            }\n        }\n\n    }\n}'\n
125	IndexReader reader = // create IndexReader\nfor (int i=0; i&lt;reader.maxDoc(); i++) {\n    if (reader.isDeleted(i))\n        continue;\n\n    Document doc = reader.document(i);\n    String docId = doc.get("docId");\n\n    // do something with docId here...\n}\n
126	{!join from=fromField to=toField fromIndex=fromCoreName}fromQuery\n
127	public static IndexWriter writer = new IndexWriter(myDir);\n\npublic JsonResult SearchForStuff(string query)\n{\n    IndexReader reader = writer.GetReader();\n    IndexSearcher search = new IndexSearcher(reader);\n    // do the search\n}\n
128	import org.apache.lucene.analysis.PorterStemmer;\n...\nString stemTerm (String term) {\n    PorterStemmer stemmer = new PorterStemmer();\n    return stemmer.stem(term);\n}\n
129	  import org.apache.lucene.search.spell.SpellChecker;\n\n  SpellChecker spellchecker = new SpellChecker(spellIndexDirectory);\n  // To index a field of a user index:\n  spellchecker.indexDictionary(new LuceneDictionary(my_lucene_reader, a_field));\n  // To index a file containing words:\n  spellchecker.indexDictionary(new PlainTextDictionary(new File("myfile.txt")));\n  String[] suggestions = spellchecker.suggestSimilar("misspelt", 5);\n
130	#!/bin/bash\n\nexport ELASTICSEARCH_ENDPOINT="http://localhost:9200"\n\n# Create indexes\n\ncurl -XPUT "$ELASTICSEARCH_ENDPOINT/play" -d '{\n    "settings": {\n        "analysis": {\n            "text": [\n                "Michael",\n                "Heaney",\n                "Heavey"\n            ],\n            "analyzer": {\n                "metaphone": {\n                    "type": "custom",\n                    "tokenizer": "standard",\n                    "filter": [\n                        "my_metaphone"\n                    ]\n                },\n                "porter": {\n                    "type": "custom",\n                    "tokenizer": "standard",\n                    "filter": [\n                        "lowercase",\n                        "porter_stem"\n                    ]\n                }\n            },\n            "filter": {\n                "my_metaphone": {\n                    "encoder": "metaphone",\n                    "replace": false,\n                    "type": "phonetic"\n                }\n            }\n        }\n    },\n    "mappings": {\n        "jr": {\n            "properties": {\n                "pty_surename": {\n                    "type": "multi_field",\n                    "fields": {\n                        "pty_surename": {\n                            "type": "string",\n                            "analyzer": "simple"\n                        },\n                        "metaphone": {\n                            "type": "string",\n                            "analyzer": "metaphone"\n                        },\n                        "porter": {\n                            "type": "string",\n                            "analyzer": "porter"\n                        }\n                    }\n                }\n            }\n        }\n    }\n}'\n\n\n# Index documents\ncurl -XPOST "$ELASTICSEARCH_ENDPOINT/_bulk?refresh=true" -d '\n{"index":{"_index":"play","_type":"jr"}}\n{"pty_surname":"Heaney"}\n{"index":{"_index":"play","_type":"jr"}}\n{"pty_surname":"Heavey"}\n'\n\n# Do searches\n\ncurl -XPOST "$ELASTICSEARCH_ENDPOINT/_search?pretty" -d '\n{\n    "query": {\n        "bool": {\n            "should": [\n                {\n                    "bool": {\n                        "should": [\n                            {\n                                "match": {\n                                    "pty_surname": {\n                                        "query": "heavey"\n                                    }\n                                }\n                            },\n                            {\n                                "match": {\n                                    "pty_surname": {\n                                        "query": "heavey",\n                                        "fuzziness": 1\n                                    }\n                                }\n                            },\n                            {\n                                "match": {\n                                    "pty_surename.metaphone": {\n                                        "query": "heavey"\n                                    }\n                                }\n                            },\n                            {\n                                "match": {\n                                    "pty_surename.porter": {\n                                        "query": "heavey"\n                                    }\n                                }\n                            }\n                        ]\n                    }\n                }\n            ]\n        }\n    }\n}\n'\n
131	    SolrQuery q = new SolrQuery("*:*");\n    q.setRows(0);  // don't actually request any data\n    return server.query(q).getResults().getNumFound();\n
132	public final class LuceneUtils {\n\n    public static List&lt;String&gt; parseKeywords(Analyzer analyzer, String field, String keywords) {\n\n        List&lt;String&gt; result = new ArrayList&lt;String&gt;();\n        TokenStream stream  = analyzer.tokenStream(field, new StringReader(keywords));\n\n        try {\n            while(stream.incrementToken()) {\n                result.add(stream.getAttribute(TermAttribute.class).term());\n            }\n        }\n        catch(IOException e) {\n            // not thrown b/c we're using a string reader...\n        }\n\n        return result;\n    }  \n}\n
133	curl -XGET 'http://127.0.0.1:9200/my_index/page/_search?pretty=1'  -d '\n{\n   "query" : {\n      "text" : {\n         "text" : "interesting keywords"\n      }\n   },\n   "highlight" : {\n      "fields" : {\n         "text" : {}\n      }\n   }\n}\n'\n
134	import java.io.IOException;\nimport java.io.Reader;\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\n\nimport org.apache.lucene.analysis.Analyzer;\nimport org.apache.lucene.analysis.ISOLatin1AccentFilter;\nimport org.apache.lucene.analysis.LowerCaseFilter;\nimport org.apache.lucene.analysis.StopFilter;\nimport org.apache.lucene.analysis.TokenStream;\nimport org.apache.lucene.analysis.ngram.EdgeNGramTokenFilter;\nimport org.apache.lucene.analysis.ngram.EdgeNGramTokenFilter.Side;\nimport org.apache.lucene.analysis.standard.StandardFilter;\nimport org.apache.lucene.analysis.standard.StandardTokenizer;\nimport org.apache.lucene.document.Document;\nimport org.apache.lucene.document.Field;\nimport org.apache.lucene.index.CorruptIndexException;\nimport org.apache.lucene.index.IndexReader;\nimport org.apache.lucene.index.IndexWriter;\nimport org.apache.lucene.index.Term;\nimport org.apache.lucene.search.IndexSearcher;\nimport org.apache.lucene.search.Query;\nimport org.apache.lucene.search.ScoreDoc;\nimport org.apache.lucene.search.Sort;\nimport org.apache.lucene.search.TermQuery;\nimport org.apache.lucene.search.TopDocs;\nimport org.apache.lucene.search.spell.LuceneDictionary;\nimport org.apache.lucene.store.Directory;\nimport org.apache.lucene.store.FSDirectory;\n\n/**\n * Search term auto-completer, works for single terms (so use on the last term\n * of the query).\n * &lt;p&gt;\n * Returns more popular terms first.\n * \n * @author Mat Mannion, M.Mannion@warwick.ac.uk\n */\npublic final class Autocompleter {\n\n    private static final String GRAMMED_WORDS_FIELD = "words";\n\n    private static final String SOURCE_WORD_FIELD = "sourceWord";\n\n    private static final String COUNT_FIELD = "count";\n\n    private static final String[] ENGLISH_STOP_WORDS = {\n    "a", "an", "and", "are", "as", "at", "be", "but", "by",\n    "for", "i", "if", "in", "into", "is",\n    "no", "not", "of", "on", "or", "s", "such",\n    "t", "that", "the", "their", "then", "there", "these",\n    "they", "this", "to", "was", "will", "with"\n    };\n\n    private final Directory autoCompleteDirectory;\n\n    private IndexReader autoCompleteReader;\n\n    private IndexSearcher autoCompleteSearcher;\n\n    public Autocompleter(String autoCompleteDir) throws IOException {\n        this.autoCompleteDirectory = FSDirectory.getDirectory(autoCompleteDir,\n                null);\n\n        reOpenReader();\n    }\n\n    public List&lt;String&gt; suggestTermsFor(String term) throws IOException {\n        // get the top 5 terms for query\n        Query query = new TermQuery(new Term(GRAMMED_WORDS_FIELD, term));\n        Sort sort = new Sort(COUNT_FIELD, true);\n\n        TopDocs docs = autoCompleteSearcher.search(query, null, 5, sort);\n        List&lt;String&gt; suggestions = new ArrayList&lt;String&gt;();\n        for (ScoreDoc doc : docs.scoreDocs) {\n            suggestions.add(autoCompleteReader.document(doc.doc).get(\n                    SOURCE_WORD_FIELD));\n        }\n\n        return suggestions;\n    }\n\n    @SuppressWarnings("unchecked")\n    public void reIndex(Directory sourceDirectory, String fieldToAutocomplete)\n            throws CorruptIndexException, IOException {\n        // build a dictionary (from the spell package)\n        IndexReader sourceReader = IndexReader.open(sourceDirectory);\n\n        LuceneDictionary dict = new LuceneDictionary(sourceReader,\n                fieldToAutocomplete);\n\n        // code from\n        // org.apache.lucene.search.spell.SpellChecker.indexDictionary(\n        // Dictionary)\n        IndexReader.unlock(autoCompleteDirectory);\n\n        // use a custom analyzer so we can do EdgeNGramFiltering\n        IndexWriter writer = new IndexWriter(autoCompleteDirectory,\n        new Analyzer() {\n            public TokenStream tokenStream(String fieldName,\n                    Reader reader) {\n                TokenStream result = new StandardTokenizer(reader);\n\n                result = new StandardFilter(result);\n                result = new LowerCaseFilter(result);\n                result = new ISOLatin1AccentFilter(result);\n                result = new StopFilter(result,\n                    ENGLISH_STOP_WORDS);\n                result = new EdgeNGramTokenFilter(\n                    result, Side.FRONT,1, 20);\n\n                return result;\n            }\n        }, true);\n\n        writer.setMergeFactor(300);\n        writer.setMaxBufferedDocs(150);\n\n        // go through every word, storing the original word (incl. n-grams) \n        // and the number of times it occurs\n        Map&lt;String, Integer&gt; wordsMap = new HashMap&lt;String, Integer&gt;();\n\n        Iterator&lt;String&gt; iter = (Iterator&lt;String&gt;) dict.getWordsIterator();\n        while (iter.hasNext()) {\n            String word = iter.next();\n\n            int len = word.length();\n            if (len &lt; 3) {\n                continue; // too short we bail but "too long" is fine...\n            }\n\n            if (wordsMap.containsKey(word)) {\n                throw new IllegalStateException(\n                        "This should never happen in Lucene 2.3.2");\n                // wordsMap.put(word, wordsMap.get(word) + 1);\n            } else {\n                // use the number of documents this word appears in\n                wordsMap.put(word, sourceReader.docFreq(new Term(\n                        fieldToAutocomplete, word)));\n            }\n        }\n\n        for (String word : wordsMap.keySet()) {\n            // ok index the word\n            Document doc = new Document();\n            doc.add(new Field(SOURCE_WORD_FIELD, word, Field.Store.YES,\n                    Field.Index.UN_TOKENIZED)); // orig term\n            doc.add(new Field(GRAMMED_WORDS_FIELD, word, Field.Store.YES,\n                    Field.Index.TOKENIZED)); // grammed\n            doc.add(new Field(COUNT_FIELD,\n                    Integer.toString(wordsMap.get(word)), Field.Store.NO,\n                    Field.Index.UN_TOKENIZED)); // count\n\n            writer.addDocument(doc);\n        }\n\n        sourceReader.close();\n\n        // close writer\n        writer.optimize();\n        writer.close();\n\n        // re-open our reader\n        reOpenReader();\n    }\n\n    private void reOpenReader() throws CorruptIndexException, IOException {\n        if (autoCompleteReader == null) {\n            autoCompleteReader = IndexReader.open(autoCompleteDirectory);\n        } else {\n            autoCompleteReader.reopen();\n        }\n\n        autoCompleteSearcher = new IndexSearcher(autoCompleteReader);\n    }\n\n    public static void main(String[] args) throws Exception {\n        Autocompleter autocomplete = new Autocompleter("/index/autocomplete");\n\n        // run this to re-index from the current index, shouldn't need to do\n        // this very often\n        // autocomplete.reIndex(FSDirectory.getDirectory("/index/live", null),\n        // "content");\n\n        String term = "steve";\n\n        System.out.println(autocomplete.suggestTermsFor(term));\n        // prints [steve, steven, stevens, stevenson, stevenage]\n    }\n\n}\n
135	/?q=query&amp;fl=field1,field2,field3\n
136	TokenStream tokenStream = analyzer.tokenStream(fieldName, reader);\nOffsetAttribute offsetAttribute = tokenStream.getAttribute(OffsetAttribute.class);\nTermAttribute termAttribute = tokenStream.getAttribute(TermAttribute.class);\n\nwhile (tokenStream.incrementToken()) {\n    int startOffset = offsetAttribute.startOffset();\n    int endOffset = offsetAttribute.endOffset();\n    String term = termAttribute.term();\n}\n
137	?q=-id:["" TO *]\n
138	Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_40, CharArraySet.EMPTY_SET);\nTokenStream tokenStream = analyzer.tokenStream("content", new StringReader(input));\nStopFilter stopFilter = new StopFilter(Version.LUCENE_40, tokenStream, stopWords);\nstopFilter.setEnablePositionIncrements(false);\nSnowballFilter snowballFilter = new SnowballFilter(stopFilter, "English");\nShingleFilter bigramShingleFilter = new ShingleFilter(snowballFilter, 2, 2);\n
139	@SearchableProperty(format = "yyyyMMdd")\n
140	TokenStream stream = TokenSources.getAnyTokenStream(searcher.getIndexReader(), sd.doc, "title", doc, analyzer);\n
141	$value = strval($node);\nif ($value)\n
142	private int index(File indexDir, File dataDir, String suffix) throws Exception {\n    RAMDirectory ramDir = new RAMDirectory();\n    IndexWriter indexWriter = new IndexWriter(ramDir,\n        new StandardAnalyzer(Version.LUCENE_CURRENT), true,  \n        IndexWriter.MaxFieldLength.UNLIMITED);\n    indexWriter.setUseCompoundFile(false);\n    indexDirectory(indexWriter, dataDir, suffix);\n    int numIndexed = indexWriter.maxDoc();\n    indexWriter.optimize();\n    indexWriter.close();\n\n\n    IndexWriter index = new IndexWriter(FSDirectory.open(indexDir),\n        new StandardAnalyzer(Version.LUCENE_CURRENT), true,  \n        IndexWriter.MaxFieldLength.UNLIMITED);\n    index.addIndexesNoOptimize(ramDir);\n    index.optimize();\n    index.close();\n\n    return numIndexed;\n}\n
143	Analyzer analyzer = fullTextSession.getSearchFactory().getAnalyzer(Item.class);\nQueryParser parser = new MultiFieldQueryParser(Version.LUCENE_31, fields, analyzer);\n
144	Declare @SearchTerm nvarchar(max)\nDeclare @MaxResultTextLen int\n\nSet @SearchTerm = 'et dolore'\nSet @MaxResultTextLen = 100\n\nSelect  CharIndex(@SearchTerm, F.TextContents),\n    Case \n    When CharIndex(@SearchTerm, F.TextContents) &lt;= @MaxResultTextLen \n        Then Substring(F.TextContents, 1, @MaxResultTextLen) + '...'\n    Else Substring(@SearchTerm\n        , CharIndex(@SearchTerm, R.TextContents) \n                - @MaxResultTextLen + Len(@SearchTerm)\n        , @MaxResultTextLen) + '...'\n    End As TextContext\nFrom Files As F\nWhere Contains(F.TextContents, @SearchTerm)\n
145	public class ModFunctionParser extends ValueSourceParser {\n  @Override\n  public ValueSource parse(FunctionQParser fp) throws ParseException {\n    ValueSource a = fp.parseValueSource();\n    ValueSource b = fp.parseValueSource();\n    return new ModFunction(a, b);\n  }\n}\n
146	  mapping do\n    indexes :id\n    indexes :authority_name, :type =&gt; :string, :index =&gt; "not_analyzed"\n    indexes :authority_detail_1, :type =&gt; :string, :index =&gt; "not_analyzed"\n    indexes :authority_detail_2, :type =&gt; :string, :index =&gt; "not_analyzed"\n\n    indexes :regulations do\n      indexes :name, :type =&gt; :string, :index =&gt; "not_analyzed"\n    end\n  end\n\n  def to_indexed_json\n    to_json(:include =&gt; [:rules, :regulations, :speaker])\n  end\n
147	session.Query&lt;TranTest, TranTestIndex&gt;()\n       .Customize(x =&gt; ((IDocumentQuery&lt;TranTest&gt;) x)\n                           .Search(q =&gt; q.Trans["en"], searchTerms))\n
148	curl -XDELETE "localhost:9200/geo-test/"\necho\n# Set proper mapping. Elasticsearch cannot automatically detect that something is a geo_point:\ncurl -XPUT "localhost:9200/geo-test" -d '{\n    "settings": {\n        "index": {\n            "number_of_replicas" : 0,\n            "number_of_shards": 1\n        }\n    },\n    "mappings": {\n        "doc": {\n            "properties": {\n                "location" : {\n                    "type" : "geo_point"\n                }\n            }\n        }\n    }\n}'\necho\n# Put some test data in Sydney\ncurl -XPUT "localhost:9200/geo-test/doc/1" -d '{ \n    "title": "abcccc",\n    "price": 3300,\n    "price_per": "task",\n    "location": { "lat": -33.8756, "lon": 151.204 },\n    "description": "asdfasdf"\n }'\ncurl -XPOST "localhost:9200/geo-test/_refresh"\necho\n# Search, and calculate distance to Brisbane \ncurl -XPOST "localhost:9200/geo-test/doc/_search?pretty=true" -d '{\n    "query": {\n        "match_all": {}\n    },\n    "script_fields": {\n        "distance": {\n            "script": "doc['\''location'\''].arcDistanceInKm(-27.470,153.021)"\n        }\n    },\n    "fields": ["title", "location"]\n}\n'\necho\n
149	doc= PDFDocumentFactory.buildPDFDocument(file, config);\n\ndoc.addField(new StringField("path", file.getPath(), Field.Store.YES));\n\nSystem.out.println(doc.get("path"));\nwriter.addDocument(doc);\n
150	&lt;field name="sdn_names_phonetic" type="doublemetaphonetic" indexed="true" stored="false" termVectors="true"/&gt;\n&lt;field name="sdn_names" type="text_standard" indexed="true" stored="true" termVectors="true"/&gt;\n\n&lt;fieldType name="text_standard" class="solr.TextField"&gt; \n  &lt;analyzer class="org.apache.lucene.analysis.standard.StandardAnalyzer"/&gt; \n&lt;/fieldType&gt; \n&lt;fieldtype name="doublemetaphonetic" stored="false" indexed="true" class="solr.TextField" &gt;\n  &lt;analyzer&gt;\n    &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;\n    &lt;filter class="solr.DoubleMetaphoneFilterFactory" inject="false"/&gt;\n  &lt;/analyzer&gt;\n&lt;/fieldtype&gt;\n
151	@PostConstruct\n    public void init() {\n        GraphDatabaseService graphDb = template.getGraphDatabaseService();\n        try (Transaction t = graphDb.beginTx()) {\n            Index&lt;Node&gt; autoIndex = graphDb.index().forNodes("node_auto_index");\n            graphDb.index().setConfiguration(autoIndex, "type", "fulltext");\n            graphDb.index().setConfiguration(autoIndex, "to_lower_case", "true");\n            graphDb.index().setConfiguration(autoIndex, "analyzer", StandardAnalyzerV36.class.getName());\n            t.success();\n        }\n    }\n
152	org.apache.lucene.search.Query query1 = ....;\norg.apache.lucene.search.Query query2 = ....;\norg.apache.lucene.search.Query query3 = ....;\n\nBooleanQuery booleanQuery = new BooleanQuery();\n\nluceneBooleanQuery.add(query1, BooleanClause.Occur.MUST);\nluceneBooleanQuery.add(query2, BooleanClause.Occur.SHOULD);\nluceneBooleanQuery.add(query3, BooleanClause.Occur.SHOULD); \n\nFullTextQuery fullTextQuery = fullTextSession.createFullTextQuery(booleanQuery, DomainClass.class);\n
153	@Entity\n@Indexed\npublic class Document {\n\n    @Id\n    @DocumentId\n    private Long id;\n\n    @Field\n    private String name;\n\n    private Instant creationTimestamp;\n\n    @Field(analyze = Analyze.NO)\n    @Facet\n    @javax.persistence.Transient\n    public Long getCreationForFaceting() {\n        return creationTimestamp == null ? null : creationTimestamp.toEpochMilli();\n    }\n}\n
154	"OR item_id=%d+X"\n
155	IndexReader indexReader = IndexReader.open(directory); // this one uses default readonly mode\n
156	TopDocs results = searcher.Search(query, filter, num);\nforeach (ScoreDoc result in results.scoreDocs)\n{\n    Document resultDoc = searcher.Doc(result.doc);\n    string valOfField = resultDoc.Get("My Field");\n}\n
157	public final class PayloadGeneratorFilter extends TokenFilter {\n  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n  private final PayloadAttribute payAtt = addAttribute(PayloadAttribute.class);\n  private final PayloadGenerator payloadGenerator;\n  private final FrequencyClassDigest frequencyClassDigest;\n\n\n  public PayloadGeneratorFilter(TokenStream input, PayloadGenerator payloadGenerator,\n                                FrequencyClassDigest frequencyClassDigest) {\n    super(input);\n    this.payloadGenerator = payloadGenerator;\n    this.frequencyClassDigest = frequencyClassDigest;\n  }\n\n  @Override\n  public boolean incrementToken() throws IOException {\n    if (input.incrementToken()) {\n      final char[] buffer = termAtt.buffer();\n      final int length = termAtt.length();\n      String token = buffer.toString();\n      byte[] payloadBytes = payloadGenerator.generatePayload(token, frequencyClassDigest);\n      payAtt.setPayload(new Payload(payloadBytes));\n      return true;\n    }\n\n    return false;\n  }\n}\n
158	    &lt;fieldType name="lowercase" class="solr.TextField" positionIncrementGap="100"&gt;\n      &lt;analyzer&gt;\n        &lt;tokenizer class="solr.KeywordTokenizerFactory"/&gt;\n        &lt;filter class="solr.LowerCaseFilterFactory" /&gt;\n      &lt;/analyzer&gt;\n    &lt;/fieldType&gt;\n
159	MultiFieldQueryParser parser = new MultiFieldQueryParser(Version.LUCENE_30, getSearchFields(), \n          new KeywordAnalyzer(Version.LUCENE_30));\n
160	IndexWriterConfig config = new IndexWriterConfig(Version.LUCENE_36, new WhitespaceAnalyzer(Version.LUCENE_36))\n            .setOpenMode(IndexWriterConfig.OpenMode.CREATE_OR_APPEND);\nwriter = new IndexWriter( directory, config);\n
161	TermRangeQuery nq1 = new TermRangeQuery("publish_date",  "1299628800000", "1336867200000", true, true);\nTermRangeQuery nq1 = new TermRangeQuery("publish_date",  "1299628800", "1336867200", true, true); \nTermRangeQuery nq1 = new TermRangeQuery("publish_date",  "2012-09-10T00:00:00Z", "2012-10-10T00:00:00Z", true, true); \nTermRangeQuery nq1 = new TermRangeQuery("publish_date",  "20120910", "20121010", true, true);\n
162	    String foo = "foo:doc1.txt";\n    StandardAnalyzer sa = new StandardAnalyzer(Version.LUCENE_34);\n    TokenStream tokenStream = sa.tokenStream("foo", new StringReader(foo));\n    while (tokenStream.incrementToken()) {\n        System.out.println(tokenStream.getAttribute(TermAttribute.class).term());\n    }\n\n    System.out.println("-------------");\n\n    KeywordAnalyzer ka = new KeywordAnalyzer();\n    TokenStream tokenStream2 = ka.tokenStream("foo", new StringReader(foo));\n    while (tokenStream2.incrementToken()) {\n        System.out.println(tokenStream2.getAttribute(TermAttribute.class).term());\n    }\n
163	&lt;add&gt;\n    &lt;doc&gt;\n        &lt;field name="id"&gt;id_001&lt;/field&gt;\n        &lt;field name="priority"&gt;10&lt;/field&gt;\n    &lt;/doc&gt;\n&lt;/add&gt;\n
164	q={!boost b=(numItems*numFollowers)}query\n
165	.../select?q=city:Nashua&amp;wt=xml&amp;indent=true\n
166	#!/bin/bash\n\nexport ELASTICSEARCH_ENDPOINT="http://localhost:9200"\n\n# Index documents\ncurl -XPOST "$ELASTICSEARCH_ENDPOINT/_bulk?refresh=true" -d '\n{"index":{"_index":"play","_type":"type"}}\n{"user":"John Smith","email":"john.smith@gmail.com"}\n{"index":{"_index":"play","_type":"type"}}\n{"user":"Alice Smith","email":"john.smith@gmail.com"}\n'\n\n# Do searches\n\ncurl -XPOST "$ELASTICSEARCH_ENDPOINT/_search?pretty" -d '\n{\n    "query": {\n        "multi_match": {\n            "fields": [\n                "user",\n                "email"\n            ],\n            "query": "john",\n            "operator": "and",\n            "type": "phrase_prefix"\n        }\n    }\n}\n'\n
167	import java.io.IOException;\nimport java.io.Reader;\nimport java.util.Iterator;\n\nimport com.google.common.collect.Iterators;\nimport org.apache.lucene.analysis.Tokenizer;\nimport org.apache.lucene.analysis.standard.ClassicTokenizer;\nimport org.apache.lucene.analysis.tokenattributes.CharTermAttribute;\nimport org.apache.lucene.analysis.tokenattributes.TypeAttribute;\n\nimport static vyre.util.search.LuceneVersion.VERSION_IN_USE;\n\n/**\n * Allows to easily manipulate with {@link ClassicTokenizer} by delegating calls to it but hiding all implementation details.\n *\n * @author Mindaugas Žakšauskas\n */\npublic abstract class ClassicTokenizerDelegate extends Tokenizer {\n\n    private final ClassicTokenizer classicTokenizer;\n\n    private final CharTermAttribute termAtt;\n\n    private final TypeAttribute typeAtt;\n\n    /**\n     * Internal buffer of tokens if any of standard tokens was split into many.\n     */\n    private Iterator&lt;String&gt; pendingTokens = Iterators.emptyIterator();\n\n    protected ClassicTokenizerDelegate(Reader input) {\n        super(input);\n        this.classicTokenizer = new ClassicTokenizer(VERSION_IN_USE, input);\n        termAtt = addAttribute(CharTermAttribute.class);\n        typeAtt = addAttribute(TypeAttribute.class);\n    }\n\n    /**\n     * Is called during tokenization for each token produced by {@link ClassicTokenizer}. Subclasses can call {@link #setTerm(String)} to override\n     * current token or {@link #setTerms(Iterator)} if current token needs to be split into more than one token.\n     *\n     * @return true whether next token exists false otherwise.\n     * @see #getTerm()\n     * @see #getType()\n     * @see #setTerm(String)\n     * @see #setTerms(Iterator)\n     */\n    protected abstract boolean onNextToken();\n\n    /**\n     * Subclasses can call this method during execution of {@link #onNextToken()} to retrieve current term.\n     *\n     * @return current term.\n     * @see #getType()\n     * @see #setTerm(String)\n     * @see #setTerms(Iterator)\n     * @see #onNextToken()\n     */\n    protected String getTerm() {\n        return new String(termAtt.buffer(), 0, termAtt.length());\n    }\n\n    /**\n     * Subclasses can call this method during execution of {@link #onNextToken()} to retrieve type of current term.\n     *\n     * @return type of current term.\n     * @see #getTerm()\n     * @see #setTerm(String)\n     * @see #setTerms(Iterator)\n     * @see #onNextToken()\n     */\n    protected String getType() {\n        return typeAtt.type();\n    }\n\n    /**\n     * Subclasses can call this method during execution of {@link #onNextToken()} to override current term.\n     *\n     * @param term the term to override with.\n     * @see #getTerm()\n     * @see #getType()\n     * @see #setTerms(Iterator) setTerms(Iterator) - if you want to override current term with more than one term\n     * @see #onNextToken()\n     */\n    protected void setTerm(String term) {\n        termAtt.copyBuffer(term.toCharArray(), 0, term.length());\n    }\n\n    /**\n     * Subclasses can call this method during execution of {@link #onNextToken()} to override current term with more than one term.\n     *\n     * @param terms the terms to override with.\n     * @see #getTerm()\n     * @see #getType()\n     * @see #setTerm(String)\n     * @see #onNextToken()\n     */\n    protected void setTerms(Iterator&lt;String&gt; terms) {\n        setTerm(terms.next());\n        pendingTokens = terms;\n    }\n\n    @Override\n    public final boolean incrementToken() throws IOException {\n        if (pendingTokens.hasNext()) {\n            setTerm(pendingTokens.next());\n            return true;\n        }\n\n        clearAttributes();\n        if (!classicTokenizer.incrementToken()) {\n            return false;\n        }\n\n        typeAtt.setType(classicTokenizer.getAttribute(TypeAttribute.class).type());        // copy type attribute from classic tokenizer attribute\n\n        CharTermAttribute stTermAtt = classicTokenizer.getAttribute(CharTermAttribute.class);\n        setTerm(new String(stTermAtt.buffer(), 0, stTermAtt.length()));\n\n        return onNextToken();\n    }\n\n    @Override\n    public void close() throws IOException {\n        super.close();\n        if (input != null) {\n            input.close();\n        }\n        classicTokenizer.close();\n    }\n\n    @Override\n    public void end() throws IOException {\n        super.end();\n        classicTokenizer.end();\n    }\n\n    @Override\n    public void reset() throws IOException {\n        super.reset();\n        this.classicTokenizer.setReader(input);        // important! input has to be carried over to delegate because of poor design of Lucene\n        classicTokenizer.reset();\n    }\n}\n
168	curl -XGET 'localhost:9200/test/_analyze?analyzer=standard&amp;pretty' -d 'Capa Bumper Iphone 5C' | grep token\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   532  100   511  100    21  32800   1347 --:--:-- --:--:-- --:--:-- 34066\n  "tokens" : [ {\n    "token" : "capa",\n    "token" : "bumper",\n    "token" : "iphone",\n    "token" : "5c",\n
169	8           Terms termVector = reader.getTermVector(docNum, "contents");\n9           TermsEnum itr = termVector.iterator();\n10          BytesRef term = null;\n11          PostingsEnum postings = null;\n12          while((term = itr.next()) != null){\n13              try{\n14                  String termText = term.utf8ToString();\n15                  postings = itr.postings(postings, PostingsEnum.FREQS);\n16                  int freq = postings.freq();\n17\n18\n19                  System.out.println("doc:" + docNum + ", term: " + termText + ", termFreq = " + freq);\n20              } catch(Exception e){\n21                  System.out.println(e);\n22              }\n23          }\n
170	Query q = .... // pass the user input to the QueryParser or similar\nTopDocs hits = searcher.search(query, 10); // Get 10 results\n
171	Tokenizer tokenizer = new StandardTokenizer(Version.LUCENE_48, reader);\nTokenStream result = new SynonymFilter(tokenizer, smap, true);\nreturn new TokenStreamComponents(tokenizer, result);\n
172	GET my_index/_search?source_content_type=application/json&amp;source={"query":{"geo_bounding_box":{"location":{"top_left":{"lat":42,"lon":-72},"bottom_right":{"lat":40,"lon":-74}}}}}\n
173	public void search() throws IOException {\n        System.out\n                .println("\nLatLonQuery around given point, 10km radius --------------------------------------");\n        Query distQuery = LatLonPoint.newDistanceQuery("latlon", -6.08165, 145.8612430, dist * 1000);\n        long startTime=0;//adjust according to your needs\n        long endTime=Long.Max_VALUE;//adjust according to your needs\n        Query timeQuery = LongPoint.newRangeQuery("timestamp", startTime, endTime);\n\n        BooleanQuery booleanQuery = new BooleanQuery.Builder()\n            .add(distQuery, Occur.MUST)\n            .add(timeQuery, Occur.MUST)\n            .build();\n        TopDocs docs = searcher.search(booleanQuery, 20);\n        for (ScoreDoc scoreDoc : docs.scoreDocs) {\n            Document doc = searcher.doc(scoreDoc.doc);\n            System.out.println(doc);\n        }\n    }\n
174	var hits = searcher.Search(query);\nvar result = hits.Id(0);\n
175	public class Catalog_ProductMetadata\n{\n\n    [Field(FieldIndex.Tokenized, FieldStore.Yes, IsKey = true)]\n    public object ProductId { get; set; }\n\n    [Field(FieldIndex.Tokenized, FieldStore.Yes, IsDefault = true)]\n    public object Name { get; set; }\n\n    [Field(FieldIndex.Tokenized, FieldStore.Yes)]\n    public object Description { get; set; }\n\n    [Field(FieldIndex.Tokenized, FieldStore.Yes)]\n    public object Breadcrumb { get; set; }\n\n    [Field(FieldIndex.Tokenized, FieldStore.Yes)]\n    public object Tab1Content { get; set; }\n\n    [Field(FieldIndex.Tokenized, FieldStore.Yes)]\n    public object Tab2Content { get; set; }\n\n    [Field(FieldIndex.Tokenized, FieldStore.Yes)]\n    public object Tab3Content { get; set; }\n\n    [Field(FieldIndex.Tokenized, FieldStore.Yes)]\n    public object Tab4Content { get; set; }\n\n    [Field(FieldIndex.Tokenized, FieldStore.Yes)]\n    public object Tab5Content { get; set; }\n\n    [Field(FieldIndex.Tokenized, FieldStore.Yes)]\n    public object Manufacturer { get; set; }\n\n}\n
176	            HashSet&lt;Long&gt; foundHashes = new HashSet&lt;Long&gt;();\n\n            LinkedList&lt;String&gt; words = new LinkedList&lt;String&gt;();\n            for(int i=0; i&lt;params.maxPhrase; i++) words.addLast("");\n\n            StandardTokenizer st = new StandardTokenizer(new StringReader(docText));\n            Token t = new Token();\n            while(st.next(t) != null) {\n                String token = new String(t.termBuffer(), 0, t.termLength());\n                words.addLast(token);\n                words.removeFirst();\n\n                for(int len=params.minPhrase; len&lt;params.maxPhrase; len++) {\n                    String term = Utils.join(new ArrayList&lt;String&gt;(words.subList(params.maxPhrase-len,params.maxPhrase)), " ");\n\n                    long hash = Utils.longHash(term);\n\n                    if(params.lexicon.isTermHash(hash)) {\n                        foundHashes.add(hash);\n                    }\n                }\n            }\n\n            for(long hash : foundHashes) {\n                if(count.containsKey(hash)) {\n                    count.put(hash, count.get(hash) + 1);\n                } else {\n                    count.put(hash, 1);\n                }\n            }\n
177	Field f = new Field(Name, StringValue, Store, Analyzed, TV);\nf.SetTokenStream(TokenStreamValue);\n
178	term1 = new TermQuery(new Term(...));\nbooleanQuery.add(term1, BooleanClause.Occur.MUST);    \n\nterm2 = new TermQuery(new Term(...));\nbooleanQuery.add(term2, BooleanClause.Occur.MUST);\n\nterm3 = new TermQuery(new Term(...));\nbooleanQuery.add(term3, BooleanClause.Occur.MUST);\n
179	@Field(name = "coursestatus", index = Index.TOKENIZED, store = Store.YES)\n
180	&lt;system-properties&gt;\n    &lt;property name="os.version" value="1.0.GAE whatever" /&gt;\n    &lt;property name="os.arch" value="GAE whatever" /&gt;\n&lt;/system-properties&gt;\n
181	lindex = SimpleFSDirectory(File(indexdir))\nireader = IndexReader.open(lindex, True)\n# Query the lucene index for the terms starting at a term named "field_name"\nterms = ireader.terms(Term("field_name", "")) #Start at the field "field_name"\nfacets = {'other': 0}\nwhile terms.next():\n    if terms.term().field() != "field_name":  #We've got every value\n        break\n    print "Field Name:", terms.term().field()\n    print "Field Value:", terms.term().text()\n    print "Matching Docs:", int(ireader.docFreq(term))\n
182	IndexWriter iw = new IndexWriter(@"C:\temp\sotests", new StandardAnalyzer(Lucene.Net.Util.Version.LUCENE_29), true);\n\nDocument doc = new Document();\nField loc = new Field("location", "", Field.Store.YES, Field.Index.NOT_ANALYZED);\ndoc.Add(loc);\n\nloc.SetValue("chicago heights");\niw.AddDocument(doc);\n\nloc.SetValue("new-york");\niw.AddDocument(doc);\n\nloc.SetValue("chicago low");\niw.AddDocument(doc);\n\nloc.SetValue("montreal");\niw.AddDocument(doc);\n\nloc.SetValue("paris");\niw.AddDocument(doc);\n\niw.Commit();\n\n\nIndexSearcher ins = new IndexSearcher(iw.GetReader());\n\nWildcardQuery query = new WildcardQuery(new Term("location", "chicago he*"));\n\nvar hits = ins.Search(query);\n\nfor (int i = 0; i &lt; hits.Length(); i++)\n    Console.WriteLine(hits.Doc(i).GetField("location").StringValue());\n\nConsole.WriteLine("---");\n\nquery = new WildcardQuery(new Term("location", "chic*"));\nhits = ins.Search(query);\n\nfor (int i = 0; i &lt; hits.Length(); i++)\n    Console.WriteLine(hits.Doc(i).GetField("location").StringValue());\n\niw.Close();\nConsole.ReadLine();\n
183	TopDocs results = searcher.search(query, 10);\n
184	string input = "concatenated string";\nreturn HttpUtility.HtmlDecode(Regex.Replace(input, "&lt;[^&gt;]*&gt;", string.Empty));\n
185	FSDirectory index = FSDirectory.open(Paths.get("C:\\temp\\index.lucene"));\n
186	START n=node:&lt;indexName&gt;(&lt;lucene query expression&gt;) // index query\nSTART n=node:&lt;indexName&gt;(key='&lt;value&gt;') // exact index lookup\n
187	Sort sort = new Sort(new SortedNumericSortField("age", SortField.Type.LONG, true));\nTopDocs docs = searcher.search(new MatchAllDocsQuery(), 100, sort);\n
188	String text = "Pubblichiamo la presentazione di IBM riguardante DB2 per i vari sistemi operativi"\n    +"Linux, UNIX e Windows.\r\n\r\nQuesto documento sta sulla piattaforma KM e lo potete"\n    +"scaricare a questo &lt;a href=\'https://sfkm.griffon.local/sites/BSF%20KM/BSF/CC%20T/Specifiche/Eventi2008/IBM%20DB2%20for%20Linux,%20UNIX%20e%20Windows.pdf\' target=blank&gt;link&lt;/a&gt;.";\n\nTextExtractor te = new TextExtractor(new Source(text)){\n    @Override\n    public boolean excludeElement(StartTag startTag) {\n        return startTag.getName() != HTMLElementName.A;\n    }\n};\nSystem.out.println(te.toString());\n
189	private int index(File indexDir, File dataDir, String suffix) throws Exception {\n    RAMDirectory ramDir = new RAMDirectory();          // 1\n    IndexWriter indexWriter = new IndexWriter(\n            ramDir,                                    // 2\n            new SimpleAnalyzer(),\n            true,\n            IndexWriter.MaxFieldLength.LIMITED);\n    indexWriter.setUseCompoundFile(false);\n    indexDirectory(indexWriter, dataDir, suffix);\n    int numIndexed = indexWriter.maxDoc();\n    indexWriter.optimize();\n    indexWriter.close();\n\n    Directory.copy(ramDir, FSDirectory.open(indexDir), false); // 3\n\n    return numIndexed;\n}\n
190	def results = Product.search {\n  must(term('$/Product/category/name', params.categoryName))\n  must(queryString(params.q))\n}\n
191	@Field(index = Index.NO, store = Store.YES)\n@FieldBridge(impl = MyFieldBridge.class)\nprivate byte[] file;\n\n\n//The file bridge\npublic class MyFieldBridge implements StringBridge {\n\n    @Override\n    public String objectToString(final Object object) {\n\n        byte[] file = (byte[]) object;\n\n        return MagicFileUtil.getTextContent(file)\n    }\n}\n
192	PrefixQuery pquery = new PrefixQuery(new Term("title", "special"));\npquery.setRewriteMethod(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE);\n
193	&lt;copyField source="*" dest="text"/&gt;\n
194	&lt;dataConfig&gt;\n    &lt;dataSource name="Sample" driver="org.postgresql.Driver" url="jdbc:postgresql://localhost:5432/Sample" user="postgres" password="P@ssword" /&gt;\n    &lt;document name="oneDocs"&gt;\n       &lt;entity dataSource="Sample" name="entity1" query="select FirstName from Employee"&gt;\n            &lt;field column="EmpId" name="EmpId" /&gt;\n            &lt;field column="FirstName" name="FirstName" /&gt;\n            &lt;field column="LastName" name="LastName" /&gt;\n        &lt;/entity&gt;\n    &lt;/document&gt;\n&lt;/dataConfig&gt;\n
195	&lt;fieldType name="phonetics" class="solr.TextField" positionIncrementGap="100" multiValued="true"&gt;\n    &lt;analyzer type="index"&gt;         \n        &lt;filter class="solr.TrimFilterFactory"/&gt;        \n        &lt;filter class="solr.NGramFilterFactory" minGramSize="2" maxGramSize="1000" /&gt;\n        &lt;filter class="solr.EdgeNGramFilterFactory" minGramSize="2" maxGramSize="1000"  /&gt;\n        &lt;filter class="solr.WordDelimiterFilterFactory" splitOnCaseChange="1" splitOnNumerics="0" \n        generateWordParts="1" stemEnglishPossessive="0" generateNumberParts="0"\n        catenateWords="1" catenateNumbers="0" catenateAll="0" preserveOriginal="1"/&gt;\n        &lt;filter class="solr.LowerCaseFilterFactory"/&gt;\n        &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;        \n        &lt;filter class="solr.DoubleMetaphoneFilterFactory" inject="true"/&gt;\n        &lt;filter class="solr.RemoveDuplicatesTokenFilterFactory"/&gt;\n    &lt;/analyzer&gt;\n    &lt;analyzer type="query"&gt;     \n        &lt;filter class="solr.TrimFilterFactory"/&gt;        \n        &lt;filter class="solr.WordDelimiterFilterFactory" splitOnCaseChange="1" splitOnNumerics="0" \n        generateWordParts="1" stemEnglishPossessive="0" generateNumberParts="0"\n        catenateWords="1" catenateNumbers="0" catenateAll="0" preserveOriginal="1"/&gt;        \n        &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;        \n        &lt;filter class="solr.LowerCaseFilterFactory"/&gt;       \n        &lt;filter class="solr.DoubleMetaphoneFilterFactory" inject="true"/&gt;\n    &lt;/analyzer&gt;\n&lt;/fieldType&gt;\n
196	StoredField strField = new StoredField("id", bag.getId());\n
197	    * SOLR_PID_DIR=/var/solr\n    * SOLR_HOME=/var/solr/data\n    * LOG4J_PROPS=/var/solr/log4j.properties\n    * SOLR_LOGS_DIR=/var/solr/logs\n    * SOLR_PORT=8983\n
198	@Entity\npublic class Staff {\n    @Id\n    @Field(name="id_numeric")\n    @NumericField(forField="id_numeric")\n    protected Long id;\n    // other fields\n}\n
199	&lt;field name="inStock" type="boolean" indexed="true" stored="true" /&gt;\n
200	Sort sort = new Sort(new SortField("name"));\nsearchQuery.setSort(sort);\nList results = searchQuery.list();\n
201	Query query = MultiFieldQueryParser.parse(Version.LUCENE_30, new String[] {"harry potter","harry potter","harry potter"},   new String[] {"title","author","content"},new SimpleAnalyzer());\nIndexSearcher searcher = new IndexSearcher(...);\nHits hits = searcher.search(query);\n
202	String dateString = DateTools.dateToString(date, Resolution.DAY);\nluceneDoc.add(new Field(key, dateString, Store.YES, Index.NOT_ANALYZED));\n
203	class MyFieldSelector : FieldSelector\n{\n    public FieldSelectorResult Accept(string fieldName)\n    {\n        if (fieldName == "field1") return FieldSelectorResult.LOAD_AND_BREAK;\n        return FieldSelectorResult.NO_LOAD;\n    }\n}\n
204	&lt;fieldType name="text_general_shingle" class="solr.TextField" positionIncrementGap="100"&gt;     \n        &lt;analyzer&gt;\n       &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;\n        &lt;filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt" /&gt;       \n        &lt;filter class="solr.LowerCaseFilterFactory"/&gt;           \n        &lt;filter class="solr.ShingleFilterFactory" maxShingleSize="3" outputUnigrams="true"/&gt;\n        &lt;filter class="solr.PatternReplaceFilterFactory" pattern=".*_.*" replacement=""/&gt;       \n    &lt;/analyzer&gt;     \n    &lt;/fieldType&gt;\n
205	            &lt;plugin&gt;\n                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;\n                &lt;executions&gt;\n                    &lt;execution&gt;\n                        &lt;phase&gt;package&lt;/phase&gt;\n                        &lt;goals&gt;\n                            &lt;goal&gt;shade&lt;/goal&gt;\n                        &lt;/goals&gt;\n                        &lt;configuration&gt;\n                            &lt;transformers&gt;\n                                &lt;transformer implementation="org.apache.maven.plugins.shade.resource.AppendingTransformer"&gt;\n                                    &lt;resource&gt;META-INF/services/org.apache.lucene.codecs.Codec&lt;/resource&gt;\n                                &lt;/transformer&gt;\n                                &lt;transformer implementation="org.apache.maven.plugins.shade.resource.AppendingTransformer"&gt;\n                                    &lt;resource&gt;META-INF/services/org.apache.lucene.codecs.DocValuesFormat&lt;/resource&gt;\n                                &lt;/transformer&gt;\n                                &lt;transformer implementation="org.apache.maven.plugins.shade.resource.AppendingTransformer"&gt;\n                                    &lt;resource&gt;META-INF/services/org.apache.lucene.codecs.PostingsFormat&lt;/resource&gt;\n                                &lt;/transformer&gt;\n                            &lt;/transformers&gt;\n                         &lt;shadedArtifactAttached&gt;true&lt;/shadedArtifactAttached&gt;\n                            &lt;shadedClassifierName&gt;fat&lt;/shadedClassifierName&gt;\n                        &lt;/configuration&gt;\n                    &lt;/execution&gt;\n                &lt;/executions&gt;\n            &lt;/plugin&gt;\n
206	Fluently.Configure()\n  .Database(...)\n  .Mappings(...)\n  .ExposeConfiguration(cfg =&gt;\n  {\n    cfg.SetListener(...);\n    cfg.SetListener(...);\n  })\n  .BuildSessionFactory();\n
207	public TokenStream tokenStream(String fieldName, Reader reader) {\n    TokenStream result = super.tokenStream(fieldName, reader);\n    result = new StopFilter(result, stopSet);\n    return result;\n}\n
208	TopDocs topDocs = searcher.search(query, filter, 1);\n
209	Query query = new QueryParser(LUCENE_VERSION, "url", analyzer).newTermQuery(new Term("url", url)).parse(url);\n
210	public class HitPositionCollector implements Formatter\n{\n    // MatchOffset is a simple DTO\n    private List&lt;MatchOffset&gt; matchList;\n    public HitPositionCollector(\n    {\n        matchList = new ArrayList&lt;MatchOffset&gt;();\n    }\n\n    // this ie where the term start and end offset as well as the actual term is captured\n    @Override\n    public String highlightTerm(String originalText, TokenGroup tokenGroup)\n    {\n        if (tokenGroup.getTotalScore() &lt;= 0)\n        {\n        }\n        else\n        {\n            MatchOffset mo= new MatchOffset(tokenGroup.getToken(0).toString(), tokenGroup.getStartOffset(),tokenGroup.getEndOffset());\n            getMatchList().add(mo);\n        }\n\n        return originalText;\n    }\n\n    /**\n    * @return the matchList\n    */\n    public List&lt;MatchOffset&gt; getMatchList()\n    {\n        return matchList;\n    }\n}\n
211	public final class YourAnalyzer extends ReusableAnalyzerBase { \n\n  @Override\n  protected TokenStreamComponents createComponents(final String fieldName, final Reader reader) {\n    final TokenStream source = new KeywordTokenizer(reader);\n    return new TokenStreamComponents(source, new LowercaseFilter(Version.LUCENE_36, source));\n  }\n}\n
212	IndexReader r1= IndexReader.open(...)\nIndexReader r2= IndexReader.open(...)\nMultiReader multiReader = new MultiReader(r1, r2);\nIndexSearcher searcher = new IndexSearcher(multiReader);\n
213	 protected override bool IsAncestorOf(Item item)\n {\n   bool result;\n   using (new SecurityDisabler())\n   {\n     using (new CachesDisabler())\n     {\n       //result = item.Axes.IsAncestorOf(item);\n       result = this.RootItem.Axes.IsAncestorOf(item);\n     }\n   }\n   return result;\n }\n
214	&lt;similarity class="org.apache.lucene.search.similarities.BM25Similarity" /&gt;\n
215	static final String INDEX_DIRECTORY = "/home/yuqing/Desktop/index";\nDirectory index = FSDirectory.open(Paths.get(INDEX_DIRECTORY));\n
216	D=sqrt((x2-x1)^2+(y2-y1)^2+...+(n2-n1)^2)\n
217	&lt;fieldType name="text_path" class="solr.TextField" positionIncrementGap="100"&gt;\n    &lt;analyzer type="index"&gt;\n        &lt;tokenizer class="solr.PathHierarchyTokenizerFactory" delimiter="/" /&gt;\n    &lt;/analyzer&gt;\n&lt;/fieldType&gt;\n
218	MoreLikeThis mlt = new MoreLikeThis(reader); // Pass the index reader\nmlt.setFieldNames(new String[] {"title", "author"}); // specify the fields for similiarity\n\nQuery query = mlt.like(docID); // Pass the doc id \nTopDocs similarDocs = searcher.search(query, 10); // Use the searcher\nif (similarDocs.totalHits == 0)\n    // Do handling\n}\n
219	charset_table     = 0..9, A..Z-&gt;a..z, _, a..z, U+23, U+410..U+42F-&gt;U+430..U+44F, U+430..U+44F\n
220	TopDocs topDocs = searcher.search(query, Integer.MAX_VALUE);\n
221	&lt;fieldType name="text_en_splitting" class="solr.TextField" positionIncrementGap="100" autoGeneratePhraseQueries="true"&gt;\n  &lt;!-- Index and Query time --&gt;\n  &lt;analyzer type="index"&gt;\n    &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;\n    &lt;filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" catenateAll="0" splitOnCaseChange="1"/&gt;\n    &lt;filter class="solr.LowerCaseFilterFactory"/&gt;\n        &lt;!-- Stemmer --&gt;\n    &lt;filter class="solr.PorterStemFilterFactory"/&gt;\n  &lt;/analyzer&gt;\n  &lt;analyzer type="query"&gt;\n    &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;\n    &lt;filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="0" catenateNumbers="0" catenateAll="0" splitOnCaseChange="1"/&gt;\n    &lt;filter class="solr.LowerCaseFilterFactory"/&gt;\n    &lt;filter class="solr.PorterStemFilterFactory"/&gt;\n  &lt;/analyzer&gt;\n&lt;/fieldType&gt;\n
222	TermQuery tq1 = new TermQuery(new Term("text", "term1"));\ntq1.setBoost(5f);\nTermQuery tq2 = new TermQuery(new Term("text", "term2"));\ntq2.setBoost(0.8f);\nBooleanQuery query = new BooleanQuery();\nquery.add(tq1, Occur.SHOULD);\nquery.add(tq2, Occur.SHOULD);\n
223	$index = new Zend_Search_Lucene_Interface_MultiSearcher();\n$index-&gt;addIndex(Zend_Search_Lucene::open('search/index1'));\n$index-&gt;addIndex(Zend_Search_Lucene::open('search/index2'));\n$index-&gt;find('someSearchQuery');\n
224	    List&lt;String&gt; valuesList= new ArrayList&lt;String&gt;();\n            for (SearchHit hit : response.getHits()) {                      \n    result.add(hit.getSource());\n    valuesList.add(hit.getSource().get("name").toString());         \n}\n
225	http://solr:8983/solr/select?q=hp%20laptop&amp;_val_="product(score,popularityrank)"\n
226	{\n  "size": 3,\n  "query": {\n    "bool": {\n      "filter": {\n        "simple_query_string": {\n          "query": "source:\"source.type:=\"type_a\"\""\n        }\n      }\n    }\n  }\n}\n
227	query = queryparser.parse(queryparser.escape(querytext.replace("AND OR", "AND or")))\n
228	private static void addDoc(IndexWriter writer, String content) throws IOException {\n    FieldType fieldType = new FieldType();\n    fieldType.setStoreTermVectors(true);\n    fieldType.setStoreTermVectorPositions(true);\n    fieldType.setIndexed(true);\n    fieldType.setIndexOptions(IndexOptions.DOCS_AND_FREQS);\n    fieldType.setStored(true);\n    Document doc = new Document();\n    doc.add(new Field("content", content, fieldType));\n    writer.addDocument(doc);\n}\n\npublic static void main(String[] args) throws IOException, ParseException {\n    Directory directory = new RAMDirectory();  \n    Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_40);\n    IndexWriterConfig config = new IndexWriterConfig(Version.LUCENE_40, analyzer);\n    IndexWriter writer = new IndexWriter(directory, config);\n    addDoc(writer, "bla bla bla bleu bleu");\n    addDoc(writer, "bla bla bla bla");\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(directory);\n    DocsEnum de = MultiFields.getTermDocsEnum(reader, MultiFields.getLiveDocs(reader), "content", new BytesRef("bla"));\n    int doc;\n    while((doc = de.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          System.out.println(de.freq());\n    }\n    reader.close();\n}\n
229	new CustomScoreQuery(bigramQuery, new FieldScoreQuery("bigram-count", Type.BYTE)) {\n  protected CustomScoreProvider getCustomScoreProvider(IndexReader ir) {\n    return new CustomScoreProvider(ir) {\n      public double customScore(int docnum, float bigramFreq, float docBigramCount) {\n         ... calculate Dice's coefficient using bigramFreq and docBigramCount...\n         if (diceCoeff &gt;= threshold) {\n           String[] stems = ir.document(docnum).getValues("stems");\n           ... calculate document similarity score using stems ...\n         }\n      }\n    };\n  }\n}\n
230	// todo: probably some thread safety\npublic class AccessFilterFactory\n{\n    private static AccessFilterFactory _instance = new AccessFilterFactory();;\n    private AccessFilterFactory()\n    {\n    }\n\n    public AccessFilterFactory Instance\n    {\n        get\n        {\n            return _instance;\n        }\n    }\n\n    private Cache&lt;int, Filter&gt; someKindaCache = new Cache&lt;int, Filter&gt; ();\n\n    // gets a cached filter if already built, if not it creates one\n    // caches it and returns it\n    public Filter GetFilterForUser(int userId)\n    {\n        // return from cache if you got it\n        if(someKindaCache.Exists(userId))\n            return someKindaCache.Get(userId);\n\n        // if not, build and cache it\n        BooleanQuery filterQuery = new BooleanQuery();\n        foreach(string id in ids)\n        {\n            filterQuery.Add(new TermQuery(new Term("EmId", id)),  BooleanClause.Occur.SHOULD);\n        }\n        Filter cachingFilter = new CachingWrapperFilter(new QueryWrapperFilter(filterQuery));\n        someKindaCache.Put(userId, cachingFilter);\n        return cachingFilter;\n    }\n\n    // removes a new invalid filter from cache (permissions changed)\n    public void InvalidateFilter(int userId)\n    {\n        someKindaCache.Remove(userId);\n    }   \n}\n
231	 &lt; str name="q.alt"&gt;*:*&lt; /str&gt;\n
232	val field0201 = new Field("content-high", "This is a test: Linux", ...)\nfield0201.setBoost(1.5f)\nval field0202 = new Field("content-low", "This is a test: Windows", ...)\n
233	require 'java'\nrequire 'lucene-core.jar'\n\njava_import 'org.apache.lucene.analysis.Analyzer'\njava_import 'org.apache.lucene.analysis.standard.StandardTokenizer'\njava_import 'org.apache.lucene.util.Version'\njava_import 'org.apache.lucene.analysis.TokenStream'\njava_import 'java.io.Reader'\n\nclass MyAnalyzer &lt; Analyzer\n\n  def tokenStream(file_name, reader) \n     result = StandardTokenizer.new(Version::LUCENE_CURRENT, reader)\n      # ...\n  end \nend\n
234	public static void main(String[] args) throws IOException, ParseException {\n    IndexWriter luceneIndexWriter = new IndexWriter(\n            FSDirectory.open(new File("/tmp/test")), createWriterConfig(64));\n    Document doc1 = createDocument(ID1, "context1", 1);\n    luceneIndexWriter.addDocument(doc1);\n    Document doc2 = createDocument(ID2, "context2", 2);\n    luceneIndexWriter.addDocument(doc2);\n\n    System.out.println("Found doc1: "\n            + findDocument(ID1, 1, luceneIndexWriter));\n    System.out.println("Found doc2: "\n            + findDocument(ID2, 2, luceneIndexWriter));\n    doc1 = findDocument(ID1, 1, luceneIndexWriter);\n\n    // Section 1\n    doc1.removeField(CONTEXT_FIELD);\n    doc1.add(new TextField(CONTEXT_FIELD, "context1_changed",\n            Field.Store.YES));\n\n    //re-adding the IntField here\n    Number num = doc1.getField(NUMBER_OF_ARGUMENTS).numericValue();\n    doc1.removeField(NUMBER_OF_ARGUMENTS);\n    doc1.add(new IntField(NUMBER_OF_ARGUMENTS, num.intValue(),\n            Field.Store.YES));\n\n    luceneIndexWriter.updateDocument(new Term(METHOD_NAME_FIELD, "text"),\n            doc1);\n\n    System.out.println("Found doc1: "\n            + findDocument(ID1, 1, luceneIndexWriter));\n    System.out.println("Found doc2: "\n            + findDocument(ID2, 2, luceneIndexWriter));\n\n    // Section 2\n    doc1 = findDocument(ID1, 1, luceneIndexWriter);\n    doc1.removeField(CONTEXT_FIELD);\n    doc1.add(new TextField(CONTEXT_FIELD, "context1_changed2",\n            Field.Store.YES));\n    luceneIndexWriter.updateDocument(new Term(METHOD_NAME_FIELD, "text"),\n            doc1);\n    num = doc1.getField(NUMBER_OF_ARGUMENTS).numericValue();\n    doc1.removeField(NUMBER_OF_ARGUMENTS);\n    doc1.add(new IntField(NUMBER_OF_ARGUMENTS, num.intValue(),\n            Field.Store.YES));\n    luceneIndexWriter.updateDocument(new Term(METHOD_NAME_FIELD, "text"),\n            doc1);\n\n    System.out.println("Found doc1: "\n            + findDocument(ID1, 1, luceneIndexWriter));\n    System.out.println("Found doc2: "\n            + findDocument(ID2, 2, luceneIndexWriter));\n\n    luceneIndexWriter.close();\n}\n
235	Query query = queryparser.parse("central -duplicate:true");\n
236	cc -fno-strict-aliasing -fno-common -dynamic -arch x86_64 -arch i386 -g -Os -pipe -fno-common -fno-strict-aliasing -fwrapv -DENABLE_DTRACE -DMACOSX -    DNDEBUG -Wall -Wstrict-prototypes -Wshorten-64-to-32 -DNDEBUG -g -fwrapv -Os -Wall -Wstrict-prototypes -DENABLE_DTRACE -dynamiclib -D_jcc_lib -DJCC_VER="2.    21" -I/Library/Java/JavaVirtualMachines/jdk1.7.0_45.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.7.0_45.    jdk/Contents/Home/include/darwin -I_jcc -Ijcc/sources -I/System/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -c jcc/sources/JCCEnv.    cpp -o build/temp.macosx-10.10-intel-2.7/jcc/sources/JCCEnv.o -DPYTHON -fno-strict-aliasing -Wno-write-strings\nclang: warning: argument unused during compilation: '-dynamiclib'\nc++ -Wl,-x -dynamiclib -undefined dynamic_lookup build/temp.macosx-10.10-intel-2.7/jcc/sources/jcc.o build/temp.macosx-10.10-intel-2.7/jcc/sources/JCCEnv.    o -o build/lib.macosx-10.10-intel-2.7/libjcc.dylib -L/Library/Java/JavaVirtualMachines/jdk1.7.0_45.jdk/Contents/Home/jre/lib -ljava -    L/Library/Java/JavaVirtualMachines/jdk1.7.0_45.jdk/Contents/Home/jre/lib/server -ljvm -Wl,-rpath -Wl,/Library/Java/JavaVirtualMachines/jdk1.7.0_45.    jdk/Contents/Home/jre/lib -Wl,-rpath -Wl,/Library/Java/JavaVirtualMachines/jdk1.7.0_45.jdk/Contents/Home/jre/lib/server -Wl,-S -install_name @rpath/libjcc.    dylib -current_version 2.21 -compatibility_version 2.21\n
237	SortField[] sortFiled = new SortField[2];\nsortFiled[0] = new SortField("name", SortField.Type.STRING);\nsortFiled[1] = new SortField("country", CaseInsensitiveStringComparator());\n
238	fq=f2:["2011-03-21" TO "2012-03-21"]\n
239	double distDEG = sc.getDistCalc().distance(args.getShape().getCenter(), lo1, la1);\ndouble distKM = DistanceUtils.degrees2Dist(distDEG, DistanceUtils.EARTH_MEAN_RADIUS_KM);\n
240	for (String field: fields){\n    Query query = new WildcardQuery(new Term(field, "*alue1"));\n    Explanation ex = searcher.explain(query, docID);\n    if (ex.isMatch()){\n        //Your query matched field\n    }\n}\n
241	public final class MyAnalyzer {\n    @Override\n    protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        final StandardTokenizer src = new StandardTokenizer(matchVersion, reader);\n        TokenStream tok = new StandardFilter(matchVersion, src);\n        tok = new LowerCaseFilter(matchVersion, tok);\n        tok = new StopFilter(matchVersion, tok, StopAnalyzer.ENGLISH_STOP_WORDS_SET);\n        return new TokenStreamComponents(src, tok);\n    }\n\n    @Override\n    protected Reader initReader(String fieldName, Reader reader) {\n        //return your CharFilter-wrapped reader here\n    }\n}\n
242	var lowerRange = new TermRangeQuery("StartDate", null, upperDate, true, true);\nvar upperRange = new TermRangeQuery("EndDate", lowerDate, null, true, true);\n
243	q=solr&amp;facet=true\n&amp;facet.date=j_createddate\n&amp;facet.date.gap=+1DAY,+2DAY,+3DAY,+10DAY\n
244	public class MyAnalyzer extends Analyzer {\n\npublic MyAnalyzer() {\n    super();\n}\n\npublic TokenStream tokenStream(String fieldName, Reader reader) {\n    TokenStream result = new MyUrlTokenizer(reader);\n    result = new LowerCaseFilter(result);\n    result = new StopFilter(result);\n    result = new SynonymFilter(result);\n\n    return result;\n}\n
245	function(doc) {\n  var result = new Document();\n  for(var i in doc.$tags) {\n    result.add(doc.$tags[i]);\n  }\n  return result;\n}\n
246	  case MINUS:\n    jj_consume_token(MINUS);\n             ret = MOD_NOT;\n    break;\n  case NOT:\n    jj_consume_token(NOT);\n           ret = MOD_NOT;\n    break;\n
247	QueryParser parser = new QueryParser("Text", analyzer);\nparser.SetAllowLeadingWildcard(true);\nvar escapedSearchText = QueryParser.Escape(searchText);\nreturn parser.Parse(string.Format("*{0}*", escapedSearchText));\n
248	public class CustomQueue : Lucene.Net.Util.PriorityQueue&lt;Document&gt;\n{\n    public CustomQueue(int maxSize): base()\n    {\n        Initialize(maxSize);\n    }\n\n    public override bool LessThan(Document a, Document b)\n    {\n        //a.GetField("field1")\n        //b.GetField("field2");\n        return  //compare a &amp; b\n    }\n}\n\npublic class MyCollector : Lucene.Net.Search.Collector\n{\n    CustomQueue _queue = null;\n    IndexReader _currentReader;\n\n    public MyCollector(int maxSize)\n    {\n        _queue = new CustomQueue(maxSize);\n    }\n\n    public override bool AcceptsDocsOutOfOrder()\n    {\n        return true;\n    }\n\n    public override void Collect(int doc)\n    {\n        _queue.InsertWithOverflow(_currentReader.Document(doc));\n    }\n\n    public override void SetNextReader(IndexReader reader, int docBase)\n    {\n        _currentReader = reader;\n    }\n\n    public override void SetScorer(Scorer scorer)\n    {\n    }\n}\n
249	&lt;fieldType name="location_rpt" class="solr.SpatialRecursivePrefixTreeFieldType"\n           spatialContextFactory="com.spatial4j.core.context.jts.JtsSpatialContextFactory"\n           distErrPct="0.025"\n           maxDistErr="0.000009"\n           units="degrees" /&gt;\n
250	BooleanFilter f = new BooleanFilter();\nfor (String unwanted: unwantedCategories) {\n    TermsFilter tf = new TermsFilter(new Term("category", unwanted));\n    f.add(new FilterClause(tf, BooleanClause.MUST_NOT));\n}\n
251	TrackingIndexWriter writer; // your writer\nSearcherFactory factory = new SearcherFactory();\nNRTManager mgr = new NRTManager(writer, factory);\n
252	stats=true&amp;stats.field=total_amount\n
253	PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n
254	CharArraySet stopWordsSet;\n\ntry {\n    // use customized stop words list\n    String stopWordsDictionary = FileUtils.readFileToString(new File(%PATH_TO_FILE%));\n    stopWordsSet = WordlistLoader.getWordSet(new StringReader(stopWordsDictionary));\n} catch (FileNotFoundException e) {\n    // use standard stop words list\n    stopWordsSet = CharArraySet.copy(StandardAnalyzer.STOP_WORDS_SET);\n}\n\ntokenStream = new StopFilter(new ASCIIFoldingFilter(new ClassicFilter(new LowerCaseFilter(stdToken))), stopWordsSet);\ntokenStream.reset();\n
255	&lt;field name="date" type="string" indexed="true" stored="true"/&gt;\n\n&lt;field name="customer" type="string" indexed="true" stored="true" multiValued="true"/&gt;\n\n&lt;field name="customer.name" type="string" indexed="true" stored="true"/&gt;\n\n&lt;field name="customer.id" type="string" indexed="true" stored="true"/&gt;\n
256	$doc = Zend_Search_Lucene_Document_Html::loadHTMLFile('http://my.host.domain/path2file');\n$index-&gt;addDocument($doc);\n
257	mark the indexes by I1..Im.\nfor i in 1..m, let Ci = all the indexes but Ii\n  for all the documents Dj in Ii,\n  let cur_term = "id:&lt;Dj's id&gt;"\n  for Ik in Ci\n    Ik.deleteDocuments(cur_term)\nmerge all indexes\n
258	    static private HashSet&lt;String&gt; m_stop_phrases = new HashSet&lt;String&gt;(); \n    static private int m_max_stop_phrase_length = 0; \n... \n    public final boolean incrementToken() throws IOException { \n        if (!input.incrementToken()) \n            return false; \n        Stack&lt;State&gt; stateStack = new Stack&lt;State&gt;(); \n        StringBuilder match_string_builder = new StringBuilder(); \n        int skippedPositions = 0; \n        boolean is_next_token = true; \n        while (is_next_token &amp;&amp; match_string_builder.length() &lt; m_max_stop_phrase_length) { \n            if (match_string_builder.length() &gt; 0) \n                match_string_builder.append(" "); \n            match_string_builder.append(termAtt.term()); \n            skippedPositions += posIncrAtt.getPositionIncrement(); \n            stateStack.push(captureState()); \n            is_next_token = input.incrementToken(); \n            if (m_stop_phrases.contains(match_string_builder.toString())) { \n              // Stop phrase is found: skip the number of tokens \n              // without restoring the state \n              posIncrAtt.setPositionIncrement(posIncrAtt.getPositionIncrement() + skippedPositions); \n              return is_next_token; \n            } \n        } \n        // No stop phrase found: restore the stream \n        while (!stateStack.empty()) \n            restoreState(stateStack.pop()); \n        return true; \n    } \n
259	private long round(double price)\n    {\n        double rounded = Math.floor(price / 3) * 3;\n\n        if (rounded != price)\n        {\n            rounded += 3; //we round up\n        }\n\n        return (long) rounded;\n    }\n
260	Hashtable&lt;Integer, Integer&gt; hits = new Hashtable&lt;Integer, Integer&gt;();\nwhile (spans.next() == true)\n{\n     int docID = spans.doc();\n     int hit = hits.get(docID) != null ? hits.get(docID) : 0;\n     hit++;\n     hits.put(docID, hit);\n}\n
261	public class MyCharAnalyzer extends Analyzer { \n\n  public TokenStream tokenStream(String fieldName, Reader reader) {\n    TokenStream result = new LetterTokenizer(reader);    \n    return result;\n  }\n}\n\nShingleAnalyzerWrapper myBigramWrapper = new ShingleAnalyzerWrapper(new MyCharAnalyzer());\n
262	&lt;type storage="unstored" stripTags="true"&gt;single-line text&lt;/type&gt;\n
263	 QueryExecution qe = QueryExecutionFactory.create(query, model) ;\n LARQ.setDefaultIndex(qe.getContext(), index) ;\n
264	    $doc = new Zend_Search_Lucene_Document();\n\n    $doc-&gt;addField(Zend_Search_Lucene_Field::Keyword('user_id', $row-&gt;user_id));\n    $doc-&gt;addField(Zend_Search_Lucene_Field::UnIndexed('date_1', $row-&gt;date_1));\n    $doc-&gt;addField(Zend_Search_Lucene_Field::Text('title', $row-&gt;title));\n\n    $index-&gt;addDocument($doc);\n\n     //etc\n
265	    IndexReader ir = IndexReader.Open(FDirectory);\n    foreach(string fName in ir.Directory().List())\n    {\n      // sum file sizes here\n    }\n    ir.Close();\n
266	 IndexReader reader = IndexReader.Open(_directory);                \n IndexSearcher indexSearcher = new IndexSearcher(reader);\n TermQuery tq= new TermQuery(new Term("Url", downloadDoc.Uri.ToString().ToLower()));                \n BooleanQuery bq = new BooleanQuery();\n bq.Add(tq, BooleanClause.Occur.SHOULD);\n TopScoreDocCollector collector = TopScoreDocCollector.create(10, true);\n
267	@SearchableMetaData(name="ordering_name", index=Index.NOT_ANALYZED))\n
268	&lt;field name="Data" type="text" indexed="true" stored="true" required="false" multiValued="true" /&gt;\n
269	MergePolicy.MergeSpecification mergeSpecification = super.findMerges(segmentInfos);\n\n// use information from mergeSpecifiation\n\nreturn mergeSpecification;\n
270	List list = query.setFirstResult(0).setMaxResults(100).list();\n
271	curl http://domain.com:8080/solr/collection1/update?commit=true -H "Content-Type: text/xml" --data-binary '&lt;delete&gt;&lt;query&gt;*:*&lt;/query&gt;&lt;/delete&gt;'\n
272	int length3 = CheckBoxList4.Items.Count;\nint count = 0;         \nfor (int i = 0; i &lt; length3; i++)\n{\n    BooleanQuery finalQuery1 = (BooleanQuery)Session["Luc_Query"];\n    finalQuery1 = (BooleanQuery)finalQuery1.Clone();\n    var query1 = new QueryParser("Industry", analyzer).Parse(CheckBoxList4.Items[i].Text);\n    finalQuery1.Add(query1, BooleanClause.Occur.MUST);                \n    hits = searcher.Search(finalQuery1);\n    count = hits.Length();\n    CheckBoxList4.Items[i].Text = CheckBoxList4.Items[i].Text + " " + count.ToString() ;\n}\n
273	boolean willMatchAll(Query query) {\n    if (query instanceof MatchAllDocsQuery)\n        return true;\n    }\n    else if (query instanceof BooleanQuery) {\n        boolean foundMatchAll = false;\n        for (BooleanClause clause : ((BooleanQuery)query).getClauses()) {\n            if (clause.isProhibited()) {\n                return false; //A reasonable assumption, that the MUST_NOT clauses won't be empty\n            }\n            else if (clause.isRequired()) {\n                if (willMatchAll(clause.getQuery())) {\n                    foundMatchAll = true;\n                } else {\n                    return false; //any MUST clause that is not a matchall means the boolean query will not match all\n                }\n            }\n            else {\n                if (willMatchAll(clause.getQuery())) {\n                    foundMatchAll = true;\n                }\n            }\n        }\n        //If a matchall has been found, and we haven't return false yet, this boolean query matches all documents\n        return foundMatchAll;\n    }\n    else if (query instanceof DisjunctionMaxQuery) {\n        boolean isMatchAll = false\n        //If any disjunct is a matchall, the query will match all documents\n        for (Query subquery : ((DisjunctuionMaxQuery)query).getDisjuncts()) {\n            isMatchAll = isMatchAll || willMatchAll(subquery);\n        }\n        return isMatchAll;\n    }\n    else if (query instanceof ConstantScoreQuery) {\n        //Traverse right through ConstantScoreQuery.  The wrapper isn't of interest here.\n        Query subquery = ((ConstantScoreQuery)query).getQuery()\n        if (subquery == null) {\n            return false; //It wraps a filter, not a query, and I don't believe a filter can be a matchall\n        }\n        return willMatchAll(subquery);\n    }\n    else {\n        //No other standard queries may be or contain MatchAllDocsQueries, I don't believe.\n        //Even a double open-ended range query restricts the results to those with a value in the specified field.\n        return false; \n    }\n}\n
274	public class CustomScoreQuerySample {\n\npublic static void main(String[] args) throws Exception {\n    CustomScoreQuerySample sample = new CustomScoreQuerySample();\n    sample.makeIndex();\n    sample.search("XX");\n}\n\nprivate static final String[][] DATAS = { { "XX", "981" }, { "XX", "672" },\n        { "XX", "521" }, { "XX", "124" }, { "XX", "908" }, { "XX", "652" },\n        { "XX", "872" }, { "XX", "278" }, { "XX", "485" }, { "XX", "372" } };\n\nprivate RAMDirectory dir = new RAMDirectory();\nprivate Analyzer analyzer = new SimpleAnalyzer(Version.LUCENE_44);\n\npublic void makeIndex() throws Exception {\n    IndexWriterConfig config = new IndexWriterConfig(Version.LUCENE_44, analyzer);\n    IndexWriter writer = new IndexWriter(dir, config);\n\n    try {\n        for (int i = 0; i &lt; DATAS.length; i++) {\n            Document doc = new Document();\n            doc.add(new TextField("value", DATAS[i][0], Field.Store.YES ));\n            doc.add(new FloatField("count", Float.parseFloat(DATAS[i][1]), Field.Store.YES ));\n            writer.addDocument(doc);\n        }\n    } finally {\n        writer.close();\n    }\n}\nprivate void search(String value) throws Exception {\n\n    DirectoryReader reader = DirectoryReader.open(dir);\n    try {\n        QueryParser parser = new QueryParser(Version.LUCENE_44, "value", analyzer);\n        Query tq = parser.parse("XX");\n\n        FloatFieldSource source = new FloatFieldSource("count");\n        FunctionQuery fq = new FunctionQuery(source);\n\n        CustomScoreQuery query = new CustomScoreQuery(tq, fq);\n\n        TopScoreDocCollector collector = TopScoreDocCollector.create(5 * 5, false);\n\n        IndexSearcher searcher = new IndexSearcher(reader);\n        searcher.search(query,collector);\n\n        ScoreDoc[] hits = collector.topDocs().scoreDocs;\n\n        int count = hits.length;\n        for (int i = 0; i &lt; count; i++) {\n            Document doc = searcher.doc(hits[i].doc);\n            float score = hits[i].score;\n            System.out.println(score + " : " + doc.get("value") + " / "\n                    + doc.get("count"));\n        }\n    } finally {\n        reader.close();\n    }\n}\n}\n
275	import org.compass.gps.device.hibernate.HibernateGpsDevice\nimport grails.plugin.searchable.internal.compass.config.SessionFactoryLookup\n\nbeans = {\n    compassGpsDevice(HibernateGpsDevice) { bean -&gt;\n        bean.destroyMethod = "stop"\n        name = "hibernate"\n        sessionFactory = { SessionFactoryLookup sfl -&gt;\n          sessionFactory = ref('sessionFactory_datasourceName') \n        }\n        fetchCount = 5000\n    }\n}\n
276	Query query = queryBuilder.phrase().onField("path").sentence("SNO_NO_D6-11100").createQuery();\n
277	// Try the system property：-Danalysis.data.dir=/path/to/analysis-data\nANALYSIS_DATA_DIR = System.getProperty("analysis.data.dir", "");\n
278	public static BasicStats getBasicStats(IndexReader indexReader, Term myTerm, float queryBoost) throws IOException {\n    String fieldName = myTerm.field();\n\n    CollectionStatistics collectionStats = new CollectionStatistics(\n            "field",\n            indexReader.maxDoc(),\n            indexReader.getDocCount(fieldName),\n            indexReader.getSumTotalTermFreq(fieldName),\n            indexReader.getSumDocFreq(fieldName)\n            );\n\n    TermStatistics termStats = new TermStatistics(\n            myTerm.bytes(),\n            indexReader.docFreq(myTerm),\n            indexReader.totalTermFreq(myTerm)\n            );\n\n    BasicStats myStats = new BasicStats(fieldName, queryBoost);\n    assert collectionStats.sumTotalTermFreq() == -1 || collectionStats.sumTotalTermFreq() &gt;= termStats.totalTermFreq();\n    long numberOfDocuments = collectionStats.maxDoc();\n\n    long docFreq = termStats.docFreq();\n    long totalTermFreq = termStats.totalTermFreq();\n\n    if (totalTermFreq == -1) {\n      totalTermFreq = docFreq;\n    }\n\n    final long numberOfFieldTokens;\n    final float avgFieldLength;\n\n    long sumTotalTermFreq = collectionStats.sumTotalTermFreq();\n\n    if (sumTotalTermFreq &lt;= 0) {\n        numberOfFieldTokens = docFreq;\n        avgFieldLength = 1;\n    } else {\n        numberOfFieldTokens = sumTotalTermFreq;\n        avgFieldLength = (float)numberOfFieldTokens / numberOfDocuments;\n    }\n\n    myStats.setNumberOfDocuments(numberOfDocuments);\n    myStats.setNumberOfFieldTokens(numberOfFieldTokens);\n    myStats.setAvgFieldLength(avgFieldLength);\n    myStats.setDocFreq(docFreq);\n    myStats.setTotalTermFreq(totalTermFreq);\n\n    return myStats;\n}\n
279	 &lt;searchComponent name="suggest" class="solr.SuggestComponent"&gt;\n      &lt;lst name="suggester"&gt;\n        &lt;str name="name"&gt;mySuggester&lt;/str&gt;\n        &lt;str name="lookupImpl"&gt;FuzzyLookupFactory&lt;/str&gt;      \n        &lt;str name="dictionaryImpl"&gt;DocumentDictionaryFactory&lt;/str&gt;\n        &lt;str name="field"&gt;name_s&lt;/str&gt;\n        &lt;str name="weightField"&gt;price&lt;/str&gt;\n        &lt;str name="suggestAnalyzerFieldType"&gt;text_general&lt;/str&gt;\n        &lt;str name="buildOnStartup"&gt;false&lt;/str&gt;\n      &lt;/lst&gt;\n    &lt;/searchComponent&gt;\n\n    &lt;requestHandler name="/suggest" class="solr.SearchHandler" \n                    startup="lazy" &gt;\n      &lt;lst name="defaults"&gt;\n        &lt;str name="suggest"&gt;true&lt;/str&gt;\n        &lt;str name="suggest.count"&gt;10&lt;/str&gt;\n      &lt;/lst&gt;\n      &lt;arr name="components"&gt;\n        &lt;str&gt;suggest&lt;/str&gt;\n      &lt;/arr&gt;\n  &lt;/requestHandler&gt;\n
280	 Configuration.setTaggerType("openNLP");\n        Configuration.setSingleStrength(6);\n        Configuration.setNoLimitStrength(5);\n        // if tagger type is "openNLP" then give the openNLP POS tagger path\n        //Configuration.setModelFileLocation("model/openNLP/en-pos-maxent.bin"); \n        // if tagger type is "default" then give the default POS lexicon file\n        //Configuration.setModelFileLocation("model/default/english-lexicon.txt");\n        // if tagger type is "stanford "\n        Configuration.setModelFileLocation("Dont need that here");\n        Configuration.setPipeline(pipeline);\n        TermsExtractor termExtractor = new TermsExtractor();\n        TermDocument topiaDoc = new TermDocument();\n        topiaDoc = termExtractor.extractTerms(text);\n        //logger.info("Extracted terms : " + topiaDoc.getExtractedTerms());\n        Map&lt;String, ArrayList&lt;Integer&gt;&gt; finalFilteredTerms = topiaDoc.getFinalFilteredTerms();\n        List&lt;KeywordsModel&gt; keywords = new ArrayList&lt;&gt;();\n        for (Map.Entry&lt;String, ArrayList&lt;Integer&gt;&gt; e : finalFilteredTerms.entrySet()) {\n            KeywordsModel keyword = new KeywordsModel();\n            keyword.setLabel(e.getKey());\n            keywords.add(keyword);\n        }\n
281	function(doc) {\n    var time = new Date(Date.parse(doc['timestamp'])); \n    var year = time.getUTCFullYear();\n    var month = time.getUTCMonth()+1;\n    var day = time.getUTCDate();\n\n    // day granularity\n    emit([year,month,day,doc.username], null);\n\n    // year granularity\n    emit([year,doc.username], null);\n}\n\n// reduce function - `_count`\n
282	TermsEnum itr = termVector.iterator();\n
283	SELECT * FROM user WHERE lucene = '{filter: {type:"boolean", must:[\n    {type : "wildcard", field : "username", value : "*%s*"},\n    {type : "match", field : "is_verified", value : true}\n]}}' LIMIT 15;\n
284	Field myField = new NumericField("myIntField").setIntValue(100000);\n
285	fq={!frange l=50}ceil(product(div(sub(swaptot,swapavail),swaptot),100))\n
286	String[] strs = {"\"avatar film fiction\"", "\"avatar-film fiction\"", "\"avatar-film-fiction\""};\n
287	class CustomizedScoreProvider extends CustomScoreProvider {\nprivate LeafReaderContext context;\npublic CustomizedScoreProvider(LeafReaderContext reader) {\n        super(reader);\n        this.context= reader;\n        // TODO Auto-generated constructor stub\n    }\n\npublic  float customScore(int doc, float subQueryScore,float valSrcScores[]) throws IOException{\n\n    Document Social=context.reader().document(doc);\n     IndexableField i= Social.getField("soc");// the field I wanted to extract\n     float k= (float)i.numericValue();\n     subQueryScore+=k;\n\nreturn subQueryScore;\n         }\n}\n
288	Query baseQuery = new MatchAllDocsQuery();\nDrillDownQuery ddQuery = new DrillDownQuery(config, baseQuery);\nddQuery.add("city", "california");\nFacetsCollector fc = new FacetsCollector();\nFacetsCollector.search(searcher, ddQuery, 10, fc);\n
289	    @AnalyzerDef(name = "TEXT_SORT",\n        tokenizer = @TokenizerDef(factory = KeywordTokenizerFactory.class),\n        filters = {\n                @TokenFilterDef(factory = ASCIIFoldingFilterFactory.class),\n                @TokenFilterDef(factory = LowerCaseFilterFactory.class),\n                @TokenFilterDef(factory = PatternReplaceFilterFactory.class, params = {\n                    @Parameter(name = "pattern", value = "('-&amp;\\.,\\(\\))"),\n                    @Parameter(name = "replacement", value = " "),\n                    @Parameter(name = "replace", value = "all")\n                }),\n                @TokenFilterDef(factory = PatternReplaceFilterFactory.class, params = {\n                    @Parameter(name = "pattern", value = "([^0-9\\p{L} ])"),\n                    @Parameter(name = "replacement", value = ""),\n                    @Parameter(name = "replace", value = "all")\n                }),\n                @TokenFilterDef(factory = TrimFilterFactory.class)\n        }\n    )\n
290	String id = ..\nlong timestamp = ...\nDocument doc = new Document();\n// The sorted version of my EntityId\ndoc.add(new SortedDocValuesField("EntityId", new BytesRef(id)));\n// The stored version of my EntityId to be able to get its value later if needed\ndoc.add(new StringField("Id", id, Field.Store.YES));\n// The sorted version of my timestamp\ndoc.add(new NumericDocValuesField("Timestamp", timestamp));\n// The stored version of my timestamp to be able to get its value later if needed\ndoc.add(new StringField("Tsp", Long.toString(timestamp), Field.Store.YES));\n
291	final IndexSearcher searcher = new IndexSearcher(reader);\nsearcher.setQueryCache(null);\nfinal boolean needsScores = false; // scores are not needed, only matching docs\nfinal Weight preserveWeight = searcher.createNormalizedWeight(preserveFilter, needsScores);\nfinal int maxDoc = in.maxDoc();\nfinal FixedBitSet bits = new FixedBitSet(maxDoc);\n// ignore livedocs here, as we filter them later:\nfinal Scorer preverveScorer = preserveWeight.scorer(context);\nif (preverveScorer != null) {\n  bits.or(preverveScorer.iterator());\n}\nif (negateFilter) {\n  bits.flip(0, maxDoc);\n}\n\nif (in.hasDeletions()) {\n  final Bits oldLiveDocs = in.getLiveDocs();\n  assert oldLiveDocs != null;\n  final DocIdSetIterator it = new BitSetIterator(bits, 0L); // the cost is not useful here\n  for (int i = it.nextDoc(); i != DocIdSetIterator.NO_MORE_DOCS; i = it.nextDoc()) {\n    if (!oldLiveDocs.get(i)) {\n      // we can safely modify the current bit, as the iterator already stepped over it:\n      bits.clear(i);\n    }\n }\n}\n\nthis.liveDocs = bits;\nthis.numDocs = bits.cardinality();\n
292	@Override\n protected TokenStreamComponents createComponents(String fieldName) {\n   Tokenizer source = new FooTokenizer();\n   TokenStream filter = new FooFilter(source);\n   filter = new BarFilter(filter);\n   return new TokenStreamComponents(source, filter);\n }\n
293	for (LeafReaderContext context : indexReader.getContext().leaves()) {\n  LeafReader reader = context.reader();\n  // run for each leaf\n}\n
294	&lt;similarity class="solr.ClassicSimilarityFactory"/&gt;\n
295	            var search = $"\"*{term}*\"";\n            var qOpt = EscapeQueryOptions.RawQuery;\n\n            query = query\n                .Search(o =&gt; o.Property1, search, escapeQueryOptions: qOpt)\n                .Search(o =&gt; o.Property2, search, escapeQueryOptions: qOpt)\n                .Search(o =&gt; o.Property3, search, escapeQueryOptions: qOpt)\n                .Search(o =&gt; o.Property4, search, escapeQueryOptions: qOpt);\n
296	    TokenStream stream = new KeywordTokenizer(reader);\n
297	// Comments out by Corey Trager, Oct 2008 to workaround permission restrictions at shared host.  This is not used.\n//        public static readonly System.String LOCK_DIR = SupportClass.AppSettings.Get("Lucene.Net.lockDir", System.IO.Path.GetTempPath());\n
298	var standardLuceneAnalyzer = new StandardAnalyzer();\n\nvar query1 = new QueryParser("SearchKey", standardLuceneAnalyzer).Parse("Kansas City*");\nvar query2 = new QueryParser("Type", standardLuceneAnalyzer).Parse("Airport");\n\nBooleanQuery filterQuery = new BooleanQuery();\nfilterQuery.Add(query1, BooleanClause.Occur.MUST);\nfilterQuery.Add(query1, BooleanClause.Occur.MUST);\n\nTopDocs results = searcher.Search(filterQuery);\n
299	SELECT  *\nFROM    workouts w\nJOIN    user_workouts uw\nON      uw.workout = w.id\nWHERE   w.query = 'query query query;filter=user_id,$user_id'\n        AND uw.user = $user_id\nORDER BY\n        uw.times_performed DESC\n
300	public class Explainer {\n\n  public static void main(String[] args) throws Exception {\n\n     if (args.length != 2) {\n        System.err.println("Usage: Explainer &lt;index dir&gt; &lt;query&gt;");\n        System.exit(1);\n     }\n\n     String indexDir = args[0];\n     String queryExpression = args[1];\n     Directory directory = FSDirectory.open(new File(indexDir));\n     QueryParser parser = new QueryParser(Version.LUCENE_CURRENT,\n                                     "contents", new SimpleAnalyzer());\n\n     Query query = parser.parse(queryExpression);\n     System.out.println("Query: " + queryExpression);\n     IndexSearcher searcher = new IndexSearcher(directory);\n     TopDocs topDocs = searcher.search(query, 10);\n     for (int i = 0; i &lt; topDocs.totalHits; i++) {\n        ScoreDoc match = topDocs.scoreDocs[i];\n        Explanation explanation = searcher.explain(query, match.doc);   \n        System.out.println("----------");\n        Document doc = searcher.doc(match.doc);\n        System.out.println(doc.get("title"));\n        System.out.println(explanation.toString());\n     }\n  }\n}\n
301	    public class CustomAnalyzer : StandardAnalyzer\n    {\n        Lucene.Net.Util.Version matchVersion;\n\n        public CustomAnalyzer(Lucene.Net.Util.Version p_matchVersion)\n            : base(p_matchVersion)\n        {\n            matchVersion = p_matchVersion;\n        }\n\n        public override TokenStream TokenStream(string fieldName, System.IO.TextReader reader)\n        {\n            TokenStream result = new StandardTokenizer(matchVersion, reader);\n            result = new StandardFilter(result);\n            result = new ASCIIFoldingFilter(result);\n            return result;\n        }\n\n    }\n
302	&lt;property name="hibernate.search.analyzer"&gt;path.to.ChemicalNameAnalyzer&lt;/property&gt;\n
303	protected override void AddFields(Item item, Document document)\n{   \n    // Add base fields\n    base.AddFields(item, document);\n\n    // Add all inherited templates id to a field \n    string TEMPLATE-PATH="get template path for this item here";\n    document.Add(new Field("template-path", TEMPLATE-PATH, Field.Store.NO, Field.Index.TOKENIZED)); \n}\n
304	hl=true&amp;hl.fl=*\n
305	g.E.filter{it.property &gt;= 0 &amp;&amp; it.property &lt;= 1.6}\n
306	class CustomDistance() extends LevensteinDistance{\n    float getDistance(String target, String other) {\n        float distance = super.getDistance();\n        if (isPermutation(target, other)) {\n            distance = distance + (1 - distance) / 2;\n        }\n        return distance;\n    }\n}\n
307	START game=node:Game('Name: "Mario Kart"') RETURN game;\n
308	IndexableField field = new StringField(fieldName, myTerm, FieldType.TYPE_NOT_STORED);\nfor (int i = 0; i &lt; frequency; i++) {\n    document.add(field);\n}\n
309	public Indexer(Directory indexDir, PrintStream printStream) throws IOException {\n    IndexWriterConfig config = new IndexWriterConfig(Version.LUCENE_43, new Analyzer());\n    writerConfig.setIndexDeletionPolicy(new SnapshotDeletionPolicy(new KeepOnlyLastCommitDeletionPolicy()));\n    indexWriter = new IndexWriter(indexDir, writerConfig);\n    snapshotter = (SnapshotDeletionPolicy) indexWriter.getConfig().getIndexDeletionPolicy();\n} \n
310	TermRangeQuery dateQuery = new TermRangeQuery("created_at",lowerDate, upperDate, includeLower, includeUpper);\nTermQuery keywordQuery = new TermQuery(new Term("keyword", "term"));\nBooleanQuery bq = new BooleanQuery();\nbq.add(new BooleanClause(dateQuery, BooleanClause.Occur.MUST))\nbq.add(new BooleanClause(keywordQuery, BooleanClause.Occur.MUST))\n\n// display search results\nTopDocs topDocs = searcher.search(bq, 10);\n
311	    List&lt;string&gt; nodesList = new List&lt;string&gt;();\n    var Searcher = ExamineManager.Instance.SearchProviderCollection["ExternalSearcher"];\n    var searchCriteria = Searcher.CreateSearchCriteria(BooleanOperation.Or);\n    var query = searchCriteria.Field("tags", queryString.Fuzzy(0.5f)).Compile();\n    var searchResults = Searcher.Search(query);\n    foreach (var item in searchResults)\n    {\n        string paths = ((Examine.SearchResult)item).Fields["tags"];\n        nodesList.Add(paths); \n    }\n
312	FilterBuilder filterBuilder = FilterBuilders.termFilter("value", "break middle");\n
313	Query priceQuery = builder.range().onField("price").below(maxPrice)\n    .createQuery();\n
314	curl -XGET 'localhost:9200/_analyze?analyzer=standard' -d '*'\n
315	SpanQuery firstwordQuery = new SpanTermQuery(new Term("myField", "system"));\n//Unfortunately, Lucene.Net doesn't have SpanMultiTermQueryWrapper...\nSpanQuery secondwordQuery = new SpanRegexQuery(new Term("myField", "clean.*"));\nSpanQuery[] spanClauses = new SpanQuery[] {firstwordQuery, secondwordQuery};\nQuery finalQuery = new SpanNearQuery(spanClauses, 0, true);\n
316	"(?&lt;=&amp;)&amp;|&amp;(?=&amp;)|(?&lt;=\\|)[|]|[|](?=\\|)|[\\]\\[!(){}^*?]"\n
317	Analyzer ana = CustomAnalyzer.builder()\n                .withTokenizer("standard")\n                .addTokenFilter("standard")\n                .addTokenFilter("lowercase")     \n                .addTokenFilter("length", "min", "4", "max", "50")\n                .addTokenFilter("stop", "ignoreCase", "false", "words", "stopwords.txt", "format", "wordset")\n                .build();\n
318	public static String[] getFieldNames(IndexReader reader) {\n    List&lt;String&gt; fieldNames = new ArrayList&lt;String&gt;();\n    //For a simple reader over only one index, reader.leaves() should only  return one LeafReaderContext\n    for (LeafReaderContext readerCtx : reader.leaves()) {\n        FieldInfos fields = readerCtx.reader().getFieldInfos();\n        for (FieldInfo field : fields) {\n            //Check whether the field is indexed and searchable, perhaps?\n            fieldNames.add(field.name);\n        }\n    }\n    return fieldNames.toArray(new String[fieldNames.size()]); \n}\n
319	private void request(String geocode) throws IOException {\n        HttpResponse response = Request.Post(SEARCH_URL).version(HttpVersion.HTTP_1_1)\n                .bodyForm(createForm(geocode).build(), Charsets.UTF_8).useExpectContinue()\n                .connectTimeout(CONNECTION_TIMEOUT_MILS)\n                .socketTimeout(CONNECTION_TIMEOUT_MILS)\n                .execute().returnResponse();\n\n        assertStatus(response, geocode);\n        getCoordinatesFromResponse(response, geocode);\n    }\n\n    private Form createForm(String geocode) {\n        return Form.form().add("format", "json").add("results", "1").add("geocode", geocode);\n    }\n\n    private void assertStatus(HttpResponse response, String requestString) {\n        StatusLine statusLine = response.getStatusLine();\n        if (statusLine.getStatusCode() &gt;= ERROR_STATUS_MIN) {\n            throw new RuntimeException(String.format(\n                    "Error sending request '%s' to the map service, server response: %s",\n                    requestString, response));\n        }\n    }\n
320	FullTextQuery myQuery = ... //setup my lucene query here\nCriteria fetchAssociationCriteria = session.createCriteria(Foo.class);\nfetchAssociationCriteria.setFetchMode("bar", FetchMode.JOIN);\nList&lt;Foo&gt; foos = myQuery.setCriteriaQuery(fetchAssociationCriteria).getResultList();\n
321	public final class SymbolSplitterFilter extends TokenFilter {\n\nprivate final CharTermAttribute termAtt;\nprivate final PositionIncrementAttribute posIncAtt;\nprivate final Stack&lt;String&gt; termStack;\nprivate AttributeSource.State current;\nprivate final TypeAttribute typeAtt;\n\npublic SymbolSplitterFilter(TokenStream in)\n{\n    super(in);\n    termStack = new Stack&lt;&gt;();\n    termAtt = addAttribute(CharTermAttribute.class);\n    posIncAtt = addAttribute(PositionIncrementAttribute.class);\n    typeAtt = addAttribute(TypeAttribute.class);\n}\n\n@Override\npublic boolean incrementToken() throws IOException\n{\n    if (!this.termStack.isEmpty()) {\n        String part = termStack.pop();\n        restoreState(current);\n        termAtt.setEmpty().append(part);\n        posIncAtt.setPositionIncrement(0);\n        return true;\n    } else if (!input.incrementToken()) {\n        return false;\n    } else {\n        final String currentTerm = termAtt.toString();\n        final int bufferLength = termAtt.length();\n\n        if (bufferLength &gt; 1 &amp;&amp; currentTerm.indexOf("@") &gt; 0) { // There must be sth more than just @\n            if (termStack.isEmpty()) {\n                termStack.addAll(Arrays.asList(currentTerm.split("@")));\n                current = captureState();\n            }\n        }\n        return true;\n\n    }\n\n}\n}\n
322	String base1 = "lawnmower";\nString syn1 = "lawn mower";\n\nSynonymMap.Builder sb = new SynonymMap.Builder(true);\nCharsRef syn1Chars = sb.Join(Regex.Split(syn1, " +"), new CharsRef());\nsb.Add(new CharsRef(base1), syn1Chars, true);\nSynonymMap smap = sb.Build();\n
323	$index = Zend_Search_Lucene::create('/path/to/public_html/public/data/users_index');\n
324	BooleanQuery query = new BooleanQuery();\nfor (String token : tokenize(queryString)) {\n  query.add(new PrefixQuery(new Term(LABEL_FIELD_NAME, token)), Occur.MUST);\n}\nreturn query;\n
325	\n@Override\npublic float coord(int overlap, int maxOverlap) {\n  return (overlap == maxOverlap) \n  ? 1f\n  : 0.5f * super.coord(overlap, maxOverlap);\n}\n
326	var q = ctx.CreateQuery&lt;LatestJobs&gt;("jobstable")\n    .Where(j =&gt; j.PartitionKey.CompareTo(LastIndexTime.GetReverseTicks()) &lt; 0)\n    .Take(1)\n    .AsTableServiceQuery()\n\nif (q.Count() &gt; 0)\n{\n    //new jobs exist since last check... re-index.\n}\n
327	$query = new SolrQuery();\n$query-&gt;setParam('fq', '{!geofilt pt=45.15,-93.85 sfield=store d=5}');\n
328	q = name:jean content:jean\n&amp;\nfq= type:(book person)\n&amp;\nfq= category:(fiction fantasy) (*:* AND -category:*)\n&amp;\nfq= group:(pangolin) (*:* AND -group:*)\n
329	&lt;solrQueryParser defaultOperator="AND"/&gt;\n
330	BooleanQuery b = new BooleanQuery();\n\nSet&lt;String&gt; fields = params.keySet();\nStandardAnalyzer analyzer = new StandardAnalyzer(version);\n\nb.add(new TermQuery(new Term("cs-method", "GET"), BooleanClause.Occur.SHOULD);\nb.add(new TermQuery(new Term("cs-uri", "/blank"), BooleanClause.Occur.SHOULD);\n\nQuery q = new QueryParser(version, "cs-method", analyzer).parse(b.toString());\n
331	&lt;requestHandler name="/dataimport" class="org.apache.solr.handler.dataimport.DataImportHandler"&gt;\n    &lt;lst name="defaults"&gt;\n        &lt;str name="config"&gt;solr-data-config.xml&lt;/str&gt;\n    &lt;/lst&gt;\n    &lt;lst name="invariants"&gt;\n        &lt;str name="clean"&gt;false&lt;/str&gt;\n    &lt;/lst&gt;\n&lt;/requestHandler&gt;\n
332	private static class CountingCollector extends Collector {\n  private final Collector other;\n  private int docBase;\n\n  public final Map&lt;Integer, Map&lt;Query, Float&gt;&gt; docCounts = new HashMap&lt;Integer, Map&lt;Query, Float&gt;&gt;();\n\n  private final Map&lt;Query, Scorer&gt; subScorers = new HashMap&lt;Query, Scorer&gt;();\n  private final ScorerVisitor&lt;Query, Query, Scorer&gt; visitor = new MockScorerVisitor();\n  private final EnumSet&lt;Occur&gt; collect;\n\n  private class MockScorerVisitor extends ScorerVisitor&lt;Query, Query, Scorer&gt; {\n\n    @Override\n    public void visitOptional(Query parent, Query child, Scorer scorer) {\n      if (collect.contains(Occur.SHOULD))\n        subScorers.put(child, scorer);\n    }\n\n    @Override\n    public void visitProhibited(Query parent, Query child, Scorer scorer) {\n      if (collect.contains(Occur.MUST_NOT))\n        subScorers.put(child, scorer);\n    }\n\n    @Override\n    public void visitRequired(Query parent, Query child, Scorer scorer) {\n      if (collect.contains(Occur.MUST))\n        subScorers.put(child, scorer);\n    }\n\n  }\n\n  public CountingCollector(Collector other) {\n    this(other, EnumSet.allOf(Occur.class));\n  }\n\n  public CountingCollector(Collector other, EnumSet&lt;Occur&gt; collect) {\n    this.other = other;\n    this.collect = collect;\n  }\n\n  @Override\n  public void setScorer(Scorer scorer) throws IOException {\n    other.setScorer(scorer);\n    scorer.visitScorers(visitor);\n  }\n\n  @Override\n  public void collect(int doc) throws IOException {\n    final Map&lt;Query, Float&gt; freqs = new HashMap&lt;Query, Float&gt;();\n    for (Map.Entry&lt;Query, Scorer&gt; ent : subScorers.entrySet()) {\n      Scorer value = ent.getValue();\n      int matchId = value.docID();\n      freqs.put(ent.getKey(), matchId == doc ? value.freq() : 0.0f);\n    }\n    docCounts.put(doc + docBase, freqs);\n    other.collect(doc);\n  }\n\n  @Override\n  public void setNextReader(IndexReader reader, int docBase)\n      throws IOException {\n    this.docBase = docBase;\n    other.setNextReader(reader, docBase);\n  }\n\n  @Override\n  public boolean acceptsDocsOutOfOrder() {\n    return other.acceptsDocsOutOfOrder();\n  }\n}\n
333	http://localhost:8080/solr/select/?qt=mlt&amp;q=id:(document_id1 OR document_id2 OR document_id3)&amp;mlt.fl=[field1],[field2],[field3]&amp;fl=id&amp;rows=10\n
334	package test;\n\nimport java.io.IOException;\nimport java.io.StringReader;\nimport java.util.LinkedList;\nimport java.util.List;\n\nimport org.apache.lucene.analysis.Analyzer;\nimport org.apache.lucene.analysis.TokenStream;\nimport org.apache.lucene.analysis.standard.StandardAnalyzer;\nimport org.apache.lucene.analysis.tokenattributes.CharTermAttribute;\nimport org.apache.lucene.index.Term;\nimport org.apache.lucene.queryparser.classic.ParseException;\nimport org.apache.lucene.queryparser.classic.QueryParser;\nimport org.apache.lucene.queryparser.ext.ExtendableQueryParser;\nimport org.apache.lucene.queryparser.ext.ExtensionQuery;\nimport org.apache.lucene.queryparser.ext.Extensions;\nimport org.apache.lucene.queryparser.ext.ParserExtension;\nimport org.apache.lucene.search.Query;\nimport org.apache.lucene.search.spans.SpanFirstQuery;\nimport org.apache.lucene.search.spans.SpanNearQuery;\nimport org.apache.lucene.search.spans.SpanQuery;\nimport org.apache.lucene.search.spans.SpanTermQuery;\nimport org.apache.lucene.util.Version;\n\npublic class ExpandableQueryParserTest {\n\n    private static Extensions createExtensions() {\n\n        ParserExtension parserExtension = new ParserExtension() {\n\n            @Override\n            public Query parse(final ExtensionQuery query) throws ParseException {\n\n                QueryParser parser = query.getTopLevelParser();\n                // Although Analyzer implements java.io.Closeable, don't use it in try-with-resources.\n                // Otherwise, it won't be able to parse the rest of the query...\n                Analyzer analyzer = parser.getAnalyzer();\n                String rawQueryString = query.getRawQueryString();\n\n                List&lt;SpanQuery&gt; spans = new LinkedList&lt;&gt;();\n                try (TokenStream stream = analyzer.tokenStream(query.getField(), new StringReader(rawQueryString))) {\n\n                    stream.reset();\n\n                    while (stream.incrementToken()) {\n                        String term = stream.getAttribute(CharTermAttribute.class).toString();\n                        spans.add(new SpanTermQuery(new Term(query.getField(), term)));\n                    }\n\n                    stream.close();\n\n                } catch (IOException e) {\n                    // Shouldn't happen as it's being read from RAM.\n                    throw new ParseException(e.getMessage());\n                }\n\n                SpanQuery[] spansArray = spans.toArray(new SpanQuery[spans.size()]);\n                SpanNearQuery spanNear = new SpanNearQuery(spansArray, 1, true, true);\n                return new SpanFirstQuery(spanNear, 1);\n            }\n        };\n\n        Extensions extensions = new Extensions('_');\n        extensions.add("spanfirst", parserExtension);\n        return extensions;\n    }\n\n    public static void main(String[] args) throws Exception {\n\n        // Be careful: QueryParser is not thread-safe.\n        QueryParser queryParser = new ExtendableQueryParser(Version.LUCENE_45, "", new StandardAnalyzer(Version.LUCENE_45), createExtensions());\n        Query query = queryParser.parse("_spanfirst:hello");\n\n        // prints "spanFirst(spanNear([hello], 1, true), 1)"\n        System.out.println(query); \n\n        queryParser = new ExtendableQueryParser(Version.LUCENE_45, "text", new StandardAnalyzer(Version.LUCENE_45), createExtensions());\n        query = queryParser.parse("_spanfirst:hello");\n\n        // prints "spanFirst(spanNear([text:hello], 1, true), 1)"\n        System.out.println(query);\n\n        queryParser = new ExtendableQueryParser(Version.LUCENE_45, "text", new StandardAnalyzer(Version.LUCENE_45), createExtensions());\n        query = queryParser.parse("hello");\n\n        // prints "text:hello"\n        System.out.println(query);\n\n    }\n}\n
335	protected Iterable&lt;String&gt; loadRules( String synonyms, ResourceLoader loader ) {\n    List&lt;String&gt; wlist=null;\n    try {\n      File synonymFile = new File(synonyms);\n      if (synonymFile.exists()) {\n        wlist = loader.getLines(synonyms);\n      } else  {\n        List&lt;String&gt; files = StrUtils.splitFileNames(synonyms);\n        wlist = new ArrayList&lt;String&gt;();\n        for (String file : files) {\n          List&lt;String&gt; lines = loader.getLines(file.trim());\n          wlist.addAll(lines);\n        }\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    return wlist;\n}\n
336	CoreDescriptor coreDescriptor = rb.req.getCore().getCoreDescriptor();\nString collectionName = coreDescriptor.getCloudDescriptor().getCollectionName();    \nZkController zkController = coreDescriptor.getCoreContainer().getZkController();    \nString zookeeperUrl = zkController.getZkServerAddress();\n\nCloudSolrServer server = new CloudSolrServer(zookeeperUrl);\nserver.setDefaultCollection(collectionName);\nserver.connect();\n\nSolrRequest request = ... //initialize the solr request to execute the query\nNamedList&lt;Object&gt; solrResponse = server.request(solrRequest);\n// do whatever you like with the returned response;\nserver.shutdown();\n
337	Analyzer analyzer = new Analyzer() {\n @Override\n  protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n    Tokenizer source = new LetterTokenizer(Version.LUCENE_44, reader);\n    TokenStream filter = new LowercaseFilter(Version.LUCENE_44, source);\n    return new TokenStreamComponents(source, filter);\n  }\n};\n
338	$searchParams['body']['query'] = $query;\n$searchParams['body']['filter'] = $filter;\n$searchParams['body']['facets'] = $facets;\n
339	FilterBuilder qFilter = FilterBuilders.regexpFilter("_all",(".*"+q+".*").replace(" ", ".*"));\n
340	&lt;mergeFactor&gt;10&lt;/mergeFactor&gt; &lt;!-- or any numbers you like--&gt;\n&lt;mergeScheduler class="org.apache.lucene.index.ConcurrentMergeScheduler"/&gt;\n
341	&lt;configuration xmlns:patch="http://www.sitecore.net/xmlconfig/"&gt;\n  &lt;sitecore&gt;\n    &lt;contentSearch&gt;\n      &lt;indexConfigurations&gt;\n        &lt;defaultLuceneIndexConfiguration type="Sitecore.ContentSearch.LuceneProvider.LuceneIndexConfiguration, Sitecore.ContentSearch.LuceneProvider"&gt;\n          &lt;fields hint="raw:AddComputedIndexField"&gt;\n            &lt;field fieldName="yourname"&gt;be.extensions.AppliedThemes, be.extensions&lt;/field&gt;\n          &lt;/fields&gt;\n        &lt;/defaultLuceneIndexConfiguration&gt;\n      &lt;/indexConfigurations&gt;\n    &lt;/contentSearch&gt;\n  &lt;/sitecore&gt;\n&lt;/configuration&gt;\n
342	 {\n      "from" : 0,\n      "size" : 10,\n      "query" : {\n        "query_string" : {\n          "query" : "some query",\n          "fields" : [ "field1", "field2", ... ],\n          "use_dis_max" : true\n        }\n      },\n      "highlight" : {\n        "pre_tags" : [ "&lt;span class=\"mark\"&gt;" ],\n        "post_tags" : [ "&lt;/span&gt;" ],\n        "order" : "score",\n        "encoder" : "html",\n        "require_field_match" : true,\n        "fields" : {\n          "*" : { }\n        }\n      }\n    }\n
343	Map&lt;String, String&gt; commitData = new HashMap();\ncommitData.put("lastModified", String.valueOf(new Date().getTime()));\nindexWriter.setCommitData(commitData);\n//Now commit...\n
344	@IndexedEmbedded\n@Field(name="roles", analyze=Analyze.NO, index=Index.YES)\n@Enumerated(EnumType.STRING)\n@ElementCollection(targetClass = UserRole.class, fetch = FetchType.EAGER)\n@JoinTable(name = "user_roles", joinColumns = {@JoinColumn(name = "user_id")})\nList&lt;UserRole&gt; roles = new ArrayList&lt;&gt;();\n
345	  r = DirectoryReader.open(dir);\n  final Version version = Version.LUCENE_47; // Your lucene version\n  final IndexSearcher searcher = new IndexSearcher(r);\n\n  final String fromField = "ID";\n  final boolean multipleValuesPerDocument = false;\n  final String toField = "SAMPLE_ID";\n  String querystr = "UserID:xxxx AND yourQueryString"; //the userID condition and your query String\n\n  Query fromQuery = new QueryParser(version, "NAME", new WhitespaceAnalyzer(version)).parse(querystr);\n  final Query joinQuery = JoinUtil.createJoinQuery(fromField, multipleValuesPerDocument, toField, fromQuery, searcher, ScoreMode.None);\n\n  final TopDocs topDocs = searcher.search(joinQuery, 10);\n
346	BooleanQuery.Builder finalQuery = new BooleanQuery.Builder();\nBooleanQuery.Builder q1 = new BooleanQuery.Builder();\nq1.add(new TermQuery(new Term("f1", "x")), Occur.SHOULD);\nq1.add(new TermQuery(new Term("f2", "x")), Occur.SHOULD);\nfinalQuery.add(q1.build(), Occur.MUST);\nfinalQuery.add(new TermQuery(new Term("f3", "y")), Occur.MUST);\nQuery queryForSearching = finalQuery.build();\n
347	var searcher = searcherManager.Acquire();\ntry\n{\n    var topDocs = searcher.Search(query, 10);\n    _totalHits = topDocs.TotalHits;\n    foreach (var result in topDocs.ScoreDocs)\n    {\n        var doc = searcher.Doc(result.Doc);\n        l.Add(new SearchResult\n        {\n            Name = doc.GetField("name")?.GetStringValue(),\n            Description = doc.GetField("description")?.GetStringValue(),\n            Url = doc.GetField("url")?.GetStringValue(),\n\n            // Results are automatically sorted by relevance\n            Score = result.Score,\n        });\n    }\n}\ncatch (Exception e)\n{\n    Console.WriteLine(e.ToString());\n}\nfinally\n{\n    searcherManager.Release(searcher);\n    searcher = null; // Never use searcher after this point!\n}\n
348	PUT /_all/_settings?preserve_existing=true          \n{\n  "index.similarity.default.type": "classic"\n} \n
349	MassIndexerProgressMonitor monitor = new CustomIndexerProgressMonitor();\nfullTextSession.createIndexer(ABC.class)\n            .progressMonitor(monitor)\n            .start();\n// Now your custom index progress monitor will receive notifications about the progress\n
350	File yourFile = indexDirectory;\nPath yourPath = yourFile.toPath();\nDirectory directory = FSDirectory.open(yourPath);\n
351	BooleanQuery query = new BooleanQuery();\n\nquery.add(new TermQuery("attributeId", 7), BooleanClause.Occur.MUST); \nquery.add(new TermQuery("value", "hindi"), BooleanClause.Occur.MUST); \nTopDocs docs = searcher.search(query, null, searchLimit);\n
352	string '"lorem ipsum" AND +type:photo' (length=29)\n\nobject(Zend_Search_Lucene_Search_Query_MultiTerm)[230]\n  private '_terms' =&gt; \n    array\n      0 =&gt; \n        object(Zend_Search_Lucene_Index_Term)[236]\n          public 'field' =&gt; null\n          public 'text' =&gt; string 'lorem' (length=5)\n      1 =&gt; \n        object(Zend_Search_Lucene_Index_Term)[237]\n          public 'field' =&gt; null\n          public 'text' =&gt; string 'ipsum' (length=5)\n      2 =&gt; \n        object(Zend_Search_Lucene_Index_Term)[238]\n          public 'field' =&gt; null\n          public 'text' =&gt; string 'and' (length=3)\n      3 =&gt; \n        object(Zend_Search_Lucene_Index_Term)[239]\n          public 'field' =&gt; null\n          public 'text' =&gt; string 'type' (length=4)\n      4 =&gt; \n        object(Zend_Search_Lucene_Index_Term)[240]\n          public 'field' =&gt; null\n          public 'text' =&gt; string 'photo' (length=5)\n
353	...\nBooleanQuery query = new BooleanQuery();\nquery.add(storedQuery, BooleanClause.Occur.MUST);\nquery.add(new TermQuery(ProjectionConstants.ID, id), BooleanClause.Occur.MUST);\n... \n
354	int freq = searcher.docFreq(new Term(FIELD, value));\nTopDocs hits = indexSearcher.search(query, freq);\nfor (int i=0 ; i&lt;hits.totalHits ; i++) {\n   // Process hit\n}\n
355	string[] terms = query.split(" ");\nBooleanQuery bq = new BooleanQuery();\n\nforeach(string term in terms)\n bq.Add(new Query("FieldName", term + "*",...);\n
356	String curDir = System.getProperty("user.dir"); \n
357	SQLiteResult *result = [SQLite query:@"SELECT * from test;"];\nNSArray *row = [result.rows objectAtIndex:0];\nNSString *firstValue = [row objectAtIndex:0];\n
358	    TermEnum te = reader.terms(new Term("field", "app"));\n    List&lt;Term&gt; termList = new LinkedList&lt;Term&gt;();       \n    while(te.next()) {\n        Term t = te.term();\n        if (!t.field().equals("field") || !t.text().startsWith("app")) {\n            break;\n        }\n        termList.add(t);\n    }\n    Term[] terms = termList.toArray(new Term[0]);\n
359	http://...?q=foo&amp;bf="fieldValue(sales)^1.5"\n
360	$query = new Zend_Search_Lucene_Search_Query_Boolean();\n$rangeQuery = ...\n$multiTermQuery = ...\n$query-&gt;addSubquery($rangeQuery, true)\n$query-&gt;addSubquery($multiTermQuery, true)\n
361	    public void fuzzysearch(String querystr) throws Exception{\n        querystr=querystr.toLowerCase();\n\n        System.out.println("\n\n-------- Start fuzzysearch -------- ");\n\n        // 3. search\n        int hitsPerPage = 10;\n        TopScoreDocCollector collector = TopScoreDocCollector.create(hitsPerPage, true);\n        IndexReader reader = IndexReader.open(index);\n\n        IndexSearcher searcher = new IndexSearcher(reader);\n        BooleanQuery bq = new BooleanQuery();\n\n        String[] searchWords = querystr.split(" ") ;\n        int id=0;\n        for(String word: searchWords ){\n            Query query = new FuzzyQuery(new Term(NAME,word));\n            if(id==0){\n                bq.add(query, BooleanClause.Occur.MUST);\n            }else{\n                bq.add(query, BooleanClause.Occur.SHOULD);\n            }\n          id++;\n        }\n        System.out.println("query ==&gt; " + bq.toString());\n        searcher.search(bq, collector );\n        parseResults(  searcher, collector  ) ;\n        searcher.close();\n    }\n\npublic void parseResults(IndexSearcher searcher, TopScoreDocCollector collector  ) throws  Exception {\nScoreDoc[] hits = collector.topDocs().scoreDocs;\n\n    // 4. display results\n    System.out.println("Found " + hits.length + " hits.");\n    for(int i=0;i&lt;hits.length;++i) {\n      int docId = hits[i].doc;\n      Document d = searcher.doc(docId);\n      System.out.println((i + 1) + ". " + d.get(NAME));\n    }\n\n}\n
362	var reader =  IndexReader.Open(dir);\nfor (int i = 0; i &lt; reader.MaxDoc(); i++)\n{\n    if (reader.IsDeleted(i)) continue;\n\n    Document d =  reader.Document(i);\n    var fieldValuePairs =  d.GetFields()\n                            .Select(f =&gt; new { \n                                  Name = f.Name(), \n                                  Value = f.StringValue() })\n                            .ToArray();\n}\n
363	print userIndex.query(Q('username', "*", wildcard=True))[:]\n
364	curl -XGET 'localhost:9200/test/_analyze?analyzer=snowball' -d 'some search query keywords'\n
365	CommonsHttpSolrServer solr = new CommonsHttpSolrServer("http://localhost:8983/solr");\nfor(int i = 0; i &lt; 1000; ++i) {\n  SolrInputDocument doc = new SolrInputDocument();\n  doc.addField("title", "My Favorite book");\n  doc.addField("author", "Kevin");\n  doc.addField("content", "Bla bla bla");\n  solr.add(doc);\n}\nsolr.commit(); \n
366	@Indexed\n@Entity\npublic class Person {\n   ...\n   @Field(name="fullName") String firstName;\n   @Field(name="fullName") String lastName;\n   @Field(name="fullName") String title;\n}\n
367	SynonymMap.Builder builder = new SynonymMap.Builder(true);\nbuilder.add(new CharsRef("Mike"), new CharsRef("Michael"), false);\nbuilder.add(new CharsRef("Rich"), new CharsRef("Richard"), false);\nbuilder.add(new CharsRef("Suzie"), new CharsRef("Susan"), false);\nSynonymMap map = builder.build();\n
368	(domain)/solr/select/?q=query+incorporating+a+whole+bunch+of+optional+terms&amp;defType=edismax&amp;mm=60%\n
369	using (Directory directory = FSDirectory.Open("LuceneIndex"))\nusing (Analyzer analyzer = new StandardAnalyzer(Lucene.Net.Util.Version.LUCENE_30))\nusing (IndexWriter writer = new IndexWriter(directory, analyzer, IndexWriter.MaxFieldLength.UNLIMITED))\nusing (IndexReader reader = writer.GetReader())\n{\n    writer.DeleteAll();\n\n    var doc = new Lucene.Net.Documents.Document();\n    doc.Add(new Lucene.Net.Documents.Field("ID", "1", Lucene.Net.Documents.Field.Store.YES, Lucene.Net.Documents.Field.Index.NOT_ANALYZED, Lucene.Net.Documents.Field.TermVector.NO));\n    doc.Add(new Lucene.Net.Documents.Field("txt", "text", Lucene.Net.Documents.Field.Store.YES, Lucene.Net.Documents.Field.Index.NOT_ANALYZED, Lucene.Net.Documents.Field.TermVector.NO));\n\n    writer.AddDocument(doc);\n    writer.Optimize();\n    writer.Flush(true, true, true);\n\n    Query query = new TermQuery(new Term("txt", "text"));\n    //Setup searcher\n    IndexSearcher searcher = new IndexSearcher(directory);\n    //Do the search\n    TopDocs hits = searcher.Search(query, 10);\n}\n
370	final CharArraySet defaultStopwords = new ItalianAnalyzer(Version.LUCENE_47).getStopWordSet();\n\nfinal CharArraySet defaultArticles = CharArraySet.unmodifiableSet(\n   new CharArraySet(Version.LUCENE_CURRENT, \n       Arrays.asList(\n      "c", "l", "all", "dall", "dell", "nell", "sull", "coll", "pell", \n       "gl", "agl", "dagl", "degl", "negl", "sugl", "un", "m", "t", "s", "v", "d"\n       ), true));\n\nAnalyzer customItalianAnalyzer = new Analyzer() {\n  @Override\n  protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n    final Tokenizer source = new StandardTokenizer(Version.LUCENE_47, reader);\n    TokenStream result = new StandardFilter(Version.LUCENE_47, source);\n    result = new ElisionFilter(result, defaultArticles);\n    result = new LowerCaseFilter(Version.LUCENE_47, result);\n    result = new StopFilter(Version.LUCENE_47, result, defaultStopwords);\n    return new TokenStreamComponents(source, result);\n  }\n};\n
371	{\n  "settings": {\n    "analysis": {\n      "char_filter": {\n        "my_mapping": {\n          "type": "mapping",\n          "mappings": [\n            "-=&gt;"\n          ]\n        }\n      },\n      "analyzer": {\n        "autocomplete_search": {\n          "tokenizer": "keyword",\n          "char_filter": [\n            "my_mapping"\n          ],\n          "filter": [\n            "trim"\n          ]\n        },\n        "autocomplete_index": {\n          "tokenizer": "keyword",\n          "filter": [\n            "trim"\n          ]\n        }\n      }\n    }\n  },\n  "mappings": {\n    "test": {\n      "properties": {\n        "ucn": {\n          "type": "multi_field",\n          "fields": {\n            "ucn_autoc": {\n              "type": "string",\n              "index": "analyzed",\n              "index_analyzer": "autocomplete_index",\n              "search_analyzer": "autocomplete_search"\n            },\n            "ucn": {\n              "type": "string",\n              "index": "not_analyzed"\n            }\n          }\n        }\n      }\n    }\n  }\n}\n
372	StringReader reader = new StringReader(text);\nTokenizer whitespaceTokenizer = new WhitespaceTokenizer();\nwhitespaceTokenizer.setReader(reader);\nTokenStream tokenStream = new StopFilter(whitespaceTokenizer, StopAnalyzer.ENGLISH_STOP_WORDS_SET);\ntokenStream = new PorterStemFilter(tokenStream);\n
373	POST /users/_search?filter_path=hits.hits._id\n{\n  "size": 1000,\n  "_source": false,\n  "query": {\n    "bool": {\n      "filter": [\n        {\n          "geo_distance": {\n            "distance": "100m",\n            "location": {\n              "lat": 32.5362723,\n              "lon": -80.3654783\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n
374	Fragmenter fragmenter = new SimpleSpanFragmenter(queryScorer, 200);\n
375	Query termQuery = new TermQuery(new Term("field", "value"));\nQuery boostedTermQuery = new BoostQuery(termQuery, 2);\n
376	&lt;field indexed="true" name="_text_" type="text_general" multiValued="true" stored="false" /&gt; \n
377	TermPositionVector vector = (TermPositionVector) reader.getTermFreqVector(docId, myfield);\n
378	        List&lt;string&gt; termSet = fieldQuery.getTermSet(fieldName);\n        VectorHighlightMapper tfv = new VectorHighlightMapper(termSet);    \n        reader.GetTermFreqVector(docId, fieldName, tfv);  // &lt;-- look at this line\n\n        string[] terms = tfv.GetTerms();\n        foreach (String term in terms)\n        {\n            if (!termSet.Contains(term)) continue;\n            int index = tfv.IndexOf(term);\n            TermVectorOffsetInfo[] tvois = tfv.GetOffsets(index);\n            if (tvois == null) return; // just return to make null snippets\n            int[] poss = tfv.GetTermPositions(index);\n            if (poss == null) return; // just return to make null snippets\n            for (int i = 0; i &lt; tvois.Length; i++)\n                termList.AddLast(new TermInfo(term, tvois[i].GetStartOffset(), tvois[i].GetEndOffset(), poss[i]));\n
379	public Query rewrite(IndexReader reader) throws IOException {\n  FilteredTermEnum enumerator = getEnum(reader);\n  BooleanQuery query = new BooleanQuery();\n  try {\n    do {\n      Term t = enumerator.term();\n      if (t != null) {\n        TermQuery tq = new TermQuery(t);      // found a match\n        tq.setBoost(getBoost() * enumerator.difference()); // set the boost\n        query.add(tq, false, false);          // add to query\n      }\n    } while (enumerator.next());\n  } finally {\n    enumerator.close();\n  }\n  return query;\n}\n
380	var directory = FSDirectory.Open(new DirectoryInfo("index"));\nvar reader = IndexReader.Open(directory, readOnly: true);\nvar documentId = 1337;\n\n// Grab all subreaders.\nvar subReaders = new List&lt;IndexReader&gt;();\nReaderUtil.GatherSubReaders(subReaders, reader);\n\n// Loop through all subreaders. While subReaderId is higher than the\n// maximum document id in the subreader, go to next.\nvar subReaderId = documentId;\nvar subReader = subReaders.First(sub =&gt; {\n    if (sub.MaxDoc() &lt; subReaderId) {\n        subReaderId -= sub.MaxDoc();\n        return false;\n    }\n\n    return true;\n});\n\nvar values = FieldCache_Fields.DEFAULT.GetInts(subReader, "newsdate");\nvar value = values[subReaderId];\n
381	public static void addDoc(IndexWriter w, String value)throws IOException{\n    Document doc = new Document();\n    doc.add(new Field("content", value, Field.Store.YES, Field.Index.ANALYZED));\n    w.addDocument(doc);\n}\n
382	var booleanQuery = new BooleanQuery();\nvar s1 = new TermQuery(new Term("companyName", searchQuery));\nbooleanQuery.Add(s1, Occur.MUST);\n
383	var date = new DateTime(2012, 12, 31, 0, 0, 0, DateTimeKind.Utc);\nsession.Advanced.LuceneQuery&lt;object&gt;().WhereLessThan("LastModified", date);\n
384	q=rint(product(query({!v="author:alice"}) ,100))&amp;defType=func&amp;fl=*,score&amp;sort=score desc, author desc\n
385	import re\nt1 = 'Hilary Clinton'\nt2 = 'Clinton, Hilary'\nre.search(r'(?:\s?Hilary()|\s?Clinton(),?){2}', t1)\nre.search(r'(?:\s?Hilary()|\s?Clinton(),?){2}', t2)\n
386	String doubleToPrefixCoded = NumericUtils.doubleToPrefixCoded(value);\nQuery termQ = new TermQuery(new Term(name, doubleToPrefixCoded));\n
387	IndexWriterConfig iwc = new IndexWriterConfig(LUCENE_VERSION, analyzer);\niwc.setOpenMode(IndexWriterConfig.OpenMode.CREATE_OR_APPEND);\nIndexWriter writer = new IndexWriter(directory, iwc);\n
388	@Entity\n@Indexed\npublic class MyEntity{\n    @Id\n    @Field \n    public Long id;\n\n    @Field(bridge=@FieldBridge(impl=EnumBridge.class))\n    @Enumerated(EnumType.STRING)\n    public Flavour flavour;\n}\n
389	&lt;SearchIndex class="com.day.crx.query.lucene.LuceneHandler"&gt;\n    &lt;param name="path" value="${wsp.home}/index"/&gt;\n    &lt;param name="resultFetchSize" value="50"/&gt;\n    &lt;!-- this is the new line: --&gt;\n    &lt;param name="indexingConfiguration" value="${wsp.home}/indexing_config.xml"/&gt;\n&lt;/SearchIndex&gt;\n
390	HttpSolrServer server0 = new HttpSolrServer("http://localhost:8983/solr/core0");\nHttpSolrServer server1 = new HttpSolrServer("http://localhost:8983/solr/core1");\n
391	curl -XGET localhost:9200/your_index/_mapping/your_type?fields=popRank\n
392	protected static IndexSearcher searcher = null;\n...\n\nif (searcher == null)\n{\n    searcher = new IndexSearcher(jobIndexFolderPath);\n}
393	FSDirectory dir = FSDirectory.open(new File("index"));\nIndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(),\n    true, new IndexWriter.MaxFieldLength(20));\nIndexReader reader = IndexReader.open(dir);\n\nDocument doc = new Document();\ndoc.add(new Field(\n    "text",\n    "Life #consists not in #holding good cards, but in playing those you hold well.",\n    Field.Store.NO, Field.Index.ANALYZED));\nwriter.addDocument(doc);\nwriter.close();\n\nWildcardTermEnum tagsEnum = new WildcardTermEnum(reader, new Term("text", "#*"));\ndo {\n    System.out.println(tagsEnum.term());\n} while (tagsEnum.next());\n
394	Document doc = searcher.Doc(i);\n
395	public class PrefixRewriter : QueryVisitor {\n    protected override Query VisitTermQuery(TermQuery query) {\n        var term = query.GetTerm();\n        var newQuery = new PrefixQuery(term);\n        return CopyBoost(query, newQuery);\n    }\n}\n
396	IndexSearcher searcher = new IndexSearcher("path_to_index");\nMatchAllDocsQuery everyDocClause = new MatchAllDocsQuery();\nTermQuery termClause = new TermQuery(new Term("text", "exclude_term"));\nBooleanQuery query = new BooleanQuery();\nquery.add(everyDocClause, BooleanClause.Occur.MUST);\nquery.add(termClause, BooleanClause.Occur.MUST_NOT);\nHits hits = searcher.search(query);  \n
397	P(A U B) = 1 - P(~(AUB)) \n         = 1 - P((~A) &amp; (~B))\n         = 1 - P(~A)P(~B)\n         = 1 - (1 - P(A))(1 - P(B))\n         = 1 - (1 - P(A) - P(B) + P(A)P(B))\n         = P(A) + P(B) - P(A)P(B)\n
398	protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n    Tokenizer source = new ClassicTokenizer(Version.LUCENE_40, reader);\n    TokenStream filter = new StandardFilter(Version.LUCENE_40, source);\n    filter = new LowerCaseFilter(Version.LUCENE_40,filter);\n    filter = new SynonymFilter(filter, mySynonymMap, false);\n    //Whatever other filter you want to add to the chain, being mindful of order.\n    return new TokenStreamComponents(source, filter);\n}\n
399	BooleanQuery qry = new BooleanQuery();\nqry.add(new TermQuery(new Term("keyword", "\"Annotation is cool\"")), BooleanClause.Occur.MUST);\n//Great! You have a termQuery added to the parent BooleanQuery which should find your keyword just fine!\n\nQuery q = new QueryParser(Version.LUCENE_42, "title", analyzer).parse(qry.toString());\n//Now all bets are off.\n
400	String str1 = "|| and &amp;&amp;";\nboolean hasDoublePipe = str1.contains("||");\nSystem.out.println("Has Double Pipe : " + hasDoublePipe);\nboolean hasDoubleAmp = str1.contains("&amp;&amp;");\nSystem.out.println("Has Double Ampersand : " + hasDoubleAmp);\n
401	String defaultField = ...;\nAnalyzer analyzer = ...;\nQueryParser queryParser = new QueryParser(defaultField, analyzer);\n\nqueryParser.setDefaultOperator(QueryParser.Operator.AND);\n\nQuery query = queryParser.parse("Searching is fun");\n
402	TopDocs hits = searcher.search(query, searchFilter, max);\nScoreDoc[] scoreDocs = hits.scoreDocs;\nfor (ScoreDoc scoreDoc : scoreDocs) {\n  String explanation = searcher.explain(query, scoreDoc.doc).toString();\n  Log.debug(explanation);\n}\n
403	curl -XPUT 'http://127.0.0.1:9200/test/test/1?pretty=1'  -d '\n{\n   "alt" : "John W Doe",\n   "name" : "John Doe"\n}\n'\ncurl -XPUT 'http://127.0.0.1:9200/test/test/2?pretty=1'  -d '\n{\n   "alt" : "John A Doe",\n   "name" : "My friend John Doe"\n}\n'\ncurl -XPUT 'http://127.0.0.1:9200/test/test/3?pretty=1'  -d '\n{\n   "alt" : "Susy",\n   "name" : "John"\n}\n'\ncurl -XPUT 'http://127.0.0.1:9200/test/test/4?pretty=1'  -d '\n{\n   "alt" : "John Doe",\n   "name" : "Jack"\n}\n'\n
404	Analyzer analyzer = new Analyzer() {\n @Override\n  protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n    Tokenizer source = new NGramTokenizer(reader, 12, 12);\n    TokenStream filter = new LowercaseFilter(source);\n    return new TokenStreamComponents(source, filter);\n  }\n};\n
405	public class CaseInsensitiveWhitespaceAnalyzer : Analyzer\n{\n    /// &lt;summary&gt;\n    /// &lt;/summary&gt;\n    public override TokenStream TokenStream(string fieldName, TextReader reader)\n    {\n        TokenStream t = null;\n        t = new WhitespaceTokenizer(reader);\n        t = new LowerCaseFilter(t);\n\n        return t;\n    }\n}\n
406	curl -XPOST 'http://192.168.1.115:9200/_bulk?routing=a' -d '\n{ "index" : { "_index" : "articles", "_type" : "article", "_id" : "1", "_routing" : "b"} }\n{ "title" : "value1" }\n{ "delete" : { "_index" : "articles", "_type" : "article", "_id" : "2", "_routing" : "b" } }\n{ "create" : { "_index" : "articles", "_type" : "article", "_id" : "3", "_routing" : "b" } }\n{ "title" : "value3" }\n{ "update" : {"_id" : "1", "_type" : "article", "_index" : "index1", "_routing" : "b"} }\n{ "doc" : {"field2" : "value2"} }'\n
407	using Sitecore.ContentSearch;\nusing Sitecore.Data.Items;\n\nnamespace MyProject.CMS.Custom.ContentSearch.Crawlers\n{\n    public class CustomItemCrawler : Sitecore.ContentSearch.SitecoreItemCrawler\n    {\n        protected override bool IsExcludedFromIndex(SitecoreIndexableItem indexable, bool checkLocation = false)\n        {\n            bool isExcluded = base.IsExcludedFromIndex(indexable, checkLocation);\n\n            if (isExcluded)\n                return true;\n\n            Item obj = (Item)indexable;\n\n            if (obj["Exclude From Index"] != "1") //or whatever logic you need\n                return true;\n\n            return false;\n        }\n\n        protected override bool IndexUpdateNeedDelete(SitecoreIndexableItem indexable)\n        {\n            if (base.IndexUpdateNeedDelete(indexable))\n            {\n                return true;\n            }\n\n            Item obj = indexable;\n            return obj["Exclude From Index"] == "1";\n        }\n    }\n}\n
408	        SequenceFile.Reader read = new SequenceFile.Reader(fs, new Path("&lt;path do dictionary&gt;"), conf);\n        IntWritable dicKey = new IntWritable();\n        Text text = new Text();\n        Map&lt;Integer, String&gt; dictionaryMap = new HashMap();\n        while (read.next(text, dicKey)) {\n            dictionaryMap.put(Integer.parseInt(dicKey.toString()), text.toString());\n        }\n        read.close();\n
409	&lt;field name="Cuvint" type="text" indexed="true" stored="true" required="true"/&gt;\n
410	/*\n* To change this template, choose Tools | Templates\n* and open the template in the editor.\n*/\n\n\n\nimport java.io.*;\nimport java.util.*;\nimport org.apache.lucene.analysis.Analyzer;\nimport org.apache.lucene.analysis.Token;\nimport org.apache.lucene.analysis.TokenStream;\nimport org.apache.lucene.analysis.WhitespaceAnalyzer;\nimport org.apache.lucene.analysis.standard.StandardAnalyzer;\nimport org.apache.lucene.analysis.tokenattributes.TermAttribute;\nimport org.apache.lucene.document.Document;\nimport org.apache.lucene.document.Field;\nimport org.apache.lucene.index.*;\nimport org.apache.lucene.queryParser.ParseException;\nimport org.apache.lucene.queryParser.QueryParser;\nimport org.apache.lucene.search.*;\nimport org.apache.lucene.store.NIOFSDirectory;\nimport org.apache.lucene.util.Version;\n\n\n/*\n* Date Author Changes April 14, 2012 Kasun Perera Created\n*/\n\n/*\n*\n* Class contains methods for indexing documents with Lucene, and    calculating\n* TFIDF weights\n*/\npublic class DocIndexer {\n\nprivate String docNames[];\nprivate String docIDS[];\nprivate String pathToIndex;\nprivate String pathToDocumentCollection;\nprivate String fiboTermList[]; //marked up fibo terms\nprivate String taxoTermList[]; // marked up taxonomy terms\nprivate RAMDirectory ramMemDir;\nprivate String fileNames[];\nprivate byte files[][];\nprivate String filesInText[];\nint noOfWordsOfDOc[];\nint noOfSentencesOfDoc[];\nArrayList&lt;String&gt; ArrLstSentencesOfDoc[];\nString removedTermsOfDOc[][];\nint freqAfterRemovalOfDoc[][];\n//int queryDocIndex ;\nprivate int curDocNo;\nprivate final int maxTerms = 1000000;\n\n\n\n\n/**\n * Constructor used when indexing directory is a RAM memory directory, We\n * need RAM directory because Stratoes Server dosen't allow access local\n * files\n *\n * @param pathToIndex- doc index path \n * @param pathToDocumentCollection - doccollection path\n */\npublic DocIndexer(String pathToIndex, String pathToDocumentCollection) {\n  //  this.docNames = docNames;\n\n    //this.bufPathToIndex= new RandomAccessBuffer() ;\n  //  this.ramMemDir = new RAMDirectory();\n    this.pathToIndex = pathToIndex;\n    this.pathToDocumentCollection= pathToDocumentCollection;\n    // this.files = files;\n   // this.filesInText = docContent;\n\n}\n\n\n\n\n/**\n * Count the number of words in a given String\n *\n * @param line- Input String\n * @return - number of words in the input String\n */\nprivate int wordCount(String line) {\n    int numWords = 0;\n    int index = 0;\n    boolean prevWhiteSpace = true;\n    while (index &lt; line.length()) {\n        char c = line.charAt(index++);\n        boolean currWhiteSpace = Character.isWhitespace(c);\n        if (prevWhiteSpace &amp;&amp; !currWhiteSpace) {\n            numWords++;\n        }\n        prevWhiteSpace = currWhiteSpace;\n    }\n    return numWords;\n}\n\n/*\n*given it's URL this methods read the text files\n*/\npublic static String fileReader(String filename) throws IOException {\n\n    String filetext = null;\n    BufferedReader reader = null;\n    //BufferedReader namesReader; //reader for followers\n    //Extractor extractor = new Extractor();\n    File inFile = new File(filename);\n    //File namesFile = new File(args[1]); //get followers file \n    //File userFile = new File(args[1]);\n\n    //READING FROM USERS FILE\n    reader = new BufferedReader(new FileReader(inFile));\n    String line = null;\n\n    int numLine = 0;\n\n    while ((line = reader.readLine()) != null) {\n        // numLine++;\n        filetext = filetext + " " + line;\n\n        // System.out.println(line);\n    }\n\n    reader.close();\n    return filetext;\n}\n\n/**\n * Method to index the documents only using the content of the document\n * "docid" field is used for indexing, since Lucene Dosen't retrieve the\n * documents in the indexed order \n *\n * @param docNo- document number of the document to be indexed\n * @throws IOException\n */\n public void indexDocs() throws IOException {\n    //String pathToDocumentCollection = "F:\\karsha project\\Term Analysis\\keygraph docs\\selected_section_collection\\compelete_collection_2\\msrb_fibo_stopwords_replaced_term_docs\\";\n   // String pathToIndex = "F:\\karsha project\\Term Analysis\\keygraph docs\\selected_section_collection\\compelete_collection_2\\INDEX_msrb_fibo_stopwords_replaced_term_docs";\n    File folder = new File(pathToDocumentCollection);\n    File[] listOfFiles = folder.listFiles();\n    int noOfFiles = listOfFiles.length;\n    System.out.println("Number of files : " + noOfFiles);\n\n    IndexWriter iW;\n    int indexDocCount = 0;\n    try {\n        NIOFSDirectory dir = new NIOFSDirectory(new File(pathToIndex));\n        iW = new IndexWriter(dir, new IndexWriterConfig(Version.LUCENE_36, new WhitespaceAnalyzer(Version.LUCENE_36)));\n\n        for (int i = 0; i &lt; noOfFiles; i++) {\n            if (listOfFiles[i].isFile()) {\n                String docName = listOfFiles[i].getName();\n                System.out.println("doc name: " + docName + "length - " + listOfFiles[i].length());\n                if (listOfFiles[i].length() &gt; 1) {\n                    String filesInText = fileReader(pathToDocumentCollection + docName);\n\n                    //docIds[i] = docNames[i].substring( 0, docName.length() - 4 );\n                    System.out.println("Added to index : " + docName);\n\n                    //  StringReader strRdElt = new StringReader(filesInText[i]);\n                    //filesInText = filesInText.replaceAll( "[^A-Za-z_]", " " );\n                    //System.out.println( "Added to index : " + docName );\n                    StringReader strRdElt = new StringReader(filesInText.replaceAll("\\d+(?:[.,]\\d+)*\\s*", ""));\n                    StringReader docId = new StringReader(docName.substring(0, docName.length() - 4)); // give a unique doc Id here\n\n                    org.apache.lucene.document.Document doc = new org.apache.lucene.document.Document();\n\n                    doc.add(new Field("doccontent", strRdElt, Field.TermVector.YES));\n                    doc.add(new Field("docid", docId, Field.TermVector.YES));\n                    iW.addDocument(doc);\n                    indexDocCount++;\n                }\n            }\n        }\n\n        System.out.println("no of documents added to index : " + indexDocCount);\n\n        iW.close();\n        // dir.close() ;\n    } catch (CorruptIndexException e) {\n        e.printStackTrace();\n    } catch (IOException e) {\n        e.printStackTrace();\n    }\n}\n\n\n\n/**\n * This method calculates the TF-IDF score for each terms in the indexed\n * documents\n *\n * @param numberOfDocs\n * @return - Hashmap of TF-IDF score per each term in document wise\n * @throws CorruptIndexException\n * @throws ParseException\n */\npublic HashMap&lt;Integer, HashMap&gt; tfIdfScore(int numberOfDocs) throws CorruptIndexException, ParseException {\n\n    int noOfDocs = docNames.length;\n\n    HashMap&lt;Integer, HashMap&gt; scoreMap = new HashMap&lt;Integer, HashMap&gt;();\n    //HashMap&lt;Integer, float[]&gt; scoreMap = new HashMap&lt;Integer, float[]&gt;();\n\n\n    try {\n\n        IndexReader re = IndexReader.open(NIOFSDirectory.open(new File(pathToIndex)), true) ;\n       // IndexReader re = IndexReader.open(ramMemDir);\n\n        int i = 0;\n        for (int k = 0; k &lt; numberOfDocs; k++) {\n            int freq[];\n            TermFreqVector termsFreq;\n            TermFreqVector termsFreqDocId;\n            //TermFreqVector termsFreq3[];\n            HashMap&lt;String, Float&gt; wordMap = new HashMap&lt;String, Float&gt;();\n            String terms[];\n            float score[] = null;\n\n            //termsFreq3=re.getTermFreqVectors(currentDocID);\n            termsFreq = re.getTermFreqVector(k, "doccontent");\n            termsFreqDocId = re.getTermFreqVector(k, "docid");\n\n            int aInt = Integer.parseInt(termsFreqDocId.getTerms()[0]);\n            freq = termsFreq.getTermFrequencies();\n\n            terms = termsFreq.getTerms();\n\n            int noOfTerms = terms.length;\n            score = new float[noOfTerms];\n            DefaultSimilarity simi = new DefaultSimilarity();\n            for (i = 0; i &lt; noOfTerms; i++) {\n                int noofDocsContainTerm = re.docFreq(new Term("doccontent", terms[i]));\n                // System.out.println(terms[i]+"\t"+freq[i]);\n                //int noofDocsContainTerm = docsContainTerm(terms[i], "docnames");\n                float tf = simi.tf(freq[i]);\n                float idf = simi.idf(noofDocsContainTerm, noOfDocs);\n                wordMap.put(terms[i], (tf * idf));\n\n            }\n            scoreMap.put(aInt, wordMap);\n        }\n\n\n    } catch (IOException e) {\n        // score = null;\n        e.printStackTrace();\n    }\n\n\n\n    //Map&lt;Integer,Float[]&gt; scoreMap=new Map&lt;Integer, Float[]&gt;(); \n\n\n    return scoreMap;\n}\n\n\npublic HashMap&lt;Integer, HashMap&gt; getTFIDF() throws IOException, CorruptIndexException, ParseException, ClassNotFoundException {\n    int noOfDocs = docNames.length;\n    float tfIdfScore[][] = new float[noOfDocs][];\n    //HashMap&lt;Integer, float[]&gt; scoreMap = new HashMap&lt;Integer, float[]&gt;();\n    HashMap&lt;Integer, HashMap&gt; scoreMap = new HashMap&lt;Integer, HashMap&gt;();\n\n\n    scoreMap = tfIdfScore(noOfDocs);\n\n\n\n\n    return  scoreMap;\n}\n
411	 q=field_name:determine\n
412	&lt;requestHandler name="search" class="solr.SearchHandler" default="true"&gt;\n &lt;lst name="defaults"&gt;\n   &lt;str name="echoParams"&gt;explicit&lt;/str&gt;\n   &lt;str name="defType"&gt;dismax&lt;/str&gt;\n   &lt;str name="qf"&gt;\n      title^1 content^0.8\n   &lt;/str&gt;\n   &lt;str name="q.alt"&gt;*:*&lt;/str&gt;\n   &lt;str name="rows"&gt;10&lt;/str&gt;\n   &lt;str name="fl"&gt;*,score&lt;/str&gt;\n &lt;/lst&gt;\n&lt;/requestHandler&gt;\n
413	  public class ArchiveIndex {\n      private IndexSearcher search;\n      private AtomicInteger activeSearches = new AtomicInteger(0);\n      private IndexWriter writer;\n      private AtomicInteger activeWrites = new AtomicInteger(0);\n\n      public List&lt;Document&gt; search( ... ) {\n          synchronized( this ) {\n              if( search != null &amp;&amp; !search.getIndexReader().isCurrent() &amp;&amp; activeSearches.get() == 0 ) {\n                 searcher.close();\n                 searcher = null;\n              }\n\n              if( search == null ) {\n                  searcher = new IndexSearcher(...);\n              }\n          }\n\n          activeSearches.increment();\n          try {\n              // do you searching\n          } finally {\n              activeSearches.decrement();\n          }\n          // do you searching\n      }\n\n\n      public void addDocuments( List&lt;Document&gt; docs ) {\n          synchronized( this ) {\n             if( writer == null ) {\n                 writer = new IndexWriter(...);\n             }\n          }\n          try {\n              activeWrites.incrementAndGet();\n              // do you writes here.\n          } finally {\n              synchronized( this ) {\n                  int writers = activeWrites.decrementAndGet();\n                  if( writers == 0 ) {\n                      writer.close();\n                      writer = null;\n                  }\n              }\n          }\n      }\n  }\n
414	var escaped = query.replace(/([\!\*\+\&amp;\|\(\)\[\]\{\}\^\~\?\:\"])/g, "\\$1");    \n
415	List&lt;Term&gt; terms = new List&lt;Term&gt;();    //will be filled with non-matched terms\nList&lt;Term&gt; hitTerms = new List&lt;Term&gt;(); //will be filled with matched terms\nGetHitTerms(query, searcher,docId, hitTerms,terms);\n
416	&lt;bool name="distrib"&gt;false&lt;/bool&gt; \n
417	    &lt;requestHandler name="/select" class="solr.SearchHandler"&gt;\n        &lt;!-- default values for query parameters can be specified, these\n                 will be overridden by parameters in the request\n            --&gt;\n         &lt;lst name="defaults"&gt;\n             &lt;int name="rows"&gt;10&lt;/int&gt;\n             &lt;str name="qf"&gt;content short_description&lt;/str&gt;\n         &lt;/lst&gt;\n    &lt;/requestHandler&gt;\n
418	public class John2Transformer : AbstractTransformerCreationTask&lt;John&gt;\n{\n      public John2Transformer ()\n      {\n          TransformResults = johns =&gt; from john in johns\n                                      from cat in john.Cats\n                                      select new\n                                      {\n                                          Name = john.Name,\n                                          Cat = cat\n                                      };\n      }\n}\n
419	Record lastRecord = ...;\n\nQueryBuilder queryBuilder = fullTextSession\n    .getSearchFactory()\n    .buildQueryBuilder()\n    .forEntity(Record.class).get();\n\nQuery query = queryBuilder.range()\n    .onField( "created" )\n    .above( lastRecord.getCreated() )\n    .createQuery();\n\nFullTextQuery fullTextQuery = fulllTextSession.createFullTextQuery(\n    query, Record.class\n);\n
420	            long modified = file.lastModified();\n            doc.add(new LongField(FILE_MODIFIED, modified, Field.Store.YES));\n
421	Query query1 = new TermQuery(new Term("your_default_field", "apple"));\nquery1.setBoost(0.2);\n\nQuery query2 = new TermQuery(new Term("your_default_field", "growers"));\nquery2.setBoost(0.7);\n\nQuery query3 = new TermQuery(new Term("your_default_field", "fruit"));\nquery3.setBoost(0.9);\n\nQuery query4 = new TermQuery(new Term("your_default_field", "ipad"));\nquery4.setBoost(0.05);\n\nQuery query5 = new TermQuery(new Term("your_default_field", "mac"));\nquery5.setBoost(0.06);\n\nBooleanQuery combining = new BooleanQuery();\ncombining.add(query1, Occur.SHOULD);  \ncombining.add(query2, Occur.SHOULD);  // and so on and so forth\n
422	{\n    "settings" : {\n        "index" : {\n            "number_of_shards" : 1,\n            "number_of_replicas" : 1\n        },  \n        "analysis" : {\n            "filter" : {\n                "custom_filter" : {\n                    "type" : "word_delimiter",\n                    "type_table": ["&gt; =&gt; ALPHA", "&lt; =&gt; ALPHA"]\n                }   \n            },\n            "analyzer" : {\n                "custom_analyzer" : {\n                    "type" : "custom",\n                    "tokenizer" : "whitespace",\n                    "filter" : ["lowercase", "custom_filter"]\n                }\n            }\n        }\n    },\n    "mappings" : {\n        "my_type" : {\n            "properties" : {\n                "msg" : {\n                    "type" : "string",\n                    "analyzer" : "custom_analyzer"\n                }\n            }\n        }\n    }\n}\n
423	function (doc) {\n   if (doc.attributeCollection &amp;&amp; doc.attributeCollection.attributeArray) {\n      for (var i=0; i&lt;doc.attributeCollection.attributeArray.length; i++) {\n         if (doc.attributeCollection.attributeArray[i].name) {\n            index("name", doc.attributeCollection.attributeArray[i].name, { store : true });\n         }\n         if (doc.attributeCollection.attributeArray[i].value) {\n            for (var j=0; j&lt;doc.attributeCollection.attributeArray[i].value.length; j++) {\n               index("value", doc.attributeCollection.attributeArray[i].value[j], { store : true });\n            }\n         }\n      }\n   }\n}\n
424	 "script" : "ctx._source.CountryName = 'Cote dIvoire'"\n
425	GET 'index/_analyze?field=quote' -d "I learned a lot at school"\n
426	public final class RomanianASCIIAnalyzer extends StopwordAnalyzerBase {\n  private final CharArraySet stemExclusionSet;\n\n  public final static String DEFAULT_STOPWORD_FILE = "stopwords.txt";\n  private static final String STOPWORDS_COMMENT = "#";\n\n  public static CharArraySet getDefaultStopSet(){\n    return DefaultSetHolder.DEFAULT_STOP_SET;\n  }\n\n  private static class DefaultSetHolder {\n    static final CharArraySet DEFAULT_STOP_SET;\n\n    static {\n      try {\n        DEFAULT_STOP_SET = loadStopwordSet(false, RomanianAnalyzer.class, \n            DEFAULT_STOPWORD_FILE, STOPWORDS_COMMENT);\n      } catch (IOException ex) {\n        throw new RuntimeException("Unable to load default stopword set");\n      }\n    }\n  }\n\n  public RomanianASCIIAnalyzer() {\n    this(DefaultSetHolder.DEFAULT_STOP_SET);\n  }\n\n  public RomanianASCIIAnalyzer(CharArraySet stopwords) {\n    this(stopwords, CharArraySet.EMPTY_SET);\n  }\n\n  public RomanianASCIIAnalyzer(CharArraySet stopwords, CharArraySet stemExclusionSet) {\n    super(stopwords);\n    this.stemExclusionSet = CharArraySet.unmodifiableSet(CharArraySet.copy(stemExclusionSet));\n  }\n\n  @Override\n  protected TokenStreamComponents createComponents(String fieldName) {\n    final Tokenizer source = new StandardTokenizer();\n    TokenStream result = new StandardFilter(source);\n    result = new LowerCaseFilter(result);\n    result = new StopFilter(result, stopwords);\n    if(!stemExclusionSet.isEmpty())\n      result = new SetKeywordMarkerFilter(result, stemExclusionSet);\n    result = new SnowballFilter(result, new RomanianStemmer());\n//This following line is the addition made to the RomanianAnalyzer source.\n    result = new ASCIIFoldingFilter(result); \n    return new TokenStreamComponents(source, result);\n  }\n}\n
427	@Override\nprotected Reader initReader(String fieldName, Reader reader) {\n    return mStripHTML ? new HTMLStripCharFilter(reader) : reader;\n}\n\n@Override\nprotected TokenStreamComponents createComponents(String fieldName) \n{\n    StandardTokenizer source = new StandardTokenizer();\n    source.setMaxTokenLength(maxTokenLength);\n    TokenStream result = new StandardFilter(source);\n    result = new LowerCaseFilter(result);\n    return new TokenStreamComponents(source, result);\n}\n
428	&lt;fieldType name="brand" class="solr.TextField"&gt;\n    &lt;analyzer type="index"&gt;\n        &lt;tokenizer class="solr.KeywordTokenizerFactory"/&gt;\n        &lt;charFilter class="solr.PatternReplaceCharFilterFactory" pattern="(\s)" replacement="_"/&gt;\n    &lt;/analyzer&gt;\n    &lt;analyzer type="query"&gt;\n        &lt;tokenizer class="solr.KeywordTokenizerFactory"/&gt;\n        &lt;charFilter class="solr.PatternReplaceCharFilterFactory" pattern="(\s)" replacement="_"/&gt;\n        &lt;filter class="solr.SynonymFilterFactory" synonyms="synonym-brand.txt" ignoreCase="false" expand="false"/&gt;\n    &lt;/analyzer&gt;\n&lt;/fieldType&gt;\n
429	     EmbeddedDriver embeddedDriver = (EmbeddedDriver) Components.driver();\n     GraphDatabaseService databaseService = embeddedDriver.getGraphDatabaseService();\n     try (Transaction t = databaseService.beginTx()) {\n      Index&lt;Node&gt; autoIndex = databaseService.index().forNodes("node_auto_index");\n      databaseService.index().setConfiguration(autoIndex, "type", "fulltext");\n      databaseService.index().setConfiguration(autoIndex, "to_lower_case", "true");\n      databaseService.index().setConfiguration(autoIndex, "analyzer", StandardAnalyzerV36.class.getName());\n      t.success();\n     }\n
430	@Field(bridge = @FieldBridge(impl = CollectionOfStringsFieldBridge.class), analyze = Analyze.NO)\n@Facet\npublic List&lt;String&gt; getAggregatedField() {\n     return yourAggregatedValue;\n}\n
431	def getTopTFIDFTerms(docID, reader):\n    termVector = reader.getTermVector(docID, "contents");\n    termsEnumvar = termVector.iterator(None)\n    termsref = BytesRefIterator.cast_(termsEnumvar)\n    tc_dict = {}                     # Counts of each term\n    dc_dict = {}                     # Number of docs associated with each term\n    tfidf_dict = {}                  # TF-IDF values of each term in the doc\n    N_terms = 0\n    try:\n        while (termsref.next()):\n            termval = TermsEnum.cast_(termsref)\n            fg = termval.term().utf8ToString()       # Term in unicode\n            tc = termval.totalTermFreq()             # Term count in the doc\n\n            # Number of docs having this term in the index\n            dc = reader.docFreq(Term("contents", termval.term())) \n            N_terms = N_terms + 1 \n            tc_dict[fg]=tc\n            dc_dict[fg]=dc\n    except:\n        print 'error in term_dict'\n\n    # Compute TF-IDF for each term\n    for term in tc_dict:\n        tf = tc_dict[term] / N_terms\n        idf = 1 + math.log(N_DOCS_INDEX/(dc_dict[term]+1)) \n        tfidf_dict[term] = tf*idf\n\n    # Here I get a representation of the sorted dictionary\n    sorted_x = sorted(tfidf_dict.items(), key=operator.itemgetter(1), reverse=True)\n\n    # Get the top 5 \n    top5 = [i[0] for i in sorted_x[:5]] # replace 5 by TOP N\n
432	var analyzer = new ShingleAnalyzerWrapper(\n        new StandardAnalyzer(Lucene.Net.Util.Version.LUCENE_30),\n        2);\n
433	 QueryBuilder qb = fullTextEntityManager.getSearchFactory()\n        .buildQueryBuilder().forEntity(Book.class)\n        .overridesForField("title","analyzerName")\n        .get();\n
434	Query wildcardQuery = new WildcardQuery(new Term("contents", searchString));\nTopDocs hits = searcher.search(wildcardQuery, 20);\n
435	// get a IndexSearcher for searching\nIndexSearcher searcher = searcherManager.aquire();\n\n// release IndexSearcher after search\nsearcherManager.release(searcher);\n\n// refresh and add new index records to next search. usually after a commit \nsearcherManager.maybeRefresh();\n
436	protected boolean accept() throws IOException {\n            String token = new String(termAtt.buffer(), 0 ,termAtt.length());\n            if (token.matches("[0-9,.]+")) {\n                return false;\n            }\n            return true;\n        }\n
437	 @TokenFilterDef(factory = WordDelimiterFilterFactory.class,\n                        params = {\n                                @Parameter(name = "catenateAll", value = "1"),\n                                @Parameter(name = "generateWordParts", value = "0")})//generateWordParts = 1 by default\n
438	@Indexed   \nclass Patient {\n    ...\n    @Field\n    protected String firstName = "";\n    @Field\n    protected String lastName = "";\n    @Field\n    protected String secondLastName = "";\n    ...\n    @javax.persistence.Transient\n    @Field\n    public String getFullName() {\n        // TODO null safety\n        return firstName + " " + lastName + " " + secondLastName;\n    }\n    ...\n}\n
439	PUT /my_index\n{                                                                                     \n  "settings": {                                                                                                                                    \n    "analysis": {\n      "analyzer": {                                                                                                                                \n        "my_analyzer": {                                                                                                                           \n          "tokenizer": "standard",\n          "char_filter": ["replace_dots"]\n        }\n      },\n      "char_filter": {\n        "replace_dots": {\n          "type": "mapping",\n          "mappings": [\n            ". =&gt; \\u0020"\n          ]\n        }\n      }\n    }\n  }\n}\n\nPOST /my_index/_analyze\n{                                                                           \n  "analyzer": "my_analyzer",                                            \n  "text": "8080 amit.foobar.getFooLabelFrombar(test.java:91)"\n}\n
440	var orderBy = new Term("MyField", string.Empty);\nusing (var reader = IndexReader.Open(FSDirectory.Open(_indexPath), true))\nusing (var termEnum = reader.Terms(orderBy))\nusing (var stream = new FileStream("TheFile.txt", FileMode.Create, FileAccess.Write))\nusing (var writer = new StreamWriter(stream))\n{\n    for (var term = termEnum.Term; term != null; termEnum.Next(), term = termEnum.Term)\n    {\n        if (term.Field != "MyField")\n        {\n            break;\n        }\n        writer.WriteLine(term.Text);\n    }\n}\n
441	=projectName\:title:"*passement*"\n
442	public class ActualOrProvisionalDateSortComparator : ScoreDocComparator\n{\n    private readonly StringIndex actualDates;\n    private readonly StringIndex provisionalDates;\n\n    public TxOrCreatedDateSortComparator(IndexReader reader, FieldCache fieldCache)\n    {\n        actualDates = fieldCache.GetStringIndex(reader, "actualDate");\n        provisionalDates = fieldCache.GetStringIndex(reader, "provisionalDate");\n    }\n\n    public int Compare(ScoreDoc i, ScoreDoc j)\n    {\n        var date1 = GetValue(i.doc);\n        var date2 = GetValue(j.doc);\n\n        return date1.CompareTo(date2);\n    }\n\n    public IComparable SortValue(ScoreDoc i)\n    {\n        return GetValue(i.doc);\n    }\n\n    public int SortType()\n    {\n        return SortField.CUSTOM;\n    }\n\n    private string GetValue(int doc)\n    {\n        return actualDates.Lookup[actualDates.Order[doc]] ?? provisionalDates.Lookup[provisionalDates.Order[doc]];\n    }\n}\n
443	class ADomainClass {\n    Date publishedOn\n\n    static searchable = {\n       'publishedOn' format:'yyyyMMdd'\n       'publishedOn' name: 'createdAt', format 'yyyyMMdd'\n    }\n}\n
444	IndexWriter writer = ...\nString id = ...\nString[] lines = ...\nDocument doc = new Document();\ndoc.add(new Field("id", id, Store.YES, Index.NOT_ANALYZED, TermVector.NO );\nfor (String line: lines) {\n    doc.add(new Field("text", line, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS);\n}\nwriter.addDocument(doc);\n
445	&lt;SearchIndex class="org.apache.jackrabbit.core.query.lucene.SearchIndex"&gt;\n  &lt;param name="path" value="${wsp.home}/index"/&gt;\n  &lt;param name="supportHighlighting" value="true"/&gt;\n  &lt;param name="tikaConfigPath" value="${wsp.home}/tika-config.xml"/&gt;\n&lt;/SearchIndex&gt;\n
446	java -Ddata=args  -jar post.jar "&lt;add&gt;&lt;doc&gt;&lt;field name=\"id\"&gt;17&lt;/field&gt;&lt;/doc&gt;&lt;/add&gt;"\n
447	#usr/bin/perl\nuse strict;\nuse warnings;\nuse Plucene::Simple;\nmy $content_1 = $ARGV[0];\nmy $content_2 = $ARGV[1];\nmy %documents;\n\n %documents = (\n"".$content_2 =&gt; { \n\n                     content =&gt; $content_1\n                   }\n);\n\nprint $content_1;\nmy $index = Plucene::Simple-&gt;open( "solutions" );\nfor my $id (keys %documents) \n{\n        $index-&gt;add($id =&gt; $documents{$id});\n}\n $index-&gt;optimize;\n
448	String json = \n{"lhs":{"lhs":{"lhs":{"lhs":"Field","rhs":"Value","operator":"EQUAL_TO"},"rhs":{"lhs":"Citation","rhs":"Citation","operator":"EQUAL_TO"},"operator":"AND"},"rhs":{"lhs":"Report","rhs":"Report1","operator":"EQUAL_TO"},"operator":"AND"},"rhs":{"lhs":"Year","rhs":"2001","operator":"EQUAL_TO"},"operator":"OR"}\n
449	&lt;?php\n\n$input1 = "Hello there, this is a test 1, you see it's almost the same";\n$input2 = "Hello there, this is a test 2, you saw it, it's almost the same";\n$input3 = "this is very different from the others, but who knows ?";\n\necho jackard($input1, $input1) . "&lt;br /&gt;"; // results 1\n\necho jackard($input1, $input2) . "&lt;br /&gt;"; // results 0.81481481481481\n\necho jackard($input1, $input3) . "&lt;br /&gt;"; // results 0.25\n\necho jackard($input2, $input3); // results 0.24\n\n\nfunction jackard($a, $b){\n    $a_arr = explode(" ", $a);\n    $b_arr = explode(" ", $b);\n    $intersect_a_b = array_intersect($a_arr,$b_arr);\n    return((count($intersect_a_b)/(count($a_arr)+count($b_arr)))*2);\n}\n?&gt;\n
450	var myField = doc.GetFieldable("mynumber").StringValue();\nvar val = Int32.Parse(myField);\n
451	curl -X POST http://&lt;host&gt;:7474/db/data/ext/CypherPlugin/graphdb/execute_query -H "Content-Type: application/json" --databinary '{\n    "query": ca=node:emp(NUM_OFC_CA = {num_ofc_ca}) RETURN distinct ca.NME_CA as `CAName`",\n    "params": {\n        "num_ofc_ca": "997015"\n    }\n}'\n
452	Filter fil= new QueryWrapperFilter(new TermQuery( new Term(field, "5/12/1998")));\nvar hits = indexSearcher.Search(QueryMaker(searchString, searchfields), fil);\n
453	def searchResults = Job.search(query,param)\n
454	&lt;index id="help_pages_index" type="Sitecore.Search.Index, Sitecore.Kernel"&gt;\n    &lt;param desc="name"&gt;$(id)&lt;/param&gt;\n    &lt;param desc="folder"&gt;help_pages_index&lt;/param&gt;\n    &lt;Analyzer ref="search/analyzer" /&gt;\n    &lt;locations hint="list:AddCrawler"&gt;\n        &lt;web-help type="Sitecore.Search.Crawlers.DatabaseCrawler,Sitecore.Kernel"&gt;\n            &lt;Database&gt;web&lt;/Database&gt;\n            &lt;Root&gt;/sitecore/content/Pages/Help&lt;/Root&gt;\n            &lt;templates hint="list:IncludeTemplate"&gt;\n                &lt;helparticle&gt;{FF48919D-393C-4F8D-9D1A-AC6B58CEC896}&lt;/helparticle&gt;\n                &lt;helpfaq&gt;{E66F97CF-18CF-4D73-AC84-0064D7626524}&lt;/helpfaq&gt;\n            &lt;/templates&gt;\n        &lt;/web-help&gt;\n    &lt;/locations&gt;\n&lt;/index&gt;\n
455	@volatile protected var LAST_SEARCHER_REFRESH = 0\nprotected lazy val SEARCHER_MANAGER = new SearcherManager (LUCENE_DIR, new SearcherFactory {\n  override def newSearcher (reader: IndexReader): IndexSearcher = {new IndexSearcher (reader)}\n})\n\n...\n\nif (LAST_SEARCHER_REFRESH != Time.relativeSeconds()) {LAST_SEARCHER_REFRESH = Time.relativeSeconds(); SEARCHER_MANAGER.maybeRefresh()}\nval searcher = SEARCHER_MANAGER.acquire(); try {\n  searcher.search (query, collector)\n  ...\n} finally {SEARCHER_MANAGER.release (searcher)}\n
456	/*\n * To change this template, choose Tools | Templates\n * and open the template in the editor.\n */\npackage com.computergodzilla.highlighter;\n\nimport java.io.File;\nimport java.io.IOException;\nimport org.apache.lucene.analysis.Analyzer;\nimport org.apache.lucene.analysis.TokenStream;\nimport org.apache.lucene.analysis.standard.StandardAnalyzer;\nimport org.apache.lucene.document.Document;\nimport org.apache.lucene.index.DirectoryReader;\nimport org.apache.lucene.index.IndexReader;\nimport org.apache.lucene.queryparser.classic.ParseException;\nimport org.apache.lucene.queryparser.classic.QueryParser;\nimport org.apache.lucene.search.IndexSearcher;\nimport org.apache.lucene.search.Query;\nimport org.apache.lucene.search.TopDocs;\nimport org.apache.lucene.search.highlight.Highlighter;\nimport org.apache.lucene.search.highlight.InvalidTokenOffsetsException;\nimport org.apache.lucene.search.highlight.QueryScorer;\nimport org.apache.lucene.search.highlight.SimpleHTMLFormatter;\nimport org.apache.lucene.search.highlight.TextFragment;\nimport org.apache.lucene.search.highlight.TokenSources;\nimport org.apache.lucene.store.FSDirectory;\nimport org.apache.lucene.util.Version;\n\n/**\n * Example of Lucene Highlighter\n * @author Mubin Shrestha\n */\npublic class LuceneHighlighter {\n\n    public void highLighter() throws IOException, ParseException, InvalidTokenOffsetsException {\n        IndexReader reader = DirectoryReader.open(FSDirectory.open(new File("D:/INDEXDIRECTORY")));\n        Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_42);\n        IndexSearcher searcher = new IndexSearcher(reader);\n        QueryParser parser = new QueryParser(Version.LUCENE_42, "ncontent", analyzer);\n        Query query = parser.parse("going");\n        TopDocs hits = searcher.search(query, reader.maxDoc());\n        System.out.println(hits.totalHits);\n        SimpleHTMLFormatter htmlFormatter = new SimpleHTMLFormatter();\n        Highlighter highlighter = new Highlighter(htmlFormatter, new QueryScorer(query));\n        for (int i = 0; i &lt; reader.maxDoc(); i++) {\n            int id = hits.scoreDocs[i].doc;\n            Document doc = searcher.doc(id);\n            String text = doc.get("ncontent");\n            TokenStream tokenStream = TokenSources.getAnyTokenStream(searcher.getIndexReader(), id, "ncontent", analyzer);\n            TextFragment[] frag = highlighter.getBestTextFragments(tokenStream, text, false, 4);\n            for (int j = 0; j &lt; frag.length; j++) {\n                if ((frag[j] != null) &amp;&amp; (frag[j].getScore() &gt; 0)) {\n                    System.out.println((frag[j].toString()));\n                }\n            }\n            //Term vector\n            text = doc.get("content");\n            tokenStream = TokenSources.getAnyTokenStream(searcher.getIndexReader(), hits.scoreDocs[i].doc, "content", analyzer);\n            frag = highlighter.getBestTextFragments(tokenStream, text, false, 4);\n            for (int j = 0; j &lt; frag.length; j++) {\n                if ((frag[j] != null) &amp;&amp; (frag[j].getScore() &gt; 0)) {\n                    System.out.println((frag[j].toString()));\n                }\n            }\n        }\n    }\n}\n
457	http://website/sitecore/shell/Applications/Buckets/Services/Search.ashx\n?callback=jQuery110209093581063207239_1392643910752\n&amp;q%5B0%5D%5Btype%5D=text\n&amp;q%5B0%5D%5Bvalue%5D=mysearchword\n&amp;q%5B0%5D%5Boperation%5D=must\n&amp;pageNumber=0\n&amp;type=Query\n&amp;pageSize=20\n&amp;version=1\n&amp;id=%7B5A0DDAAF-8438-4E8A-BA93-4F30212099D4%7D\n&amp;indexName=\n&amp;sc_content=master\n&amp;_=1392643910762\n
458	@Test\npublic void testDiscoverArtworksByTitle()\n{\n    FullTextEntityManager ftem = Search.getFullTextEntityManager(this.entityManager);\n\n    this.prepareArtworksForListing();\n    this.entityManager.flush(); // &lt;-- ADDED FLUSH() CALL\n    ftem.flushToIndexes();\n\n    try\n    {\n        List&lt;ArtworkListItem&gt; listItems = this.artworkService.discoverArtworksByTitle("Die Hard");\n        Assert.assertNotEquals("There are some items in the list", 0, listItems.size());\n    }\n    finally\n    {\n        // housekeeping\n        ftem.purgeAll(ArtworkEntity.class);\n    }\n}\n
459	public final class StopFilter extends FilteringTokenFilter {\n\n    private final CharArraySet stopWords;\n    private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n\n    public StopFilter(Version matchVersion, TokenStream in, CharArraySet stopWords) {\n        super(matchVersion, in);\n        this.stopWords = stopWords;\n    }\n\n    @Override\n    protected boolean accept() {\n        return !stopWords.contains(termAtt.buffer(), 0, termAtt.length());\n    }\n}\n
460	TokenFilter filter = new ICUTransformFilter(\n    previousFilter, \n    Transliterator.getInstance("Cyrillic-Latin")\n);\n
461	import org.apache.lucene.search.similarities.DefaultSimilarity;\n\npublic class EnhancedTFIDFSimilarity extends DefaultSimilarity {\n\n    @Override\n    public float coord(int overlap, int maxOverlap) {\n        float _coord = (overlap / (float)maxOverlap);    \n        return (_coord * _coord);\n    }\n}\n
462	void OnApplicationStarted(UmbracoApplicationBase app, ApplicationContext ctx)\n{\n    ExamineManager.Instance\n                  .IndexProviderCollection["ExternalIndexer"]\n                  .GatheringNodeData += OnGatheringNodeData;\n}\n
463	&lt;Context docBase="/usr/share/solr/example/multicore/solr.war" debug="0" crossContext="true"&gt;\n  &lt;Environment name="solr/home" type="java.lang.String" value="/usr/share/solr/data" override="true" /&gt;\n&lt;/Context&gt;\n
464	/**\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the "License"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an "AS IS" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Ported to C# from Java source at http://grepcode.com/file/repo1.maven.org/maven2/org.apache.lucene/lucene-misc/3.0.3/org/apache/lucene/queryParser/complexPhrase/ComplexPhraseQueryParser.java\n\n\nusing Lucene.Net.Analysis;\nusing Lucene.Net.Index;\n\nusing Lucene.Net.QueryParsers;\nusing Lucene.Net.Search;\nusing Lucene.Net.Search.Spans;\nusing System;\nusing System.Collections.Generic;\n\nusing Version = Lucene.Net.Util.Version;\n\npublic class ComplexPhraseQueryParser : QueryParser\n{\n    private List&lt;ComplexPhraseQuery&gt; complexPhrases = null;\n\n    private Boolean isPass2ResolvingPhrases;\n\n    private ComplexPhraseQuery currentPhraseQuery = null;\n\n    public ComplexPhraseQueryParser(Version matchVersion, String f, Analyzer a) : base(matchVersion, f, a) { }\n\n\n\n    protected override Query GetFieldQuery(String field, String queryText, int slop)\n    {\n        ComplexPhraseQuery cpq = new ComplexPhraseQuery(field, queryText, slop);\n        complexPhrases.Add(cpq); // add to list of phrases to be parsed once\n        // we\n        // are through with this pass\n        return cpq;\n    }\n\n\n    public override Query Parse(String query)\n    {\n        if (isPass2ResolvingPhrases)\n        {\n            RewriteMethod oldMethod = MultiTermRewriteMethod;\n            try\n            {\n                // Temporarily force BooleanQuery rewrite so that Parser will\n                // generate visible\n                // collection of terms which we can convert into SpanQueries.\n                // ConstantScoreRewrite mode produces an\n                // opaque ConstantScoreQuery object which cannot be interrogated for\n                // terms in the same way a BooleanQuery can.\n                // QueryParser is not guaranteed threadsafe anyway so this temporary\n                // state change should not\n                // present an issue\n                MultiTermRewriteMethod = MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE;\n                return base.Parse(query);\n            }\n            finally\n            {\n                MultiTermRewriteMethod = oldMethod;\n            }\n        }\n\n        // First pass - parse the top-level query recording any PhraseQuerys\n        // which will need to be resolved\n        complexPhrases = new List&lt;ComplexPhraseQuery&gt;();\n        Query q = base.Parse(query);\n\n        // Perform second pass, using this QueryParser to parse any nested\n        // PhraseQueries with different\n        // set of syntax restrictions (i.e. all fields must be same)\n        isPass2ResolvingPhrases = true;\n        try\n        {\n            using (IEnumerator&lt;ComplexPhraseQuery&gt; enumerator = complexPhrases.GetEnumerator())\n            {\n                while (enumerator.MoveNext())\n                {\n                    currentPhraseQuery = enumerator.Current;\n                    currentPhraseQuery.ParsePhraseElements(this);\n                }\n            }\n        }\n        finally\n        {\n            isPass2ResolvingPhrases = false;\n        }\n        return q;\n    }\n\n    // There is No "getTermQuery throws ParseException" method to override so\n    // unfortunately need\n    // to throw a runtime exception here if a term for another field is embedded\n    // in phrase query\n    protected override Query NewTermQuery(Term term)\n    {\n        if (isPass2ResolvingPhrases)\n        {\n            try\n            {\n                CheckPhraseClauseIsForSameField(term.Field);\n            }\n            catch (ParseException pe)\n            {\n                throw new SystemException("Error parsing complex phrase", pe);\n            }\n        }\n        return base.NewTermQuery(term);\n    }\n\n    // Helper method used to report on any clauses that appear in query syntax\n    private void CheckPhraseClauseIsForSameField(String field)\n    {\n        if (!field.Equals(currentPhraseQuery.Field))\n        {\n            throw new ParseException("Cannot have clause for field \"" + field\n                + "\" nested in phrase " + " for field \"" + currentPhraseQuery.Field\n                + "\"");\n        }\n    }\n\n    protected override Query GetWildcardQuery(String field, String termStr)\n    {\n        if (isPass2ResolvingPhrases)\n        {\n            CheckPhraseClauseIsForSameField(field);\n        }\n        return base.GetWildcardQuery(field, termStr);\n    }\n\n    protected override Query GetRangeQuery(String field, String part1, String part2, Boolean inclusive)\n    {\n        if (isPass2ResolvingPhrases)\n        {\n            CheckPhraseClauseIsForSameField(field);\n        }\n        return base.GetRangeQuery(field, part1, part2, inclusive);\n    }\n\n    protected override Query NewRangeQuery(String field, String part1, String part2,\n        Boolean inclusive)\n    {\n        if (isPass2ResolvingPhrases)\n        {\n            // Must use old-style RangeQuery in order to produce a BooleanQuery\n            // that can be turned into SpanOr clause\n            TermRangeQuery rangeQuery = new TermRangeQuery(field, part1, part2, inclusive, inclusive, RangeCollator);\n            rangeQuery.RewriteMethod = MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE;\n            return rangeQuery;\n        }\n        return base.NewRangeQuery(field, part1, part2, inclusive);\n    }\n\n\n    protected Query GetFuzzyQuery(String field, String termStr, float minSimilarity)\n    {\n        if (isPass2ResolvingPhrases)\n        {\n            CheckPhraseClauseIsForSameField(field);\n        }\n        return base.GetFuzzyQuery(field, termStr, minSimilarity);\n    }\n\n    /*\n     * Used to handle the query content in between quotes and produced Span-based\n     * interpretations of the clauses.\n     */\n    class ComplexPhraseQuery : Query\n    {\n\n        public string Field { get; set; }\n\n        public string PhrasedQueryStringContents { get; set; }\n\n        public int SlopFactor { get; set; }\n\n        private Query Contents;\n\n        public ComplexPhraseQuery(string Field, string PhrasedQueryStringContents, int SlopFactor)\n            : base()\n        {\n            this.Field = Field;\n            this.PhrasedQueryStringContents = PhrasedQueryStringContents;\n            this.SlopFactor = SlopFactor;\n        }\n\n        // Called by ComplexPhraseQueryParser for each phrase after the main\n        // parse\n        // thread is through\n        public void ParsePhraseElements(QueryParser qp)\n        {\n            // TODO ensure that field-sensitivity is preserved ie the query\n            // string below is parsed as\n            // field+":("+phrasedQueryStringContents+")"\n            // but this will need code in rewrite to unwrap the first layer of\n            // boolean query\n            Contents = qp.Parse(PhrasedQueryStringContents);\n        }\n\n        public override Query Rewrite(IndexReader reader)\n        {\n            // ArrayList spanClauses = new ArrayList();\n            if (Contents is TermQuery)\n            {\n                return Contents;\n            }\n\n            // Build a sequence of Span clauses arranged in a SpanNear - child\n            // clauses can be complex\n            // Booleans e.g. nots and ors etc\n            int numNegatives = 0;\n            if (!(Contents is BooleanQuery))\n            {\n                throw new ArgumentException("Unknown query type \""\n                    + Contents.GetType()\n                    + "\" found in phrase query string \"" + PhrasedQueryStringContents\n                    + "\"");\n            }\n            BooleanQuery bq = (BooleanQuery)Contents;\n            BooleanClause[] bclauses = bq.GetClauses();\n            SpanQuery[] allSpanClauses = new SpanQuery[bclauses.Length];\n            // For all clauses e.g. one* two~\n            for (int i = 0; i &lt; bclauses.Length; i++)\n            {\n                // HashSet bclauseterms=new HashSet();\n                Query qc = bclauses[i].Query;\n                // Rewrite this clause e.g one* becomes (one OR onerous)\n                qc = qc.Rewrite(reader);\n                if (bclauses[i].Occur.Equals(Occur.MUST_NOT))\n                {\n                    numNegatives++;\n                }\n\n                if (qc is BooleanQuery)\n                {\n                    List&lt;SpanQuery&gt; sc = new List&lt;SpanQuery&gt;();\n                    AddComplexPhraseClause(sc, (BooleanQuery)qc);\n                    if (sc.Count &gt; 0)\n                    {\n                        allSpanClauses[i] = sc[0];\n                    }\n                    else\n                    {\n                        // Insert fake term e.g. phrase query was for "Fred Smithe*" and\n                        // there were no "Smithe*" terms - need to\n                        // prevent match on just "Fred".\n                        allSpanClauses[i] = new SpanTermQuery(new Term(Field,\n                            "Dummy clause because no terms found - must match nothing"));\n                    }\n                }\n                else\n                {\n                    if (qc is TermQuery)\n                    {\n                        TermQuery tq = (TermQuery)qc;\n                        allSpanClauses[i] = new SpanTermQuery(tq.Term);\n                    }\n                    else\n                    {\n                        throw new ArgumentException("Unknown query type \""\n                            + qc.GetType()\n                            + "\" found in phrase query string \""\n                            + PhrasedQueryStringContents + "\"");\n                    }\n\n                }\n            }\n            if (numNegatives == 0)\n            {\n                // The simple case - no negative elements in phrase\n                return new SpanNearQuery(allSpanClauses, SlopFactor, true);\n            }\n            // Complex case - we have mixed positives and negatives in the\n            // sequence.\n            // Need to return a SpanNotQuery\n            List&lt;SpanQuery&gt; positiveClauses = new List&lt;SpanQuery&gt;();\n            for (int j = 0; j &lt; allSpanClauses.Length; j++)\n            {\n                if (!bclauses[j].Occur.Equals(Occur.MUST_NOT))\n                {\n                    positiveClauses.Add(allSpanClauses[j]);\n                }\n            }\n\n            //SpanQuery[] includeClauses = positiveClauses.ToArray(new SpanQuery[positiveClauses.Count]);\n            SpanQuery[] includeClauses = positiveClauses.ToArray();\n\n            SpanQuery include = null;\n            if (includeClauses.Length == 1)\n            {\n                include = includeClauses[0]; // only one positive clause\n            }\n            else\n            {\n                // need to increase slop factor based on gaps introduced by\n                // negatives\n                include = new SpanNearQuery(includeClauses, SlopFactor + numNegatives,\n                    true);\n            }\n            // Use sequence of positive and negative values as the exclude.\n            SpanNearQuery exclude = new SpanNearQuery(allSpanClauses, SlopFactor,\n                true);\n            SpanNotQuery snot = new SpanNotQuery(include, exclude);\n            return snot;\n        }\n\n        private void AddComplexPhraseClause(List&lt;SpanQuery&gt; spanClauses, BooleanQuery qc)\n        {\n            List&lt;SpanQuery&gt; ors = new List&lt;SpanQuery&gt;();\n            List&lt;SpanQuery&gt; nots = new List&lt;SpanQuery&gt;();\n            BooleanClause[] bclauses = qc.GetClauses();\n\n            // For all clauses e.g. one* two~\n            for (int i = 0; i &lt; bclauses.Length; i++)\n            {\n                Query childQuery = bclauses[i].Query;\n\n                // select the list to which we will add these options\n                List&lt;SpanQuery&gt; chosenList = ors;\n                if (bclauses[i].Occur == Occur.MUST_NOT)\n                {\n                    chosenList = nots;\n                }\n\n                if (childQuery is TermQuery)\n                {\n                    TermQuery tq = (TermQuery)childQuery;\n                    SpanTermQuery stq = new SpanTermQuery(tq.Term);\n                    stq.Boost = tq.Boost;\n                    chosenList.Add(stq);\n                }\n                else if (childQuery is BooleanQuery)\n                {\n                    BooleanQuery cbq = (BooleanQuery)childQuery;\n                    AddComplexPhraseClause(chosenList, cbq);\n                }\n                else\n                {\n                    // TODO alternatively could call extract terms here?\n                    throw new ArgumentException("Unknown query type:"\n                        + childQuery.GetType());\n                }\n            }\n            if (ors.Count == 0)\n            {\n                return;\n            }\n            SpanOrQuery soq = new SpanOrQuery(ors.ToArray());\n            if (nots.Count == 0)\n            {\n                spanClauses.Add(soq);\n            }\n            else\n            {\n                SpanOrQuery snqs = new SpanOrQuery(nots.ToArray());\n                SpanNotQuery snq = new SpanNotQuery(soq, snqs);\n                spanClauses.Add(snq);\n            }\n        }\n\n\n        public override String ToString(String field)\n        {\n            return "\"" + PhrasedQueryStringContents + "\"";\n        }\n\n\n        public override int GetHashCode()\n        {\n            const int prime = 31;\n            int result = 1;\n            result = prime * result + ((Field == null) ? 0 : Field.GetHashCode());\n            result = prime\n                * result\n                + ((PhrasedQueryStringContents == null) ? 0\n                    : PhrasedQueryStringContents.GetHashCode());\n            result = prime * result + SlopFactor;\n            return result;\n        }\n\n        public override Boolean Equals(Object obj)\n        {\n            if (this == obj)\n                return true;\n            if (obj == null)\n                return false;\n            if (GetType() != obj.GetType())\n                return false;\n            ComplexPhraseQuery other = (ComplexPhraseQuery)obj;\n            if (Field == null)\n            {\n                if (other.Field != null)\n                    return false;\n            }\n            else if (!Field.Equals(other.Field))\n                return false;\n            if (PhrasedQueryStringContents == null)\n            {\n                if (other.PhrasedQueryStringContents != null)\n                    return false;\n            }\n            else if (!PhrasedQueryStringContents\n              .Equals(other.PhrasedQueryStringContents))\n                return false;\n            if (SlopFactor != other.SlopFactor)\n                return false;\n            return true;\n        }\n    }\n}\n
465	@Column(name = "my_date")\n@Temporal(TemporalType.TIMESTAMP)\n@Field\n@DateBridge(resolution = org.hibernate.search.annotations.Resolution.DAY, encoding = EncodingType.STRING)\nprivate Date myDate;\n
466	// Boxed Long, can be null to mean that you might not have a token.\n// You could also use Optional or something similar.\nLong tokenFromRequest;\n\n// Might be the same as the incoming token or a new one.\nLong tokenToReturn;\n\nIndexSearcher is;\nif (tokenFromRequest == null) { // no previous token, need a new one\n  is = manager.acquire(); // get searcher from SearcherManager\n  tokenToReturn = mgr.record(is); // get a fresh token for this searcher\n} else {\n  is = mgr.acquire(tokenFromRequest); // try to get the recorded searcher\n  if (is != null) { // token is valid\n    tokenToReturn = tokenFromRequest;\n  } else {\n    is = manager.acquire(); // get searcher from SearcherManager\n    tokenToReturn = mgr.record(is); // get a token for this searcher\n  }\n}\n\ntry {\n  // Do searching, doc retrieval, etc. with is\n} finally {\n  if (tokenToReturn == tokenFromRequest) {\n    // token was valid, we need to release the searcher\n    // to the LifetimeManager, the SearcherManager had\n    // nothing to do with this IndexSearcher\n    mgr.release(is);\n  } else {\n    // we got this searcher from SearcherManager,\n    // the LifetimeManager records its usage as well and\n    // we need to release it to show that _we_ are finished\n    // using it, otherwise it would be kept open forever\n    manager.release(is);\n  }\n\n}\n// Do not use is after this!\nis = null;\n\n//return tokenToReturn to user to be sent with the next request\n
467	&lt;fieldType name="search" class="solr.TextField" positionIncrementGap="150"&gt;\n    &lt;analyzer type="index"&gt;\n        &lt;tokenizer class="solr.KeywordTokenizerFactory"/&gt;\n        &lt;filter class="solr.LowerCaseFilterFactory"/&gt;\n        &lt;filter class="solr.NGramFilterFactory" minGramSize="1" maxGramSize="50"/&gt;\n    &lt;/analyzer&gt;\n    &lt;analyzer type="query"&gt;\n        &lt;tokenizer class="solr.KeywordTokenizerFactory"/&gt;\n        &lt;filter class="solr.LowerCaseFilterFactory"/&gt;\n    &lt;/analyzer&gt;\n&lt;/fieldType&gt;\n
468	var orderedResult = from i in ids\n                join a in query on i equals a.ID\n                select a;\n
469	&lt;field name="_path" type="string" indexed="true" stored="true" multiValued="true" /&gt;\n
470	dbms.auto_index.nodes.enabled=true                                                                                                                                                                                 \ndbms.auto_index.nodes.keys=name \n
471	sort=sub(Total_ix, count_ix) desc, date desc\n
472	http://localhost:8080/alfresco/service/slingshot/node/search\n   ?lang=lucene\n   &amp;store=workspace://SpacesStore\n   &amp;q=\n     TYPE:"cm:folder" \n     AND PATH:"/app:company_home//*" \n     AND @cm\:name:"SOMETHING"\n
473	public void updateField(String docId, int newFieldvalue) {\n    MyDataObject data = primaryDataStore.fetch(docId);\n    data.setFieldValue(newFieldValue);\n    primaryDataStore.save(data);\n    updateIndex(data);\n}\n\npublic void updateIndex(MyDataObject object) {\n    // convertToLucene is more or less the code in the\n    // first snippet of your question \n    Document d = convertToLucene(object);\n    // IndexWriter should be created once\n    // IndexWriter.updateDocument will internally delete and index \n    // the document\n    this.writer.updateDocument(new Term("id", object.getId()), d);\n    // potentially call writer.commit()\n}\n
474	var rawString = "//test:123 DIR/FILE.TXT @"; \n=&gt; rawString: //test:123 DIR/FILE.TXT @   \nvar encodedString = search.ISO9075Encode(rawString);\n=&gt; encodedString: _x002f__x002f_test_x003a_123_x0020_DIR_x002f_FILE.TXT_x0020__x0040_\nvar decodedString = search.ISO9075Decode(encodedString);\n=&gt; decodedString: //test:123 DIR/FILE.TXT @  \n
475	Query query = parser.parse("Apple");\nTopDocs topDocs = searcher.search(query, 20);\n
476	...\nWikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(test));\nint count = 0;\nint numItalics = 0;\nint numBoldItalics = 0;\nint numCategory = 0;\nint numCitation = 0;\nTermAttribute termAtt = tf.addAttribute(TermAttribute.class);\nTypeAttribute typeAtt = tf.addAttribute(TypeAttribute.class);\n\nwhile (tf.incrementToken()) {\n  String tokText = termAtt.term();\n  //System.out.println("Text: " + tokText + " Type: " + token.type());\n  String expectedType = (String) tcm.get(tokText);\n  assertTrue("expectedType is null and it shouldn't be for: " + tf.toString(), expectedType != null);\n  assertTrue(typeAtt.type() + " is not equal to " + expectedType + " for " + tf.toString(), typeAtt.type().equals(expectedType) == true);\n  count++;\n  if (typeAtt.type().equals(WikipediaTokenizer.ITALICS)  == true){\n    numItalics++;\n  } else if (typeAtt.type().equals(WikipediaTokenizer.BOLD_ITALICS)  == true){\n    numBoldItalics++;\n  } else if (typeAtt.type().equals(WikipediaTokenizer.CATEGORY)  == true){\n    numCategory++;\n  }\n  else if (typeAtt.type().equals(WikipediaTokenizer.CITATION)  == true){\n    numCitation++;\n  }\n}\n...\n
477	?q=john&amp;fl=score,*&amp;rows=2&amp;debugQuery=on\n
478	bq=price:[500 TO 1000]^10\n
479	 &lt;arr name="components"&gt;\n   &lt;str&gt;highlight&lt;/str&gt;\n &lt;/arr&gt;\n
480	$client = new Apache_Solr_Service($hostname, $port, $url);\n$response = $client-&gt;search($q, $offset, $limit, $parameters);\n\n$debug = reset($response['debug']['explain']); // get the first explanation\nif (preg_match('#([\d\.]+) = coord\((\d+)/(\d+)\)#m', $debug, $matches)) {\n    $coord = floatval($matches[1]);\n    $overlap = intval($matches[2]); // the number of matching keywords\n    $max_overlap = intval($matches[3]); // the total number of keywords\n}\n
481	IndexSearcher is = new IndexSearcher(indexWriter.GetReader());\nTopDocs topDocs = is.Search(query, 100);\n
482	&lt;similarity class="com.mycompany.MySimilarity" /&gt; \n
483	&lt;solr persistent="false" sharedLib="lib"&gt;\n  &lt;cores adminPath="/admin/cores" defaultCoreName="core0"&gt;\n    &lt;core name="core0" instanceDir="." /&gt;\n  &lt;/cores&gt;\n&lt;/solr&gt;\n
484	public class UserCompetenceBridge implements FieldBridge {\n    @Override\n    public void set(\n            String name, Object value, Document document, LuceneOptions luceneOptions) {\n        UserCompetence pc = (UserCompetence ) value;\n\n        // Add competence level + competence id combined field for specific competence querying\n        String lvl = pc.getLevel() == null ? "0" : pc.getLevel().toString();\n        String comp = pc.getCompetence().getId().toString();\n\n        String fieldValue = comp + SearchFields.FIELD_SEPERATOR + lvl;\n        Field compLvl = new Field(SearchFields.COMPETENCE_LEVEL, fieldValue, Field.Store.NO, Field.Index.NOT_ANALYZED);\n        compLvl.setBoost(luceneOptions.getBoost());\n        document.add(compLvl);\n\n        // Add competence names for free text search\n        Field compName = new Field(SearchFields.COMPETENCE_NAME, pc.getCompetence().getName(), Field.Store.NO, Field.Index.ANALYZED);\n        document.add(compName);\n\n    }\n}\n\n@Entity\n@Table(name = "user_competence")\n@ClassBridge(impl = UserCompetenceBridge.class)\npublic class UserCompetence {\n\n    @ManyToOne\n    @JoinColumn(name = "user_id", referencedColumnName = "id")\n    @ContainedIn\n    private User user;\n\n    @ManyToOne\n    @JoinColumn(name = "competence_id", referencedColumnName = "id")\n    private Competence competence;\n\n    @Basic\n    @Column(name = "competence_level")\n    private Integer level;\n}\n
485	http://localhost:8983/solr/collection1/select?q=*:*&amp;wt=json&amp;indent=true&amp;facet=true&amp;facet.pivot=publisherid,text\n
486	    FSDirectory directory = FSDirectory.open(new File("/tmp/moo"));\n    /*\n    IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(Version.LUCENE_44, new StandardAnalyzer(Version.LUCENE_44)));\n    Document document = new Document();\n    document.add(new TextField("foo", "abc", Store.YES));\n    document.add(new TextField("foo", "abc", Store.YES));\n    document.add(new TextField("foo", "aaa", Store.YES));\n    document.add(new TextField("bar", "abc", Store.YES));\n    writer.addDocument(document);\n    writer.commit();\n    writer.close(true);\n    */\n\n    IndexReader indexReader = DirectoryReader.open(directory);\n\n    Bits liveDocs = MultiFields.getLiveDocs(indexReader);\n    Fields fields = MultiFields.getFields(indexReader);\n    for (String field : fields) {\n\n        TermsEnum termEnum = MultiFields.getTerms(indexReader, field).iterator(null);\n        BytesRef bytesRef;\n        while ((bytesRef = termEnum.next()) != null) {\n            if (termEnum.seekExact(bytesRef, true)) {\n\n                DocsEnum docsEnum = termEnum.docs(liveDocs, null);\n\n                if (docsEnum != null) {\n                    int doc;\n                    while ((doc = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                        System.out.println(bytesRef.utf8ToString() + " in doc " + doc + ": " + docsEnum.freq());\n                    }\n                }\n            }\n        }\n    }\n\n    for (String field : fields) {\n        TermsEnum termEnum = MultiFields.getTerms(indexReader, field).iterator(null);\n        BytesRef bytesRef;\n        while ((bytesRef = termEnum.next()) != null) {\n            int freq = indexReader.docFreq(new Term(field, bytesRef));\n\n            System.out.println(bytesRef.utf8ToString() + " in " + freq + " documents");\n\n        }\n    }\n
487	String[] arrKeywords = keywords.split(" ");\nthis.search(Arrays.asList(arrKeywords));\n
488	BooleanQuery bq = new BooleanQuery();\n//etc.\nConstantScoreQuery query = new ConstantScoreQuery(bq);\nsearcher.search(query, collector);\n
489	public static String stem(String string) throws IOException {\n    TokenStream tokenizer = new StandardTokenizer(Version.LUCENE_47, new StringReader(string));\n    tokenizer = new StandardFilter(Version.LUCENE_47, tokenizer);\n    tokenizer = new LowerCaseFilter(Version.LUCENE_47, tokenizer);\n    tokenizer = new PorterStemFilter(tokenizer);\n\n    CharTermAttribute token = tokenizer.getAttribute(CharTermAttribute.class);\n\n    tokenizer.reset();\n\n    StringBuilder stringBuilder = new StringBuilder();\n\n    while(tokenizer.incrementToken()) {\n        if(stringBuilder.length() &gt; 0 ) {\n            stringBuilder.append(" ");\n        }\n\n        stringBuilder.append(token.toString());\n    }\n\n    tokenizer.end();\n    tokenizer.close();\n\n    return stringBuilder.toString();\n}\n
490	&lt;fieldtype name="shingle"&gt;\n  &lt;analyzer&gt;\n  &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;\n  &lt;filter class="solr.ShingleFilterFactory" minShingleSize="2" maxShingleSize="5"/&gt;\n  &lt;/analyzer&gt;\n&lt;/fieldtype&gt;\n
491	StandardAnalyzer analyzer = new StandardAnalyzer();\n
492	4.296241 = fieldWeight in 177, product of:\n  2.236068 = tf(freq=5.0), with freq of:\n    5.0 = termFreq=5.0\n  4.391628 = idf(docFreq=6, maxDocs=208)\n  0.4375 = fieldNorm(doc=177)\n
493	public class StartsWithCapitalTokenFilter extends FilteringTokenFilter {\n\n    private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n\n    public StartsWithCapitalTokenFilter(TokenStream tokenStream) {\n        super(tokenStream);\n    }\n\n    @Override\n    public boolean accept() {\n        // When accept() is called, my understanding is that termAtt.buffer() will\n        // contain the particular string (in char[] form) of whichever token\n        // is under consideration. This call gets the Unicode code point of the\n        // first character and checks if it's uppercase.\n        return Character.isUpperCase(Character.codePointAt(termAtt.buffer(),0));\n\n        // Or if you don't want to care about Unicode about U+FFFF, use the below.\n        //return Character.isUpperCase(termAtt.buffer()[0]);\n    }\n}\n
494	CREATE KEYSPACE demo\nWITH REPLICATION = {'class' : 'SimpleStrategy', 'replication_factor': 1};\nUSE demo;\nCREATE TABLE tweets (\n    id INT PRIMARY KEY,\n    user TEXT,\n    body TEXT,\n    time TIMESTAMP,\n    latitude FLOAT,\n    longitude FLOAT,\n    lucene TEXT\n);\n\n\n\nCREATE CUSTOM INDEX tweets_index ON tweets (lucene)\nUSING 'com.stratio.cassandra.lucene.Index'\nWITH OPTIONS = {\n    'refresh_seconds' : '1',\n    'schema' : '{\n        fields : {\n            id    : {type : "integer"},\n            user  : {type : "string"},\n            body  : {type : "text", analyzer : "english"},\n            time  : {type : "date", pattern : "yyyy/MM/dd", sorted : true},\n            place : {type : "geo_point", latitude:"latitude", longitude:"longitude"}\n        }\n    }'\n};\n
495	CREATE KEYSPACE test\nWITH REPLICATION = {'class' : 'SimpleStrategy', 'replication_factor': 1};\nUSE test;\nCREATE TABLE test (\n    id INT PRIMARY KEY,\n    body TEXT\n);\n\nCREATE CUSTOM INDEX test_index ON test ()\nUSING 'com.stratio.cassandra.lucene.Index'\nWITH OPTIONS = {\n    'refresh_seconds' : '1',\n    'schema' : '{\n        fields : {\n            body1 : {type :"string", column:"body", case_sensitive:false},\n            body2 : {type :"string", column:"body", case_sensitive:true}\n        }\n    }'\n};\n\nINSERT INTO test(id,body) VALUES ( 1, 'foo');\nINSERT INTO test(id,body) VALUES ( 2, 'Foo');\nINSERT INTO test(id,body) VALUES ( 3, 'bar');\nINSERT INTO test(id,body) VALUES ( 4, 'Bar');\n\n\nSELECT * FROM test WHERE expr(test_index, \n   '{filter:{type:"prefix", field:"body2", value:"f"}}'); -- Returns foo\nSELECT * FROM test WHERE expr(test_index, \n   '{filter:{type:"prefix", field:"body2", value:"F"}}'); -- Returns Foo\n\nSELECT * FROM test WHERE expr(test_index, \n   '{filter:{type:"prefix", field:"body1", value:"f"}}'); -- Returns foo and Foo\nSELECT * FROM test WHERE expr(test_index, \n   '{filter:{type:"prefix", field:"body1", value:"F"}}'); -- Returns no results\n
496	Query q = new QueryParser("param", paramAnalyzer).parse(param);\nq = new BoostQuery(q, 10f);\n
497	public void foo() throws IOException {\n  Closer closer = Closer.create();\n  try {\n    InputStream in = closer.register(openInputStream());\n    OutputStream out = closer.register(openOutputStream());\n    // do stuff with in and out\n  } catch (Throwable e) { // must catch Throwable\n    throw closer.rethrow(e);\n  } finally {\n    closer.close();\n  }\n}\n
498	RAMDirectory idx = new RAMDirectory(); \nIndexWriterConfig iwc = new IndexWriterConfig(analyzer);\niwc.setOpenMode(OpenMode.CREATE_OR_APPEND);\nIndexWriter writer = new IndexWriter(idx, iwc);\nIterator&lt;Map.Entry&lt;String, String&gt;&gt; it = inputCategoryMap.entrySet().iterator();\n    while (it.hasNext()) {\n        Document doc = new Document();\n        Map.Entry&lt;String, String&gt; pair = it.next();\n        FieldType contentType = new FieldType();\n        contentType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n        contentType.setStored(true);\n        contentType.setTokenized(true);\n        contentType.setStoreTermVectors(true);\n        doc.add(new Field(TITLE, pair.getKey(), TextField.TYPE_STORED));\n        doc.add(new Field(CONTENT, pair.getValue(),contentType ));\n        dcmnts.add(doc);\n    }\n    writer.addDocuments(dcmnts);\n    writer.commit();\n    System.out.println("No of documents added in the index "+writer.maxDoc());\n    writer.close();\n   System.out.println("Size consumed by Ram  "+idx.ramBytesUsed()+"\nTime Consumed By Lucene  "+stopwatch.getTime(TimeUnit.SECONDS));\n
499	POST /myindex/_forcemerge?only_expunge_deletes=true\n
500	QueryParser queryParser = new QueryParser("content", analyzer);\nQuery query = queryParser.parse(searchTerm);\n
501	var input = "This is a test";\n\nvar fieldName = "yourField";\nvar minimumSimilarity = 0.5f;\nvar prefixLength = 3;\nvar query = new BooleanQuery();\n\nvar segments = input.Split(new[] {" "}, StringSplitOptions.RemoveEmptyEntries);\nforeach (var segment in segments)\n{\n    var term = new Term(fieldName, segment);\n    var fuzzyQuery = new FuzzyQuery(term, minimumSimilarity, prefixLength);\n    query.Add(fuzzyQuery, BooleanClause.Occur.SHOULD);\n}\n
502	FullTextEntityManager fullTextEntityManager = Search.getFullTextEntityManager(em);\nQueryBuilder qb = fullTextEntityManager.getSearchFactory().buildQueryBuilder().forEntity(Customer.class).get();\nTermMatchingContext onFields = qb.keyword().onFields("customer.shortDescription",  "customer.longDescription");\n\nBooleanJunction&lt;BooleanJunction&gt; bool = qb.bool();\norg.apache.lucene.search.Query query = null;\nString[] searchTerms = searchQuery.split("\\s+");\nfor (int j = 0; j &lt; searchTerms.length; j++) {\n   String currentTerm = searchTerms[j];\n   bool.must(onFields.matching(currentTerm).createQuery());\n}\n\nquery = bool.createQuery();\n\nFullTextQuery persistenceQuery = fullTextEntityManager.createFullTextQuery(query, Customer.class);\nresultList = persistenceQuery.getResultList();\n
503	Item myItem = Sitecore.Context.Database.GetItem("/sitecore/content/data/MyItem");\nItem lastItem = myItem.Children.Last();\n
504	    Directory directory = FSDirectory.open(folder);\n    if (directory.fileExists(IndexWriter.WRITE_LOCK_NAME)) {\n        directory.clearLock(IndexWriter.WRITE_LOCK_NAME);\n        log.warn("Existing write.lock at [" + folder.getAbsolutePath() + "] has been found and removed. This is a likely result of non-gracefully terminated server. Check for index discrepancies!");\n    }\n    directory.close();\n
505	import org.apache.lucene.analysis.TokenStream;\nimport org.apache.lucene.analysis.charfilter.MappingCharFilter;\nimport org.apache.lucene.analysis.charfilter.NormalizeCharMap;\nimport org.apache.lucene.analysis.core.LowerCaseFilter;\nimport org.apache.lucene.analysis.core.StopAnalyzer;\nimport org.apache.lucene.analysis.core.StopFilter;\nimport org.apache.lucene.analysis.standard.StandardFilter;\nimport org.apache.lucene.analysis.standard.StandardTokenizer;\nimport org.apache.lucene.analysis.util.StopwordAnalyzerBase;\nimport org.apache.lucene.util.Version;\n\nimport java.io.IOException;\nimport java.io.Reader;\n\npublic final class MyAnalyzer extends StopwordAnalyzerBase {\n  private int maxTokenLength = 255;\n  public MyAnalyzer() {\n    super(Version.LUCENE_41, StopAnalyzer.ENGLISH_STOP_WORDS_SET);\n  }\n\n  @Override\n  protected TokenStreamComponents createComponents\n      (final String fieldName, final Reader reader) {\n    final StandardTokenizer src = new StandardTokenizer(matchVersion, reader);\n    src.setMaxTokenLength(maxTokenLength);\n    TokenStream tok = new StandardFilter(matchVersion, src);\n    tok = new LowerCaseFilter(matchVersion, tok);\n    tok = new StopFilter(matchVersion, tok, stopwords);\n    return new TokenStreamComponents(src, tok) {\n      @Override\n      protected void setReader(final Reader reader) throws IOException {\n        src.setMaxTokenLength(MyAnalyzer.this.maxTokenLength);\n        super.setReader(reader);\n      }\n    };\n  }\n\n  @Override\n  protected Reader initReader(String fieldName, Reader reader) {\n    NormalizeCharMap.Builder builder = new NormalizeCharMap.Builder();\n    builder.add(".", " ");\n    builder.add("_", " ");\n    NormalizeCharMap normMap = builder.build();\n    return new MappingCharFilter(normMap, reader);\n  }\n}\n
506	@Field(index=Index.YES, analyze=Analyze.YES, store=Store.NO, analyzer = @Analyzer(impl = org.apache.lucene.analysis.pt.PortugueseAnalyzer.class))\n
507	&lt;str name="suggestAnalyzerFieldType"&gt;text&lt;/str&gt;\n&lt;str name="queryAnalyzerFieldType"&gt;text_suggest&lt;/str&gt;\n
508	var query = session.Query&lt;Person&gt;().Where("Age &lt;= 11 And Age &gt;= 5"); \n
509	for (int i=0; i&lt;arr.length; i++ {\n    Field field = new StringField("field_name", arr[i], Field.Store.YES);\n    doc.add(field);\n}\n
510	Cache nonIndexedCache = cache.getAdvancedCache().withFlags(Flag.SKIP_INDEX);\nnonIndexedCache.put( ... );\n
511	Document doc = new Document();\nField field1 = new TextField("field1", field1Value, Field.Store.YES);\ndoc.add(field1);\nField field2 = new StringField("field2", field2Value,Field.Store.YES);\ndoc.add(field2);\nwhile ((line = br.readLine()) != null) {\n    field1.setStringValue("field1Value");\n    field2.setStringValue("field2Value");\n\n    writer.addDocument(doc);\n}\n
512	QueryBuilder hospitalQb = fullTextEntityManager.getSearchFactory().buildQueryBuilder().forEntity(Hospital.class)\n    .overridesForField( "name", "my_analyzer_without_ngrams" )\n    .get();\n// Then it's business as usual\n
513	@AnalyzerDef(name = "edgeNgram",\n    tokenizer = @TokenizerDef(factory = WhitespaceTokenizerFactory.class),\n    filters = {\n            @TokenFilterDef(factory = ASCIIFoldingFilterFactory.class), // Replace accented characeters by their simpler counterpart (è =&gt; e, etc.)\n            @TokenFilterDef(factory = LowerCaseFilterFactory.class), // Lowercase all characters\n            @TokenFilterDef(\n                    factory = EdgeNGramFilterFactory.class, // Generate prefix tokens\n                    params = {\n                            @Parameter(name = "minGramSize", value = "1"),\n                            @Parameter(name = "maxGramSize", value = "10")\n                    }\n            )\n    })\n
514	./select/?q=location:N1 %2Btype:blue&amp;rows=100&amp;fl=*,score&amp;debugQuery=true\n
515	&lt;include hint="list:IncludeTemplate"&gt;\n    &lt;template&gt;{2A609D52-7B9F-49F3-83BE-047FD16397A7} &lt;/template&gt;\n    &lt;template&gt;{F98712D8-27DB-4324-82E6-65242F0977F9} &lt;/template&gt;\n    &lt;template&gt;{849CA304-3F51-4FCB-B9B3-2AC7E950B476} &lt;/template&gt;\n    &lt;template&gt;{A87A00B1-E6DB-45AB-8B54-636FEC3B5523} &lt;/template&gt;\n    &lt;template&gt;{52BDB3C4-0585-437C-89AD-6AAC81950633} &lt;/template&gt;\n&lt;/include&gt;\n
516	MultiReader multiReader = new MultiReader(reader1, reader2);\nIndexSearcher searcher = new IndexSearcher(multiReader);\n
517	session.Query&lt;Person&gt;.Where(x =&gt; x.IsActive).ToString()\n
518	CloudSolrServer server = new CloudSolrServer(zknodesurlstring);\n\n    server.setDefaultCollection("mycollection");\n    server.connect();\n\n    ZkStateReader reader = server.getZkStateReader();\n    Collection&lt;Slice&gt; slices = reader.getClusterState().getSlices("mycollection");\n    Iterator&lt;Slice&gt; iter = slices.iterator();\n\n    while (iter.hasNext()) {\n        Slice slice = iter.next();\n        for(Replica replica:slice.getReplicas()) {\n\n            System.out.println("replica state for " + replica.getStr("core") + " : "+ replica.getStr( "state" ));\n\n            System.out.println(slice.getName());\n            System.out.println(slice.getState());\n        }\n    }\n
519	static Stream&lt;String&gt; allCombinations(String line) {\n    String[] words = line.split(" ");\n    return Arrays.stream(words)\n        .flatMap(word1 -&gt;\n            Arrays.stream(words)\n                  .filter(words2 -&gt; word1.compareTo(words2)&lt;0)\n                  .map(word2 -&gt; word1+'_'+word2))\n        .distinct();\n}\n
520	start n=node:myindes('time: [1 to 1000]') return n order by n.time asc\n
521	String contents = "";\nPDDocument doc = null;\ntry {\n    doc = PDDocument.load(file);\n    PDFTextStripper stripper = new PDFTextStripper();\n\n    stripper.setLineSeparator("\n");\n    stripper.setStartPage(1);\n    stripper.setEndPage(5);// this mean that it will index the first 5 pages only\n    contents = stripper.getText(doc);\n\n} catch(Exception e){\n    e.printStackTrace();\n}\n
522	try { \n    // if new IndexSearcher throws, searcher will not be initialized, and doesn't need a close. The catch below takes care of reporting the error.\n    final IndexSearcher searcher = new IndexSearcher(index.getDirectory(),true);\n    try {\n        searcher.search(query, new HitCollector() {\n                public void collect(int doc, float score) {\n                    try {\n                        resultWorker.add(new ProcessDocument(searcher.doc(doc)));\n                    } catch (CorruptIndexException e) {\n                        log.error("Corrupt index found during search", e);\n                    } catch (IOException e) {\n                        log.error("Error during search", e);\n                    }\n                }\n            });\n    } finally {\n        searcher.close();\n    }\n} catch (CorruptIndexException e) {\n    log.error("Corrupt index found during search", e);\n} catch (IOException e) {\n    log.error("Error during search", e);\n} finally {\n}\n
523	Directory index = new RAMDirectory();\n
524	IndexReader reader = IndexReader.open(FSDirectory.open(indexDirectory));\nint num = reader.numDocs();\n
525	QueryResponse response = solr.query(query);\n
526	Query query = queryParser.parse("My Query!");\nScoreDoc[] docs = searcher.search(query, 100).scoreDocs;\nFor (ScoreDoc doc : docs) {\n    indexReader.deleteDocument(doc.doc);\n}\n
527	curl -XGET http://my_server:9200/idx_occurrence/Occurrence/_search?pretty=true -d '{\n  "query": {\n    "query_string": {\n      "fields": ["dataset"],\n      "query": "2",\n      "default_operator": "AND"\n    }\n  },\n  "facets": {\n    "test": {\n      "terms": {\n        "script_field": "_fields['\''speciesName'\''].value",\n        "size": 50000\n      }\n    }\n  }\n}\n'\n
528	private static string DeNormalizeName(string name)\n{\n    string answer = string.Empty;\n\n    var wordsOnly = Regex.Replace(name, "[^\\w0-9 ]+", string.Empty);\n    var filterText = (name != wordsOnly) ? name + " " + wordsOnly : name;\n\n    foreach (var subName in filterText.Split(' '))\n    {\n        if (subName.Length &gt;= 1)\n        {\n            for (var j = 1; j &lt;= subName.Length; j++)\n            {\n                answer += subName.Substring(0, j) + " ";\n            }\n        }\n    }\n    return answer.TrimEnd();\n}\n
529	Field boostField = oldDoc.getField("saved_boost");\nfloat newBoost = boostField.numericValue().floatValue() * 1.5F;\nupdatedDocument.setBoost(newBoost);\nupdatedDocument.removeField("saved_boost");\nNumericField boostField = new NumericField("saved_boost",Field.Store.YES,false);\nboostField.setFloatValue(newBoost);\nupdatedDocument.add(boostField);\n\n//No changes from here on...\n\nIndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(Version.LUCENE_30), false, MaxFieldLength.LIMITED);\n\nTerm uniqueTerm = new term("content_id", content_id_from_old_document);\n\nwriter.UpdateDocument(uniqueTerm, updatedDocument);\n
530	public static void main(String[] args) throws IOException, ParseException {\n    StandardAnalyzer standardAnalyzer = new StandardAnalyzer(Version.LUCENE_46);\n    RAMDirectory ramDirectory = new RAMDirectory();\n\n    IndexWriter indexWriter = new IndexWriter(ramDirectory, new IndexWriterConfig(Version.LUCENE_46, standardAnalyzer));\n\n    Document d0 = new Document();\n    d0.add(new TextField("employeeId", "foo", Field.Store.YES));\n    d0.add(new IntField("documentoId", 1, Field.Store.YES));\n    indexWriter.addDocument(d0);\n\n    Document d1 = new Document();\n    d1.add(new TextField("employeeId", "bar", Field.Store.YES));\n    d1.add(new IntField("documentoId", 20, Field.Store.YES));\n    indexWriter.addDocument(d1);\n\n    Document d2 = new Document();\n    d2.add(new TextField("employeeId", "baz", Field.Store.YES));\n    d2.add(new IntField("documentoId", 3, Field.Store.YES));\n    indexWriter.addDocument(d2);\n\n    indexWriter.commit();\n\n    GroupingSearch groupingSearch = new GroupingSearch("documentoId");\n    Sort groupSort = new Sort(new SortField("documentoId", SortField.Type.INT, true));  // in descending order\n    groupingSearch.setGroupSort(groupSort);\n    groupingSearch.setSortWithinGroup(groupSort);\n\n    IndexReader reader = DirectoryReader.open(ramDirectory);\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    TopGroups&lt;?&gt; groups = groupingSearch.search(searcher, new MatchAllDocsQuery(), 0, 10);\n\n    Document highestScoredDocument = reader.document(groups.groups[0].scoreDocs[0].doc);\n    System.out.println(\n            "Descending order, first document is " +\n                    "employeeId:" + highestScoredDocument.get("employeeId") + " " +\n                    "documentoId:" + highestScoredDocument.get("documentoId")\n    );\n}\n
531	filters = { \n        @TokenFilterDef(factory = StandardFilterFactory.class), \n        @TokenFilterDef(factory = LowerCaseFilterFactory.class),\n        @TokenFilterDef(factory = PhoneticFilterFactory.class, params = {\n            @Parameter(name = "encoder", value = "DoubleMetaphone"), \n            @Parameter(name = "inject", value = "true") \n        }) \n}\n
532	String normalized = Normalizer.normalize(string, Form.NFD)\n        .replaceAll("\\p{InCombiningDiacriticalMarks}+", "");\n
533	public String processReader(Reader reader) throws IOException {\n    char[] arr = new char[8 * 1024]; // 8K at a time\n    StringBuffer buf = new StringBuffer();\n    int numChars;\n    while ((numChars = reader.read(arr, 0, arr.length)) &gt; 0) {\n        buf.append(arr, 0, numChars);\n    }\n    return lemmatizer.getLemma(buf.toString());\n}\n
534	http://localhost:9200/myindex/_search?q=invoices.poNumber:123-4Q*&amp;analyzer=keyword&amp;lowercase_expanded_terms=false\n
535	BooleanQuery query = new BooleanQuery();\nquery.add(new BooleanClause(NumericRangeQuery.newIntRange("ID", myIdValue, myIdValue, true, true), BooleanClause.Occur.MUST));\nquery.add(new BooleanClause(new TermQuery(new Term("First_Name", myNameValue)), BooleanClause.Occur.MUST));\n
536	package solrExtension;\n\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport org.apache.lucene.search.Query;\nimport org.apache.lucene.search.postingshighlight.DefaultPassageFormatter;\nimport org.apache.lucene.search.postingshighlight.Passage;\nimport org.apache.lucene.search.postingshighlight.PassageFormatter;\nimport org.apache.lucene.search.postingshighlight.PostingsHighlighter;\nimport org.apache.solr.common.params.HighlightParams;\nimport org.apache.solr.common.util.NamedList;\nimport org.apache.solr.highlight.PostingsSolrHighlighter;\nimport org.apache.solr.request.SolrQueryRequest;\nimport org.apache.solr.search.DocList;\n\n\npublic class CustomPostingsSolrHighlighter extends PostingsSolrHighlighter {\n\n\n    protected PostingsHighlighter getHighlighter(SolrQueryRequest req) {\n        return new customSolrExtendedPostingsHighlighter(req);\n    }\n\n    public class customSolrExtendedPostingsHighlighter extends PostingsSolrHighlighter.SolrExtendedPostingsHighlighter {\n        public customSolrExtendedPostingsHighlighter(SolrQueryRequest req) {\n            super(req);\n        }\n\n        @Override\n        protected PassageFormatter getFormatter(String fieldName) {\n              String preTag = params.getFieldParam(fieldName, HighlightParams.TAG_PRE, "&lt;em&gt;");\n              String postTag = params.getFieldParam(fieldName, HighlightParams.TAG_POST, "&lt;/em&gt;");\n              String ellipsis = params.getFieldParam(fieldName, HighlightParams.TAG_ELLIPSIS, "... ");\n              String encoder = params.getFieldParam(fieldName, HighlightParams.ENCODER, "simple");\n              return new CustomPassageFormatter(preTag, postTag, ellipsis, "html".equals(encoder));\n        }\n    }\n\n\n    public class CustomPassageFormatter extends DefaultPassageFormatter {\n        public CustomPassageFormatter() {\n            super();\n        }\n\n        public CustomPassageFormatter(String preTag, String postTag,\n                String ellipsis, boolean escape) {\n            super(preTag, postTag, ellipsis, escape);\n        }\n\n        @Override\n        public String format(Passage passages[], String content) {\n            StringBuilder sb = new StringBuilder();\n            int pos = 0;\n            double psgTtlScore = 0, maxScore=0, score;\n            int psgCounts = 0;\n            List&lt;CustomPsg&gt; psgGroups = new ArrayList&lt;CustomPsg&gt;();\n            for (Passage passage : passages) {\n                // don't add ellipsis if it's the first one, or if it's\n                // connected.\n                if (passage.getStartOffset() &gt; pos &amp;&amp; pos &gt; 0) {\n                    score = psgTtlScore / psgCounts;\n                    if (score &gt; maxScore)\n                        maxScore = score;\n                    sb.append("[[").append(score).append("]]");\n                    psgGroups.add(new CustomPsg(sb.toString(), score));\n                    //sb.append(ellipsis);\n                    psgTtlScore = 0;\n                    psgCounts = 0;\n                    sb = new StringBuilder();\n                }\n                psgTtlScore += passage.getScore();\n                psgCounts++;\n                pos = passage.getStartOffset();\n                for (int i = 0; i &lt; passage.getNumMatches(); i++) {\n                    int start = passage.getMatchStarts()[i];\n                    int end = passage.getMatchEnds()[i];\n                    // it's possible to have overlapping terms\n                    if (start &gt; pos) {\n                        append(sb, content, pos, start);\n                    }\n                    if (end &gt; pos) {\n                        sb.append(preTag);\n                        append(sb, content, Math.max(pos, start), end);\n                        sb.append(postTag);\n                        pos = end;\n                    }\n                }\n                sb.append("[").append(passage.getScore()).append("]");\n                // it's possible a "term" from the analyzer could span a\n                // sentence boundary.\n                append(sb, content, pos, Math.max(pos, passage.getEndOffset()));\n                pos = passage.getEndOffset();\n            }\n            sb.append("[[").append(psgTtlScore / psgCounts).append("]]");\n            psgGroups.add(new CustomPsg(sb.toString(), psgTtlScore / psgCounts));\n\n\n            sb = new StringBuilder();\n            for (CustomPsg psg : psgGroups) {\n                sb.append(psg.psg).append("{{").append(psg.score/maxScore).append("}}").append(ellipsis);\n            }\n            return sb.toString();\n        }\n\n        private class CustomPsg {\n            public String psg;\n            public double score;\n            public CustomPsg(String psg, double score) {\n                this.psg = psg;\n                this.score = score;\n            }\n        }\n    }\n\n}\n
537	return AddClauseGroup(searchTerms, word =&gt; new TermQuery(new Term(field, word  + boostStr)));\n
538	&lt;filter class="solr.NGramFilterFactory" minGramSize="2" maxGramSize="25" /&gt;\n
539	MultiTermQuery query = new WildcardQuery(new Term("field1", "sum of sa*"));\n            query.setRewriteMethod(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE);\n
540	@Override\n    public Boolean addWords(Set&lt;String&gt; words) throws IOException{\n\n        synchronized(modifyCurrentIndexLock){\n\n        IndexWriterConfig wConfig = new IndexWriterConfig(new SimpleAnalyzer());\n        wConfig.setOpenMode(OpenMode.CREATE_OR_APPEND);\n\n        try(Directory spellIndex = FSDirectory.open(new File(indexLocation).toPath());\n            SpellChecker spellchecker = new SpellChecker(spellIndex); \n            IndexWriter writer = new IndexWriter(spellIndex, wConfig);)\n        {\n            for(String word:words){\n                if(!spellchecker.exist(word)){\n\n                    logger.debug("Word -&gt; "+word +" doesn't exist in dictionary to trying to add to index");\n                    Document doc = createDocument(word, getMin(word.length()), getMax(word.length()));\n                    writer.addDocument(doc);\n                    writer.commit();\n                }\n            }\n            logger.debug("All valid words added to dictionary");\n            return true;\n        }\n\n        }\n\n    }\n
541	ERROR: [doc=http://www.cmo.com/features/articles/2017/8/21/5-emerging-technologies-rewrite-the-media-and-entertainment-script-.html] unknown field 'sp_type'\nat org.apache.solr.client.solrj.impl.HttpSolrClient.executeMethod(HttpSolrClient.java:576)\nat org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:240)\n
542	Document doc = new Document();\n        doc.add(new TextField("text", "BlueCross BlueShield", Field.Store.YES));\n        writer.addDocument(doc);\n
543	http://localhost:8983/solr/select?q=neuron&amp;stats=true&amp;stats.field=q_visits&amp;rows=1&amp;indent=true&amp;stats.facet=q_date\n
544	import java.util.*;\n\nclass ConvertWordToNumber {\n\n    public static String WithSeparator(long number) {\n        if (number &lt; 0) {\n            return "-" + WithSeparator(-number);\n        }\n        if (number / 1000L &gt; 0) {\n            return WithSeparator(number / 1000L) + ","\n                    + String.format("%1$03d", number % 1000L);\n        } else {\n            return String.format("%1$d", number);\n        }\n    }\n\n    private static String[] numerals = { "zero", "one", "two",\n            "three", "four", "five", "six", "seven", "eight", "nine", "ten",\n            "eleven", "twelve", "thirteen", "fourteen", "fifteen", "sixteen",\n            "seventeen", "eighteen", "ninteen", "twenty", "thirty", "forty",\n            "fifty", "sixty", "seventy", "eighty", "ninety", "hundred" };\n\n    private static long[] values = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n            13, 14, 15, 16, 17, 18, 19, 20, 30, 40, 50, 60, 70, 80, 90, 100 };\n\n    private static ArrayList&lt;String&gt; list = new ArrayList&lt;String&gt;(\n            Arrays.asList(numerals));\n\n    public static long parseNumerals(String text) throws Exception {\n        long value = 0;\n        String[] words = text.replaceAll(" and ", " ").split("\\s");\n        for (String word : words) {\n            if (!list.contains(word)) {\n                throw new Exception("Unknown token : " + word);\n            }\n\n            long subval = getValueOf(word);\n            if (subval == 100) {\n                if (value == 0)\n                    value = 100;\n                else\n                    value *= 100;\n            } else\n                value += subval;\n        }\n\n        return value;\n    }\n\n    private static long getValueOf(String word) {\n        return values[list.indexOf(word)];\n    }\n\n    private static String[] words = { "trillion", "billion", "million", "thousand" };\n    private static long[] digits = { 1000000000000L, 1000000000L, 1000000L, 1000L };\n\n    public static long parse(String text) throws Exception {\n        text = text.toLowerCase().replaceAll("[\\-,]", " ").replaceAll(" and "," ");\n        long totalValue = 0;\n        boolean processed = false;\n        for (int n = 0; n &lt; words.length; n++) {\n            int index = text.indexOf(words[n]);\n            if (index &gt;= 0) {\n                String text1 = text.substring(0, index).trim();\n                String text2 = text.substring(index + words[n].length()).trim();\n\n                if (text1.equals(""))\n                    text1 = "one";\n\n                if (text2.equals(""))\n                    text2 = "zero";\n\n                totalValue = parseNumerals(text1) * digits[n] + parse(text2);\n                processed = true;\n                break;\n            }\n        }\n\n        if (processed)\n            return totalValue;\n        else\n            return parseNumerals(text);\n    }\n\n\n    public static void main(String[] args) throws Exception {\n        Scanner in = new Scanner(System.in);\n        System.out.print("Number in words : ");\n        String numberWordsText = in.nextLine();\n        System.out.println("Value : " + \n                ConvertWordToNumber.WithSeparator(\n                ConvertWordToNumber.parse(numberWordsText)));\n    }\n}\n
545	Term key = new Term(YammerMessageFields.THREAD_ID, \n   newdoc.getFieldable(YammerMessageFields.THREAD_ID).toString());\n
546	//span[. = 'A'][following-sibling::node()[1] = ' B']\n
547	$results = $li-&gt;find('username:"foobar"');\n
548	curl "http://localhost:8983/solr/update/extract?literal.path=\path\to\tutorial&amp;commit=true" -F "myfile=@tutorial.html"\n
549	Map&lt;K,Integer&gt; runningCount = new Map&lt;K,Integer&gt;();\nint totalCount = 0;\n\npublic void addValue(K k) {\n    runningCount.insert(k, runningCount.get(k) + 1);\n    totalCount += 1;\n}\n\npublic Map&lt;K,Double&gt; getDistribution() {\n    Map&lt;K,Double&gt; dist = new Map&lt;K,Double&gt;();\n    for (K k : runningCount.keys()) {\n        dist.insert(k, runningCount.get(k) / totalCount);\n    }\n    return dist;\n}\n
550	private ArrayList&lt;Long&gt; myIndexIDs = new ArrayList&lt;Long&gt;();\nprivate int minCacheSize = 100;\nprivate int maxCacheSize = 5000;\n\npublic Node getRandomNode() {\n    boolean found  = false;\n    Node n = null;\n\n    int index = getMyNodeIndex();\n    long id = myIndexIDs.get(index);\n\n    System.out.println(String.format("found id %d at index: %d", id, index));\n    ExecutionResult result = search.execute("START n=node(" + id + ") RETURN n");\n\n    for (Map&lt;String, Object&gt; row : result) {\n        n = (Node) row.get("n");\n        found = true;\n        break;\n    }\n\n    if (found) {\n        myIndexIDs.remove(index);\n        myIndexIDs.trimToSize();\n    }\n\n    return n;\n}\n\n// fill the arraylist with node ids\nprivate void createMyNodeIDs() {\n    System.out.println("create node cache");\n    IndexHits&lt;Node&gt; result = this.myIndex.query("used:false");\n    int count = 0;\n\n    while (result.hasNext() &amp;&amp; count &lt;= this.maxCacheSize) {\n        Node n = result.next();\n        if (!(n.hasProperty("foo") &amp;&amp; "bar" == (String) n.getProperty("foo"))) {\n            myIndexIDs.add(n.getId());\n            count++;\n        }\n    }\n\n    result.close();\n}\n\n// returns a random index from the cache\nprivate int getMyIndexNodeIndex() {\n    // create a new index if you're feeling that it became too small\n    if (this.myIndexIDs.size() &lt; this.minCacheSize) {\n        createMyNodeIDs();\n    }\n    // the current size of the cache\n    System.out.println(this.myIndexIDs.size());\n\n    // http://stackoverflow.com/a/363732/520544\n    return (int) (Math.random() * ((this.myIndexIDs.size() - 1) + 1));\n}\n
551	&lt;entity \n    name="posts"\n    pk="id"\n    datasource="mysql"\n    query="select id, title, description from posts"\n    deltaQuery="select id from posts where updated_at &gt; '${dataimporter.last_index_time}'"\n    deltaImportQuery="select id, title, description from posts where id='${dataimporter.delta.id}'"&gt;\n  &lt;field column="id" name="id" indexed="true" stored="true" /&gt;\n  &lt;field column="title" name="title" indexed="true" stored="true" /&gt;\n  &lt;field column="description" name="description" indexed="true" stored="true" /&gt;    \n&lt;/entity&gt;\n
552	Filter filter = new TermRangeFilter("field", "0", null, true, false);\n
553	curl -XGET http://my_url:9200/idx_occurrence/Occurrene/_search?pretty=true -d '{\n    "query": {\n        "filtered": {\n            "filter": {\n                "missing": {\n                    "field": "decimallatitude",\n                    "existence": true,\n                    "null_value": true\n                }\n            },\n            "query": {\n                "query_string": {\n                    "fields": ["dataset"],\n                    "query": "3",\n                    "default_operator": "AND"\n                }\n            }\n        }\n    },\n    "facets": {\n        "test": {\n            "terms": {\n                "field": ["kingdom_interpreted"],\n                "size": 5\n            }\n        }\n    }\n}\n'\n
554	cypherQ =       """START n=node:my_nodes({queryParam})\n               RETURN n"""\n
555	var articles = RavenSession.Advanced\n                           .LuceneQuery&lt;Article&gt;("AllDocs/ByTags")\n                           .Include(x =&gt; x.Author);\nif (user.FavouriteTags.Any()) {\n    string favesQuery = String.Join(" OR ", user.FavouriteTags.Select(x =&gt; String.Format("Tags:{0}", x)));\n    articles = articles.Where(favesQuery).Boost(0.5m);\n}\nif (user.BlockedTags.Any()) {\n    string blockedQuery = String.Join(" AND ", user.BlockedTags.Select(x =&gt; String.Format("-Tags:{0}", x)));\n    articles = articles.Where(blockedQuery);\n}\narticles = articles.OrderByDescending(x =&gt; x.DatePublished);\n
556	&lt;doc&gt;\n  &lt;str name="collection"&gt;61&lt;/str&gt;\n  ...other fields...\n  &lt;long name="uuid"&gt;1111&lt;/long&gt;\n&lt;/doc&gt;\n
557	&lt;script&gt;&lt;![CDATA[\n    function addFields(row)    {\n        var databaseId = row.get('databaseId');\n        row.put(databaseId_'company'+, "value");\n        return row;\n    } \n]]&gt;&lt;/script&gt;\n
558	hibernate.search.default.directory_provider=com.foo.MongoLuceneDirectoryProvider\n
559	String join_token = tok.nextToken(); // token is like "stem1 stem2 stem3"\nTokenStream stream = new WhitespaceTokenizer(Version.LUCENE_41, new StringReader(join_token));\nstream = new PositionFilter(stream, 0); // 0 also happens to be the default\n
560	&lt;updateRequestProcessorChain&gt;\n    &lt;processor class="solr.NoCommitUpdateRequestProcessorFactory" /&gt;\n    &lt;processor class="solr.LogUpdateProcessorFactory" /&gt;\n    &lt;processor class="solr.RunUpdateProcessorFactory" /&gt;\n&lt;/updateRequestProcessorChain&gt;\n
561	StringReader reader = new StringReader("ramarim 129202 cinza");\nLetterTokenizer stream = new LetterTokenizer(Version.LUCENE_42, reader);        \nstream.setReader(reader);\nstream.reset();\nwhile(stream.incrementToken()) {\n    System.out.println(stream.reflectAsString(false));\n}\nstream.close();\n
562	Analyzer analyzer = new Analyzer() {\n @Override\n  protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n    Tokenizer source = new FooTokenizer(reader); //ex. StandardAnalyzer\n    TokenStream filter = new FooFilter(source); //ex. StandardFilter, LowercaseFilter, StopFilter, etc.\n    filter = new BarFilter(filter);\n    filter = new LengthFilter(true, filter, 3, Integer.MAX_VALUE);\n    return new TokenStreamComponents(source, filter);\n  }\n};\n
563	List&lt;POISearch&gt; results = repository.findByAutocompleteStartingWith(Arrays.asList("los", "ange"));\n
564	@Field(bridge = @FieldBridge(impl = UUIDFieldBridge.class))\nprivate Set&lt;UUID&gt; uuids;\n
565	public static void PrintLines(string filepath,string key)\n    {\n        int counter = 1;\n        string line;\n\n        // Read the file and display it line by line.\n        System.IO.StreamReader file = new System.IO.StreamReader(filepath);\n        while ((line = file.ReadLine()) != null)\n        {\n            if (line.Contains(key))\n            {\n                Console.WriteLine("\t"+counter.ToString() + ": " + line);\n            }\n            counter++;\n        }\n        file.Close();\n    }\n
566	Analyzer analyzer = new Analyzer() {\n @Override\n  protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n    KeywordTokenizer source = new KeywordTokenizer(reader);\n    LowercaseFilter filter = new LowercaseFilter(source);\n    filter = new EdgeNGramTokenFilter(filter, EdgeNGramTokenFilter.Side.BACK, 2, 50);\n    return new TokenStreamComponents(source, filter);\n  }\n};\n
567	&lt;script&gt;&lt;![CDATA[\n        function changeFileName(row){\n            var fileName= row.get('fileName');\n            // Replace or remove the extension .. e.g. from last index of . \n            file_name_new = file_name.replace ......\n            row.put(fileName, row.get('file_name_new'));\n            return row;\n        }\n]]&gt;&lt;/script&gt;\n
568	for (int i=0;i&lt;res.length;i++)\n{\n    int flag=1;\n    String s1=res[i].toLowerCase();\n\n    for(int j=0;j&lt;stopwords.length;j++)\n    {\n        if(s1.equals(stopwords[j]))\n        {\n            flag=0;\n        }\n        // -------- We are still looping through stopwords!  This for loop should be closed here! ---------\n        if(flag!=0)\n        {\n            if (s1.length() &gt; 0) \n            { \n                //Now this is going to add to the list for every entry in stopwords, until we find a match!\n                Integer frequency = (Integer) map.get(s1);\n                if (frequency == null) \n                {\n                    frequency = ONE;\n                } else \n                {\n                    int value = frequency.intValue();\n                    frequency = new Integer(value + 1);\n                }\n                map.put(s1, frequency);\n            }  \n        }\n    }\n}\n
569	org.hibernate.Query fullTextQuery = fts.createFullTextQuery(luceneQuery, CatalogueBase.class);\nfullTextQuery.setProjection("title");\nList contactList = fullTextQuery.list();\n
570	 if (expand) {\n   for (int i = 0; i &lt; size; i++) {\n     for (int j = 0; j &lt; size; j++) {\n       add(synset[i], synset[j], false);\n     }\n   }\n } else {\n   for (int i = 0; i &lt; size; i++) {\n     add(synset[i], synset[0], false);\n   }\n }\n
571	 DocumentBuilder builder = /*create your builder*/\n Document doc = builder.createDocument(image, id);\n Field metadata = /*create your metadata field*/\n doc.add(metadata);\n indexWriter.addDocument(doc);\n
572	public final class CustomKeywordAnalyzer extends Analyzer {\n  public CustomKeywordAnalyzer() {\n  }\n\n  @Override\n  protected TokenStreamComponents createComponents(final String fieldName, final Reader reader) {\n    Tokenizer tokenizer = new KeywordTokenizer(reader)\n    TokenStream filter = new TrimFilter(Version.LUCENE_43, tokenizer);\n    return new TokenStreamComponents(tokenizer, filter);\n  }\n}\n
573	&lt;searchComponent name="terms" class="solr.TermsComponent"/&gt;\n  &lt;!-- A request handler for demonstrating the terms component --&gt;\n  &lt;requestHandler name="/terms" class="solr.SearchHandler" startup="lazy"&gt;\n     &lt;lst name="defaults"&gt;\n      &lt;bool name="terms"&gt;true&lt;/bool&gt;\n      &lt;bool name="distrib"&gt;false&lt;/bool&gt;\n    &lt;/lst&gt;     \n    &lt;arr name="components"&gt;\n      &lt;str&gt;terms&lt;/str&gt;\n    &lt;/arr&gt;\n  &lt;/requestHandler&gt;\n
574	using System;\nusing System.IO;\nusing Lucene.Net.Analysis;\nusing Lucene.Net.Analysis.Tokenattributes;\nusing Lucene.Net.Documents;\nusing Lucene.Net.Index;\nusing Lucene.Net.Search;\nusing Lucene.Net.Search.Payloads;\nusing Lucene.Net.Store;\n\npublic static class Program {\n    public static void Main() {\n        var directory = new RAMDirectory();\n\n        // Initialization; create 50 documents with payload\n        var writer = new IndexWriter(directory, new KeywordAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);\n        for (var i = 0; i &lt; 50; ++i) {\n            AddDocument(writer, i, "lorem ipsum etc blah blah");\n        }\n        writer.Commit();\n\n        var searcher = new IndexSearcher(directory, readOnly: true);\n        searcher.Similarity = new ShazaamPayloadSimilarity();\n\n        // The term we'll be looking for. This should match all documents.\n        var term = new Term("Data", "lorem");\n        var query = new PayloadTermQuery(term, new MaxPayloadFunction());\n        var topDocs = searcher.Search(query, 40);\n\n        // This is a bad example of a FieldCache usage.\n        var iValues = FieldCache_Fields.DEFAULT.GetStrings(searcher.IndexReader, "Id");\n        foreach (var scoreDoc in topDocs.ScoreDocs) {\n            Console.WriteLine("Score: {0:0.0000}  i={1}", scoreDoc.Score, iValues[scoreDoc.Doc]);\n        }\n\n        Console.ReadLine();\n    }\n\n    public static void AddDocument(IndexWriter writer, Int32 id, String data) {\n        var payload = BitConverter.GetBytes(id);\n        var analyzer = new ShazaamPayloadAnalyzer(payload);\n        var textReader = new StringReader(data);\n\n        var document = new Document();\n        document.Add(new Field("Id", id.ToString(), Field.Store.NO, Field.Index.NOT_ANALYZED));\n        document.Add(new Field("Data", analyzer.TokenStream(null, textReader)));\n\n        writer.AddDocument(document);\n    }\n}\n\npublic class ShazaamPayloadAnalyzer : Analyzer {\n    private readonly Byte[] _value;\n\n    public ShazaamPayloadAnalyzer(Byte[] value) {\n        _value = value;\n    }\n\n    public override TokenStream TokenStream(String fieldName, TextReader reader) {\n        TokenStream result = new WhitespaceTokenizer(reader);\n        result = new ShazaamPayloadFilter(result, _value);\n        return result;\n    }\n}\n\npublic class ShazaamPayloadFilter : TokenFilter {\n    private readonly byte[] _payload;\n    private readonly IPayloadAttribute _payloadAttr;\n\n    public ShazaamPayloadFilter(TokenStream input, Byte[] payload)\n        : base(input) {\n        _payload = payload;\n        _payloadAttr = AddAttribute&lt;IPayloadAttribute&gt;();\n    }\n\n    public override Boolean IncrementToken() {\n        if (input.IncrementToken()) {\n            _payloadAttr.Payload = new Payload(_payload);\n            return true;\n        }\n\n        return false;\n    }\n}\n\npublic class ShazaamPayloadSimilarity : DefaultSimilarity {\n    public override Single ScorePayload(Int32 docId, String fieldName, Int32 start, Int32 end, Byte[] payload, Int32 offset, Int32 length) {\n        var originalValue = BitConverter.ToInt32(payload, startIndex: 0);\n\n        // Advanced logic ahead!\n        return (originalValue % 3);\n    }\n}\n
575	TextField field = new TextField(FULLTEXT_COL, value, Field.Store.NO);\nd.add(field);\n//exact search\nField exactField = new TextField(FullTextIndexationManager.EXACT_COL + FULLTEXT_COL, value, Field.Store.NO);                    \nd.add(exactField);\n
576	public static TermInfo[] GetHighFreqTerms(Directory dir,\n                                          Hashtable junkWords,\n                                          int numTerms,\n                                          String[] fields)\n{\n    if (dir == null || fields == null) return new TermInfo[0];\n\n    IndexReader reader = IndexReader.Open(dir, true);\n    TermInfoQueue tiq = new TermInfoQueue(numTerms);\n    TermEnum terms = reader.Terms();\n\n    int minFreq = 0;\n\n    while (terms.Next())\n    {\n        String field = terms.Term.Field;\n\n        if (fields != null &amp;&amp; fields.Length &gt; 0)\n        {\n            bool skip = true;\n\n            for (int i = 0; i &lt; fields.Length; i++)\n            {\n                if (field.Equals(fields[i]))\n                {\n                    skip = false;\n                    break;\n                }\n            }\n            if (skip) continue;\n        }\n\n        if (junkWords != null &amp;&amp; junkWords[terms.Term.Text] != null)\n            continue;\n\n        if (terms.DocFreq() &gt; minFreq)\n        {\n            tiq.Add(new TermInfo(terms.Term, terms.DocFreq()));\n            if (tiq.Size() &gt;= numTerms)              // if tiq overfull\n            {\n                tiq.Pop();                   // remove lowest in tiq\n                minFreq = ((TermInfo)tiq.Top()).DocFreq; // reset minFreq\n            }\n        }\n    }\n\n    TermInfo[] res = new TermInfo[tiq.Size()];\n\n    for (int i = 0; i &lt; res.Length; i++)\n    {\n        res[res.Length - i - 1] = (TermInfo)tiq.Pop();\n    }\n\n    reader.Dispose();\n\n    return res;\n}\n
577	0.26880693 = (MATCH) product of:\n  0.40321037 = (MATCH) sum of:\n    0.10876686 = (MATCH) weight(text:a in 0) [DefaultSimilarity], result of:\n      0.10876686 = score(doc=0,freq=2.0 = termFreq=2.0\n), product of:\n        0.25872254 = queryWeight, product of:\n          0.5945349 = idf(docFreq=2, maxDocs=2)\n          0.435168 = queryNorm\n        0.42039964 = fieldWeight in 0, product of:\n          1.4142135 = tf(freq=2.0), with freq of:\n            2.0 = termFreq=2.0\n          0.5945349 = idf(docFreq=2, maxDocs=2)\n          0.5 = fieldNorm(doc=0)\n    0.07690979 = (MATCH) weight(text:b in 0) [DefaultSimilarity], result of:\n      0.07690979 = score(doc=0,freq=1.0 = termFreq=1.0\n), product of:\n        0.25872254 = queryWeight, product of:\n          0.5945349 = idf(docFreq=2, maxDocs=2)\n          0.435168 = queryNorm\n        0.29726744 = fieldWeight in 0, product of:\n          1.0 = tf(freq=1.0), with freq of:\n            1.0 = termFreq=1.0\n          0.5945349 = idf(docFreq=2, maxDocs=2)\n          0.5 = fieldNorm(doc=0)\n    0.10876686 = (MATCH) weight(text:a in 0) [DefaultSimilarity], result of:\n      0.10876686 = score(doc=0,freq=2.0 = termFreq=2.0\n), product of:\n        0.25872254 = queryWeight, product of:\n          0.5945349 = idf(docFreq=2, maxDocs=2)\n          0.435168 = queryNorm\n        0.42039964 = fieldWeight in 0, product of:\n          1.4142135 = tf(freq=2.0), with freq of:\n            2.0 = termFreq=2.0\n          0.5945349 = idf(docFreq=2, maxDocs=2)\n          0.5 = fieldNorm(doc=0)\n    0.10876686 = (MATCH) weight(text:a in 0) [DefaultSimilarity], result of:\n      0.10876686 = score(doc=0,freq=2.0 = termFreq=2.0\n), product of:\n        0.25872254 = queryWeight, product of:\n          0.5945349 = idf(docFreq=2, maxDocs=2)\n          0.435168 = queryNorm\n        0.42039964 = fieldWeight in 0, product of:\n          1.4142135 = tf(freq=2.0), with freq of:\n            2.0 = termFreq=2.0\n          0.5945349 = idf(docFreq=2, maxDocs=2)\n          0.5 = fieldNorm(doc=0)\n  0.6666667 = coord(4/6)\n
578	function(doc) {\n   var included = [];\n   if(doc.foo) {\n       index("foo", doc.foo);\n       included.push("foo");\n   }\n\n   if(doc.bar) {\n       index("bar", doc.bar);\n       included.push("bar");\n   }\n\n   if(included.length &gt; 0) {\n       index("has", included.join(" "));\n   }\n}\n
579	RegexpQuery dynamicRegEx3 = new RegexpQuery(new Term("name", "C[^ ]* O.*"));\n
580	http://patrickisgreat.me:8983/solr/privatelounge/select?q=text%3A%22hello%22&amp;wt=json&amp;indent=true\n
581	Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_48);\nQueryParser parser = new QueryParser(Version.LUCENE_48, "keyword", analyzer);\nQuery query = parse.parse("The quick brown fox jumped over the lazy dog");\nint maxHits = 10; //Or however many you want\nTopDocs topdocs = indexSearcher.search(query, maxHits);\n
582	@Test\npublic void testSearchUsingQueryBuilder() {\n    FullTextSession fullTextSession = org.hibernate.search.Search\n            .getFullTextSession(sessionFactory.getCurrentSession());\n    QueryBuilder builder = fullTextSession.getSearchFactory()\n            .buildQueryBuilder().forEntity(Country.class).get();\n    org.apache.lucene.search.Query query = builder.all().createQuery();\n    FullTextQuery jpaQuery = fullTextSession.createFullTextQuery(query,\n            Country.class);\n    System.out.println("Before HQL =" + startDate);\n    List&lt;Country&gt; tempCountry = jpaQuery.list();\n}\n
583	sitecore://web/{110D559F-DEA5-42EA-9C1C-8A5DF7E70EF9}?lang=en&amp;ver=1\nsitecore://master/{110D559F-DEA5-42EA-9C1C-8A5DF7E70EF9}?lang=en&amp;ver=1 (subresult)\nsitecore://master/{110D559F-DEA5-42EA-9C1C-8A5DF7E70EF9}?lang=ja-JP&amp;ver=1 (subresult)\nsitecore://master/{110D559F-DEA5-42EA-9C1C-8A5DF7E70EF9}?lang=de-DE&amp;ver=1 (subresult)\nsitecore://master/{110D559F-DEA5-42EA-9C1C-8A5DF7E70EF9}?lang=da&amp;ver=1 (subresult)\n
584	// source text\nval rdd = sc.textFile(...)  \n// stop words src\nval stopWordsRdd = sc.textFile(...) \n// bring stop words to the driver to broadcast =&gt; more efficient than rdd.subtract(stopWordsRdd)\nval stopWords = stopWordsRdd.collect.toSet\nval stopWordsBroadcast = sc.broadcast(stopWords)\nval words = rdd.flatMap(line =&gt; line.split("\\W").map(_.toLowerCase))\nval cleaned = words.mapPartitions{iterator =&gt; \n    val stopWordsSet = stopWordsBroadcast.value\n    iterator.filter(elem =&gt; !stopWordsSet.contains(elem))\n    }\n// plug the normalizer function here\nval normalized = cleaned.map(normalForm(_)) \n
585	string strIndexDir = @"D:\Index";\nLucene.Net.Store.Directory indexDir = Lucene.Net.Store.FSDirectory.Open(new System.IO.DirectoryInfo(strIndexDir));\nAnalyzer std = new JapaneseAnalyzer(Lucene.Net.Util.Version.LUCENE_29); //Version parameter is used for backward compatibility. Stop words can also be passed to avoid indexing certain words\nIndexWriter idxw = new IndexWriter(indexDir, std, true, IndexWriter.MaxFieldLength.UNLIMITED);     \n\n//Create an Index writer object.\nLucene.Net.Documents.Document doc = new Lucene.Net.Documents.Document();\nLucene.Net.Documents.Field fldText = new Lucene.Net.Documents.Field("text", System.IO.File.ReadAllText(@"d:\test.txt"), Lucene.Net.Documents.Field.Store.YES, Lucene.Net.Documents.Field.Index.ANALYZED, Lucene.Net.Documents.Field.TermVector.YES);\ndoc.Add(fldText);\n\n//write the document to the index\nidxw.AddDocument(doc);\n\n//optimize and close the writer\nidxw.Optimize();\nidxw.Close();\nResponse.Write("Indexing Done");\n
586	_enIndexWriter.DeleteDocuments(query);\n_enIndexWriter.Commit();\n_enIndexReader = _enIndexReader.Reopen();\n_enIndexSearcher = new IndexSearcher(_enIndexReader);\n
587	@Override\nprotected Query getWildcardQuery (String field, String termStr) throws ParseException\n{\n    String term = termStr;\n    TokenStream stream = null;\n    try\n    {\n        // we want only a single token and we don't want to lose special characters\n        stream = new KeywordTokenizer (new StringReader (term));\n\n        stream = new LowerCaseFilter (Version.LUCENE_45, stream);\n        stream = new ASCIIFoldingFilter (stream);\n\n        CharTermAttribute charTermAttribute = stream.addAttribute (CharTermAttribute.class);\n\n        stream.reset ();\n        while (stream.incrementToken ())\n        {\n            term = charTermAttribute.toString ();\n        }\n    }\n    catch (IOException e)\n    {\n        LOGGER.debug ("Failed to filter search query token {}", term, e);\n    }\n    finally\n    {\n        IOUtils.closeQuietly (stream);\n    }\n    return super.getWildcardQuery (field, term);\n}\n
588	Directory dir = new RAMDirectory();\nAnalyzer analyzer = new KeywordAnalyzer();\nIndexWriterConfig iwc = new IndexWriterConfig(analyzer);\niwc.setOpenMode(IndexWriterConfig.OpenMode.CREATE);\nIndexWriter writer = new IndexWriter(dir, iwc);\n
589	@Entity\n@Indexed\npublic class B {\n   @IndexedEmbedded(includePaths="a.b")\n   @ManyToOne\n   private A a;\n   ......\n}\n
590	tok=new NGramTokenFilter(tok,2,5);\n
591	private Query createQuery(PriorityQueue&lt;ScoreTerm&gt; q) {\n  BooleanQuery query = new BooleanQuery();\n  ScoreTerm scoreTerm;\n  float bestScore = -1;\n\n  while ((scoreTerm = q.pop()) != null) {\n    TermQuery tq = new TermQuery(new Term(scoreTerm.topField, scoreTerm.word));\n\n    if (boost) {\n      if (bestScore == -1) {\n        bestScore = (scoreTerm.score);\n      }\n      float myScore = (scoreTerm.score);\n      tq.setBoost(boostFactor * myScore / bestScore);\n    }\n\n    try {\n      query.add(tq, BooleanClause.Occur.SHOULD);\n    }\n    catch (BooleanQuery.TooManyClauses ignore) {\n      break;\n    }\n  }\n  return query;\n}\n
592	// Your user query, which contains both Occur.SHOULD and Occur.MUST clauses\nBooleanQuery userQuery = createUserQuery();\n\n// Your filter query, could be a BooleanQuery on its own\nQuery filterQuery = getFilterQuery();\n\nBooleanQuery finalQuery = new BooleanQuery();\nfinalQuery.add(userQuery, Occur.MUST);\nfinalQuery.add(filterQuery, Occur.FILTER);\n// Search using the finalQuery\nTopDocs docs = mySearcher.search(finalQuery, maxdocs, sort);\n
593	double d = 1.0 / Math.sqrt(2);\nfloat f = (float)d;\nbyte b = SmallFloat.floatToByte315(f);\nfloat bf = SmallFloat.byte315ToFloat(b);\nSystem.out.println(bf);\n
594	    @Override\n    public void postProcessSearchQuery(BooleanQuery searchQuery, SearchContext searchContext)\n        throws Exception {\n        System.out.println("-----postProcessSearchQuery called------");\n\n        addSearchTerm(searchQuery, searchContext, "sampleCollectionName", true);\n        addSearchTerm(searchQuery, searchContext, "biobankName", true);\n        addSearchTerm(searchQuery, searchContext, "materialType", true);\n        addSearchTerm(searchQuery, searchContext, "container", true);\n        addSearchTerm(searchQuery, searchContext, "storageTemperature", true);\n        .....\n        .....\n\n        LinkedHashMap&lt;String, Object&gt; params =\n            (LinkedHashMap&lt;String, Object&gt;)searchContext.getAttribute("params");\n\n        if (params != null) {\n            String expandoAttributes = (String)params.get("expandoAttributes");\n\n            if (Validator.isNotNull(expandoAttributes)) {\n                addSearchExpando(searchQuery, searchContext, expandoAttributes);\n            }\n        }\n    }\n
595	&lt;IndexSet SetName="UserIndexSet" IndexPath="~/App_Data/TEMP/ExamineIndexes/User/"&gt;\n   &lt;IndexUserFields&gt;\n        &lt;&lt; Your nodes here &gt;&gt;\n   &lt;/IndexUserFields&gt;\n   &lt;IncludeNodeTypes&gt;\n      &lt;add Name="&lt;&lt; Document Type name &gt;&gt;" /&gt;\n   &lt;/IncludeNodeTypes&gt;\n&lt;/IndexSet&gt;\n
596	/***************************UpdateScript*********************************/\n\n    function getAnalyzerResult(analyzer, fieldName, fieldValue) {\n      var result = [];\n      var token_stream = analyzer.tokenStream(fieldName, new java.io.StringReader(fieldValue));//null value?\n      var term_att = token_stream.getAttribute(Packages.org.apache.lucene.analysis.tokenattributes.CharTermAttribute.class);\n      token_stream.reset();\n      while (token_stream.incrementToken()) {\n        result.push(term_att.toString());\n      }\n      token_stream.end();\n      token_stream.close();\n      return result;\n    }\n    function processAdd(cmd) {\n      doc = cmd.solrDoc;  // org.apache.solr.common.SolrInputDocument\n      id = doc.getFieldValue("id");\n      logger.warn("update-script#processAdd: id=" + id);\n\n      var content = doc.getFieldValue("content"); // Comes from /update/extract\n      //facetList contains the actual facet terms\n      //facetAnalyzerName contains the Analyzer name for the term vector list names. (i.e the field type)\n      var facetList = ["cytokineTerms", "cancerTerms"];\n      var facetAnalyzerName = ["key_phrases", "ColonCancer"];\n      /*\n        Loop through all of the facets, and get the analyzer and the name for the field\n        Then add the terms to the document\n      */\n      for(var i = 0; i &lt; facetList.length; i++){\n        var analyzer = req.getCore().getLatestSchema().getFieldTypeByName(facetAnalyzerName[i]).getIndexAnalyzer();\n        var terms = getAnalyzerResult(analyzer, null, content);\n        for(var index = 0; index &lt; terms.length; index++){\n          doc.addField(facetList[i], terms[index]);  \n        }\n      }  \n    }\n    // The functions below must be defined, but there's rarely a need to implement\n    // anything in these.\n    function processDelete(cmd) {\n      // no-op\n    }\n    function processMergeIndexes(cmd) {\n      // no-op\n    }\n    function processCommit(cmd) {\n      // no-op\n    }\n    function processRollback(cmd) {\n      // no-op\n    }\n    function finish() {\n      // no-op\n    }\n    /***************************UpdateScript*********************************/\n
597	public class SearchLayer1 {\n  // ...\n  private DirectoryReader directoryReader;\n  private IndexSearcher indexSearcher;\n\n  public SearchLayer1() throws IOException {\n    // ...\n    this.directoryReader = DirectoryReader.open(indexWriter, false);\n    this.indexSearcher = new IndexSearcher(directoryReader);\n  }\n\n  // ...\n\n  private void refreshReader() throws IOException {\n    DirectoryReader newReader = DirectoryReader.openIfChanged(this.directoryReader);\n    if (newReader != null &amp;&amp; newReader != this.directoryReader) {\n      this.directoryReader.close();\n      this.directoryReader = newReader;\n      this.indexSearcher = new IndexSearcher(this.directoryReader);\n    }\n  }\n\n  public void experiment() throws IOException {\n    refreshReader();\n    IndexSearcher isearcher = this.indexSearcher;\n    // ...\n  }\n\n  public void close() throws IOException {\n    directoryReader.close();\n    // ...\n  }\n}\n
598	public class FilterByIntegerSetQuery extends Query\n{\n    protected String numericDocValueFieldName;\n    protected Set&lt;Integer&gt; allowedValues;\n\n\n    public FilterByIntegerSetQuery(String numericDocValueFieldName, Set&lt;Integer&gt; allowedValues)\n    {\n        this.numericDocValueFieldName = numericDocValueFieldName;\n        this.allowedValues = allowedValues;\n    }\n\n    @Override\n    public Weight createWeight(IndexSearcher searcher, boolean needsScores)\n    {\n        return new RandomAccessWeight(this)\n        {\n            @Override\n            protected Bits getMatchingDocs(LeafReaderContext context) throws IOException\n            {\n                final int len = context.reader().maxDoc();\n                final NumericDocValues values = context.reader().getNumericDocValues(numericDocValueFieldName);\n                return new Bits()\n                {\n                    @Override\n                    public boolean get(int index)\n                    {\n                        return allowedValues.contains((int) values.get(index));\n                    }\n\n                    @Override\n                    public int length()\n                    {\n                        return len;\n                    }\n                };\n            }\n        };\n    }\n\n\n    @Override\n    public String toString(String field)\n    {\n        return "(filter "+numericDocValueFieldName+" by set)";\n    }\n}\n
599	&lt;field name="ratio" type="tfloat" stored="true" indexed="true"/&gt;\n
600	public String removeStopWords(){\n    StandardAnalyzer analyser = new StandardAnalyzer();\n    Analyzer analyzer = new StopAnalyzer();\n    Tokenizer tokenizer = new StandardTokenizer();\n    tokenizer.setReader(new StringReader(GetTweets.tweetContent));\n    TokenStream tokenStream = tokenizer;\n    StringBuilder sb = new StringBuilder();\n    tokenStream = new StopFilter(tokenStream, StandardAnalyzer.STOP_WORDS_SET);\n    CharTermAttribute token = tokenStream.getAttribute(CharTermAttribute.class);\n
601	curl "http://localhost:8983/solr/dummy/update?wt=json&amp;indent=true&amp;commitWithin=2000" -d '[\n{"id":"4", "name":"Bob Smith", "create_date":"2016-02-16T14:36:12Z"}\n]'\n
602	[TestClass]\npublic class UnitTest4\n{\n    [TestMethod]\n    public void TestLucene()\n    {\n        var writer = CreateIndex();\n        Add(writer, "tasty", "00018389732061");\n        writer.Flush(true, true, true);\n\n        var searcher = new IndexSearcher(writer.GetReader());\n        Test(searcher, "(Description:tasty) (Gtin:00018389732061)");\n        Test(searcher, "Description:tasty Gtin:00018389732061");\n        Test(searcher, "+Description:tasty +Gtin:00018389732061");\n        Test(searcher, "+Description:tasty +Gtin:000*");\n\n        writer.Dispose();\n    }\n\n    private void Test(IndexSearcher searcher, string query)\n    {\n        var result = Search(searcher, query);\n        Console.WriteLine(string.Join(", ", result));\n        Assert.AreEqual(1, result.Count);\n        Assert.AreEqual("00018389732061", result[0]);\n    }\n\n    private List&lt;string&gt; Search(IndexSearcher searcher, string expr)\n    {\n        using (var analyzer = new StandardAnalyzer(Lucene.Net.Util.Version.LUCENE_30))\n        {\n            var queryParser = new QueryParser(Lucene.Net.Util.Version.LUCENE_30, "Description", analyzer);\n            var collector = TopScoreDocCollector.Create(1000, true);\n            var query = queryParser.Parse(expr);\n            searcher.Search(query, collector);\n\n            var result = new List&lt;string&gt;();\n            var matches = collector.TopDocs().ScoreDocs;\n            foreach (var item in matches)\n            {\n                var id = item.Doc;\n                var doc = searcher.Doc(id);\n                result.Add(doc.GetField("Gtin").StringValue);\n            }\n            return result;\n        }\n    }\n\n    IndexWriter CreateIndex()\n    {\n        var directory = new RAMDirectory();\n\n        var analyzer = new StandardAnalyzer(Lucene.Net.Util.Version.LUCENE_30);\n        var writer = new IndexWriter(directory, analyzer, new IndexWriter.MaxFieldLength(1000));\n\n        return writer;\n    }\n    void Add(IndexWriter writer, string desc, string id)\n    {\n        var document = new Document();\n        document.Add(new Field("Description", desc, Field.Store.YES, Field.Index.ANALYZED));\n        document.Add(new Field("Gtin", id, Field.Store.YES, Field.Index.NOT_ANALYZED));\n\n        writer.AddDocument(document);\n    }\n}\n
603	Query prequery = parser.parse("\"test text\"");\nQuery postquery = parser.parse("*relevant*");\nTopDocs docs = searcher.search(prequery, 10);\ndocs = QueryRescorer.rescore(searcher, docs, postquery, 2, 10);\n
604	DELIMITER $$\nCREATE FUNCTION levenshtein( s1 VARCHAR(255), s2 VARCHAR(255) )\nRETURNS INT\nDETERMINISTIC\nBEGIN\nDECLARE s1_len, s2_len, i, j, c, c_temp, cost INT;\nDECLARE s1_char CHAR;\n-- max strlen=255\nDECLARE cv0, cv1 VARBINARY(256);\nSET s1_len = CHAR_LENGTH(s1), s2_len = CHAR_LENGTH(s2), cv1 = 0x00, j = 1, i = 1, c = 0;\nIF s1 = s2 THEN\nRETURN 0;\nELSEIF s1_len = 0 THEN\nRETURN s2_len;\nELSEIF s2_len = 0 THEN\nRETURN s1_len;\nELSE\nWHILE j &lt;= s2_len DO\nSET cv1 = CONCAT(cv1, UNHEX(HEX(j))), j = j + 1;\nEND WHILE;\nWHILE i &lt;= s1_len DO\nSET s1_char = SUBSTRING(s1, i, 1), c = i, cv0 = UNHEX(HEX(i)), j = 1;\nWHILE j &lt;= s2_len DO\nSET c = c + 1;\nIF s1_char = SUBSTRING(s2, j, 1) THEN\nSET cost = 0; ELSE SET cost = 1;\nEND IF;\nSET c_temp = CONV(HEX(SUBSTRING(cv1, j, 1)), 16, 10) + cost;\nIF c &gt; c_temp THEN SET c = c_temp; END IF;\nSET c_temp = CONV(HEX(SUBSTRING(cv1, j+1, 1)), 16, 10) + 1;\nIF c &gt; c_temp THEN\nSET c = c_temp;\nEND IF;\nSET cv0 = CONCAT(cv0, UNHEX(HEX(c))), j = j + 1;\nEND WHILE;\nSET cv1 = cv0, i = i + 1;\nEND WHILE;\nEND IF;\nRETURN c;\nEND$$\nDELIMITER ;\n
605	writer = new IndexWriter(indexDirectory, new StandardAnalyzer(Version.LUCENE_36), false, IndexWriter.MaxFieldLength.UNLIMITED);\n
606	int year = datetime.getYear();\nint month = datetime.getMonthOfYear();\nint day = datetime.getDayOfMonth();\n\nQueryBuilder qb = sm.buildQueryBuilderForClass(BlogEntry.class).get();\nQuery q = qb.bool()\n    .must( qb.keyword().onField("creationdate.year").ignoreFieldBridge().ignoreAnalyzer()\n                .matching(year).createQuery() )\n    .must( qb.keyword().onField("creationdate.month").ignoreFieldBridge().ignoreAnalyzer()\n                .matching(month).createQuery() )\n    .must( qb.keyword().onField("creationdate.day").ignoreFieldBridge().ignoreAnalyzer()\n                .matching(day).createQuery() )\n   .createQuery();\n\nCacheQuery cq = sm.getQuery(q, BlogEntry.class);\nSystem.out.println(cq.getResultSize());\n
607	@ImplementedBy(classOf[ConcreteIndex])\ntrait Index {\n  def index(s: String): Unit\n}\n\n@Singleton\ncase class ConcreteIndex @Inject()(conf: Configuration,\n                                   lifecycle: play.api.inject.ApplicationLifecycle) extends Index {\n\n  private val dir = FSDirectory.open(FileSystems.getDefault.getPath(conf.getString("index.dir").get))\n  private val writer = new IndexWriter(dir, new IndexWriterConfig(new StandardAnalyzer()))\n\n  // Add a lifecycle stop hook that will be called when the Play\n  // server is cleanly shut down...\n  lifecycle.addStopHook(() =&gt; scala.concurrent.Future.successful(writer.close()))\n\n  // For good measure you could also add a JVM runtime shutdown hook\n  // which should be called even if the Play server is terminated.\n  // If the writer is already closed this will be a no-op.\n  Runtime.getRuntime.addShutdownHook(new Thread() { \n    override def run() = writer.close()\n  })\n\n  def index(s: String): Unit = ???\n}\n
608	public List&lt;Address&gt; search() {\n    List&lt;Address&gt; results = new ArrayList&lt;&gt;();\n\n    BooleanQuery.Builder queryBuilder = new BooleanQuery.Builder();\n    queryBuilder.setMinimumNumberShouldMatch(1);\n\n    if(!getLineOne().equals("")) {\n        //This is a MUST clause, and so doesn't factor into the minimumShouldMatch\n        Query query = new FuzzyQuery(new Term("addr1", getLineOne()));\n        queryBuilder.add(query, BooleanClause.Occur.MUST);\n    }\n\n    if(!getLineTwo().equals("")) {\n        Query query = new FuzzyQuery(new Term("addr2", getLineTwo()));\n        queryBuilder.add(query, BooleanClause.Occur.SHOULD);\n    }\n    if(!getCity().equals("")) {\n        Query query = new FuzzyQuery(new Term("addrcity", getCity()));\n        queryBuilder.add(query, BooleanClause.Occur.SHOULD);\n    }\n    if(!getCounty().equals("")) {\n        Query query = new FuzzyQuery(new Term("addrcounty", getCounty()));\n        queryBuilder.add(query, BooleanClause.Occur.SHOULD);\n    }\n    if(!getCountry().equals("")) {\n        Query query = new FuzzyQuery(new Term("addrcountry", getCountry()));\n        queryBuilder.add(query, BooleanClause.Occur.SHOULD);\n    }\n    if(!getPostcode().equals("")) {\n        Query query = new FuzzyQuery(new Term("addrpostcode", getPostcode()));\n        queryBuilder.add(query, BooleanClause.Occur.SHOULD);\n    }\n\n    try {\n        Query toRun = queryBuilder.build();\n\n        List&lt;Document&gt; searchResults = SearchEngine.getInstance(SEARCH_ENGINE)\n                .performSearch(toRun, 50);\n\n        searchResults.forEach(result -&gt; {\n            results.add(new Address(result));\n        });\n    } catch (IOException e) {\n        e.printStackTrace();\n    }\n\n    return results;\n}\n
609	q=email.raw:*domain*\n
610	QueryBuilder qb = fullTextSession.getSearchFactory().buildQueryBuilder().forEntity(Student.class).get();\n    org.apache.lucene.search.Query query = qb.bool().must( qb.keyword().onFields("name","country","skills.skill").matching(keyword).createQuery())\n            .must( qb.keyword().onField("skills.skill").matching(skill).createQuery())\n            .must( qb.keyword().onField("locations.city").matching(location).createQuery())\n            .must( qb.keyword().onField("jobStatus").matching("active").createQuery())\n            .must( qb.keyword().onField("postedOrSaved").matching("posted").createQuery())\n            .createQuery();\n    org.hibernate.Query hibQuery = fullTextSession.createFullTextQuery(query, Jobs.class);\n    List&lt;Jobs&gt; result = hibQuery.list();\n
611	private static Document createDocument(String title, String content) {\n    Document doc = new Document();\n\n    doc.add(new StringField("title", title, Field.Store.YES));\n    FieldType type = new FieldType();\n    type.setTokenized(true);\n    type.setStoreTermVectors(true);\n    type.setStored(false);\n    type.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    doc.add(new Field("content", content, type));\n\n    return doc;\n}\n
612	&lt;field name="trendid" type="string" indexed="true" stored="true" required="true"/&gt;\n&lt;field name="lastmodified" type="date" indexed="true" stored="true"/&gt;\n&lt;field name="modifiedby" type="string" indexed="true" stored="true" /&gt;\n&lt;field name="title" type="text_en" indexed="true" stored="true"/&gt;\n&lt;field name="keywords" type="text_en" indexed="true" stored="true"/&gt;\n&lt;field name="general" type="text_en" indexed="true" stored="true"/&gt;\n&lt;field name="symptom" type="text_en" indexed="true" stored="true"/&gt;\n&lt;field name="diagnosis" type="text_en" indexed="true" stored="true"/&gt;\n&lt;field name="resolution" type="text_en" indexed="true" stored="true"/&gt;\n
613	Document doc = new Document();\ndoc.add(new Field("DOC_ID", "DOCONE", new FieldType(\n            TextField.TYPE_STORED)));\ndoc.add(new Field("TEXT_FIELD", "This", new FieldType(\n            TextField.TYPE_NOT_STORED)));\nwriter.addDocument(doc);\n
614	Query q = new FunctionQuery(new DistanceDualFloatFunction(new IntFieldSource("weight"), new ConstValueSource(245)));\n\nstatic class DistanceDualFloatFunction extends DualFloatFunction {\n\n    public DistanceDualFloatFunction(ValueSource a, ValueSource b) {\n      super(a, b);\n    }\n\n    @Override\n    protected String name() {\n      return "distance function";\n    }\n\n    @Override\n    protected float func(int doc, FunctionValues aVals, FunctionValues bVals) {\n      return 1000 - Math.abs(aVals.intVal(doc) - bVals.intVal(doc));\n    }\n  }\n
615	  @OneToOne\n  @JoinColumn(name="student_student_id")\n  @ContainedIn\n  private Student student\n
616	AtomicReference&lt;Monitor&gt; monitorHolder = new AtomicReference&lt;Monitor&gt;();\n
617	String rawQuery = "name:abc AND age:26";\nQueryParser parser = new QueryParser(Version.LUCENE_45, null, new WhitespaceAnalyzer(Version.LUCENE_45));\nBooleanQuery query = (BooleanQuery) praser.parse(rawQuery);\nquery.clauses().get(0).setQuery(new TermQuery(new Term("name", "abcmodified")));\nquery.clauses().get(1).setQuery(new TermQuery(new Term("userage", "26")));\nSystem.out.println(query);\n
618	  // function to do index search by source\n  private static TopDocs searchBySource(String source, IndexSearcher searcher) throws Exception {\n    // phrase query build\n    PhraseQuery.Builder builder = new PhraseQuery.Builder();\n    TokenStream tokenStream = new StandardAnalyzer().tokenStream("source", source);\n    tokenStream.reset();\n    while (tokenStream.incrementToken()) {\n      CharTermAttribute charTermAttribute = tokenStream.getAttribute(CharTermAttribute.class);\n      builder.add(new Term("source", charTermAttribute.toString()));\n    }\n    tokenStream.end();\n    tokenStream.close();\n    builder.setSlop(2);\n    PhraseQuery query = builder.build();\n    System.out.println(query);\n    // phrase search\n    TopDocs hits = searcher.search(query, 10);\n    return hits;\n  }\n
619	$ python setup.py install\nfound JAVAHOME = /Library/Java/JavaVirtualMachines/jdk1.8.0_141.jdk/Contents/Home\nfound JAVAFRAMEWORKS = /System/Library/Frameworks/JavaVM.framework\n...\n
620	SpanQuery[] clauses = new SpanQuery[2];\n    clauses[0] = new SpanMultiTermQueryWrapper(new FuzzyQuery(new Term("content", "some"), 2));\n    clauses[1] = new SpanMultiTermQueryWrapper(new FuzzyQuery(new Term("content", "text"), 2));\n    SpanNearQuery query = new SpanNearQuery(clauses, 0, true);\n
621	db=# SELECT ST_Within('POINT(14.48 35.85)'::GEOMETRY,'POLYGON((14.45 35.87,14.56 35.87,14.56 35.80,14.45 35.80,14.45 35.87))'::GEOMETRY);\n st_within \n-----------\n t\n(1 row)\n
622	StringBuilder concat = new StringBuilder();\n\nIndexReader reader = DirectoryReader.open(directory);\n\nconcat.append("Path , Parent \n" );\nfor( int i = 0; i &lt;reader.maxDoc(); i++)  {\n    Document doc = reader.document(i);\n\n    String docPath = doc.get("path");\n    String docParent = doc.get("Parent");\n\n    concat.append(docPath + "," + docParent + "\n");\n\n}\n
623	QueryParser parser = new QueryParser(Version.LUCENE_CURRENT, "nombreAnalizado", new StandardAnalyzer(Version.LUCENE_CURRENT));\nQuery query = parser.parse(_que);\n
624	public function updateAction()\n{\n   // something made the db change\n   $hits = $index-&gt;find("name: " . $name);\n   foreach($hits as $hit) {\n     $index-&gt;delete($hit-&gt;id)  \n   }\n\n   $doc = new Zend_Search_Lucene_Document();\n   // build your doc\n\n   $index-&gt;add($doc);\n\n}\n
625	public class TokenGroup {\n\n    ...\n\n    protected boolean isDistinct() {\n        return offsetAtt.startOffset() &gt;= endOffset;\n    }\n\n    ...\n
626	$query = Zend_Search_Lucene_Search_QueryParser::parse("title:($query1) detail:($query2) category:($query3)";\n$hits = $index-&gt;find( $query);\n
627	int myNewDocFreq = bigIndexReader.docFreq(t) / bigIndexReader.maxDoc() * smallIndexReader.maxDoc()\n
628	// Co-occurrence matrix\nHashmap&lt;String,HashMap&lt;String,Integer&gt;&gt; map = new HashMap();\n\n// List of ngrams\nArrayList&lt;ArrayList&lt;String&gt;&gt; ngrams = ..... // assume we've loaded them into here already\n\n// build the matrix\nfor(ArrayList&lt;String&gt; ngram:ngrams){\n  // Calculate word co-occurrence in ngram for all words\n  // result is an map strings-&gt; count\n  // words in alphabetical order\n  Hashmap&lt;String,&lt;ArrayList&lt;String&gt;,Integer&gt; wordCoocurrence = cooccurrence(ngram) // assume we have this\n\n  // then just join this with original\n}\n\n// and just query with words in alphabetic order\n
629	&lt;requestHandler name="/mlt" class="org.apache.solr.handler.MoreLikeThisHandler"/&gt; \n
630	{!edismax boost=recip(pow(ms(NOW,PubDate),&lt;val&gt;),3.16e-11,1,1)}\n
631	IndexSetCollection sets = Examine.LuceneEngine.Config.IndexSets.Instance.Sets;\nIndexSet set = sets["Set_Name"];\nDirectoryInfo dir = set.IndexDirectory;\nstring path = Path.Combine(dir.FullName, "Index");\n
632	@Fields({\n   @Field(name = "owningOrganization_untokenized", index = Index.UN_TOKENIZED),\n   @Field(name = "owningOrganization", index = Index.TOKENIZED, store = Store.YES\n   }\n)\npublic Organization getOwningOrganization(){\n}\n
633	class MyTokenStream(lucene.PythonTokenStream):\n    def __init__(self, terms):\n        lucene.PythonTokenStream.__init__(self)\n        self.terms = iter(terms)\n        self.addAttribute(lucene.TermAttribute.class_)\n    def incrementToken(self):\n        for term in self.terms:\n            self.getAttribute(lucene.TermAttribute.class_).setTermBuffer(term)\n            return True\n        return False\n\nmts = MyTokenStream(['pant', 'on', 'ground', 'look', 'like', 'fool'])\nwhile mts.incrementToken():\n    print mts\n\n&lt;MyTokenStream: (pant)&gt;\n&lt;MyTokenStream: (on)&gt;\n&lt;MyTokenStream: (ground)&gt;\n&lt;MyTokenStream: (look)&gt;\n&lt;MyTokenStream: (like)&gt;\n&lt;MyTokenStream: (fool)&gt;\n
634	public class ShingleFilterTests {\n    private Analyzer analyzer;\n    private IndexSearcher searcher;\n    private IndexReader reader;\n    private QueryParser qp;\n    private Sort sort;\n\n    public static Analyzer createAnalyzer(final int shingles) {\n        return new Analyzer() {\n            @Override\n            public TokenStream tokenStream(String fieldName, Reader reader) {\n                TokenStream tokenizer = new WhitespaceTokenizer(reader);\n                tokenizer = new StopFilter(false, tokenizer, ImmutableSet.of("de", "la", "en"));\n                if (shingles &gt; 0) {\n                    tokenizer = new ShingleFilter(tokenizer, shingles);\n                }\n                return tokenizer;\n            }\n        };\n    }\n\n    public class PopulationComparatorSource extends FieldComparatorSource {\n        @Override\n        public FieldComparator newComparator(String fieldname, int numHits, int sortPos, boolean reversed) throws IOException {\n            return new PopulationComparator(fieldname, numHits);\n        }\n\n        private class PopulationComparator extends FieldComparator {\n            private final String fieldName;\n            private Integer[] values;\n            private int[] populations;\n            private int bottom;\n\n            public PopulationComparator(String fieldname, int numHits) {\n                values = new Integer[numHits];\n                this.fieldName = fieldname;\n            }\n\n            @Override\n            public int compare(int slot1, int slot2) {\n                if (values[slot1] &gt; values[slot2]) return -1;\n                if (values[slot1] &lt; values[slot2]) return 1;\n                return 0;\n            }\n\n            @Override\n            public void setBottom(int slot) {\n                bottom = values[slot];\n            }\n\n            @Override\n            public int compareBottom(int doc) throws IOException {\n                int value = populations[doc];\n                if (bottom &gt; value) return -1;\n                if (bottom &lt; value) return 1;\n                return 0;\n            }\n\n            @Override\n            public void copy(int slot, int doc) throws IOException {\n                values[slot] = populations[doc];\n            }\n\n            @Override\n            public void setNextReader(IndexReader reader, int docBase) throws IOException {\n                /* XXX uses field cache */\n                populations = FieldCache.DEFAULT.getInts(reader, "population");\n            }\n\n            @Override\n            public Comparable value(int slot) {\n                return values[slot];\n            }\n        }\n    }\n\n    @Before\n    public void setUp() throws Exception {\n        Directory dir = new RAMDirectory();\n        analyzer = createAnalyzer(3);\n\n        IndexWriter writer = new IndexWriter(dir, analyzer, IndexWriter.MaxFieldLength.UNLIMITED);\n        ImmutableList&lt;String&gt; cities = ImmutableList.of("Bosc de Planavilla", "Planavilla", "Bosc de la Planassa",\n                                                               "Bosc de Plana en Blanca");\n        ImmutableList&lt;Integer&gt; populations = ImmutableList.of(5000, 20000, 1000, 100000);\n\n        for (int id = 0; id &lt; cities.size(); id++) {\n            Document doc = new Document();\n            doc.add(new Field("id", String.valueOf(id), Field.Store.YES, Field.Index.NOT_ANALYZED));\n            doc.add(new Field("city", cities.get(id), Field.Store.YES, Field.Index.ANALYZED));\n            doc.add(new Field("population", String.valueOf(populations.get(id)),\n                                     Field.Store.YES, Field.Index.NOT_ANALYZED));\n            writer.addDocument(doc);\n        }\n        writer.close();\n\n        qp = new QueryParser(Version.LUCENE_30, "city", createAnalyzer(0));\n        sort = new Sort(new SortField("population", new PopulationComparatorSource()));\n        searcher = new IndexSearcher(dir);\n        searcher.setDefaultFieldSortScoring(true, true);\n        reader = searcher.getIndexReader();\n    }\n\n    @After\n    public void tearDown() throws Exception {\n        searcher.close();\n    }\n\n    @Test\n    public void testShingleFilter() throws Exception {\n        System.out.println("shingle filter");\n\n        printSearch("city:\"Bosc de Planavilla\"");\n        printSearch("city:Planavilla");\n        printSearch("city:Bosc");\n    }\n\n    private void printSearch(String query) throws ParseException, IOException {\n        Query q = qp.parse(query);\n        System.out.println("query " + q);\n        TopDocs hits = searcher.search(q, null, 4, sort);\n        System.out.println("results " + hits.totalHits);\n        int i = 1;\n        for (ScoreDoc dc : hits.scoreDocs) {\n            Document doc = reader.document(dc.doc);\n            System.out.println(i++ + ". " + dc + " \"" + doc.get("city") + "\" population: " + doc.get("population"));\n        }\n        System.out.println();\n    }\n}\n
635	    //Get everything that matches #4\n    ?q=Genre:'Adventure' Director:'Chris Columnbus' Actors:('Daniel Radcliffe' 'Rupert Grint' 'Emma Watson')\n\n    //use dismax\n    &amp;defType=dismax\n\n    //boost some fields with a "query filter"\n    //this will make a match on director worth the most\n    //each actor will be worth a little bit less, but 2+ actors will be more\n    //all matches will be added together to create a score similar to your example\n    &amp;qf=Director^2.0+Actor^1.5+Genre^1.0\n\n    //Make sure you can see the score for debugging\n    &amp;fl=*,score\n
636	// Parse a query by the user.\nQueryParser qp = new QueryParser(Version.LUCENE_35, "text", new StandardAnalyzer());\nQuery standardQuery = qp.parse("User query may go here");\n\n// Make a query that matches everything, but has no boost.\nMatchAllDocsQuery matchAllDocsQuery = new MatchAllDocsQuery();\nmatchAllDocsQuery.setBoost(0f);\n\n// Combine the queries.\nBooleanQuery boolQuery = new BooleanQuery();\nboolQuery.add(standardQuery, BooleanClause.Occur.SHOULD);\nboolQuery.add(matchAllDocsQuery, BooleanClause.Occur.SHOULD);\n\n// Now just pass it to the searcher.\n
637	&lt;fieldType name="tdouble" class="solr.TrieDoubleField" precisionStep="8" omitNorms="true" positionIncrementGap="0"/&gt;\n&lt;fieldType name="latLon" class="solr.LatLonType" subFieldSuffix="_latLon"/&gt;\n&lt;field name="lat_lon" type="latLon" indexed="true" stored="true"/&gt;\n&lt;dynamicField name="*_latLon" type="tdouble" indexed="true" stored="false" multiValued="true"/&gt;\n
638	class AlphaNumberBoundaryFilter(lucene.PythonTokenFilter):\n    seq = re.compile(r"((?:\d+")|(?:\D+))")\n\n    def __init__(self, in_stream):\n        lucene.PythonTokenFilter.__init__(self, in_stream)\n        term = self.term = self.addAttribute(lucene.TermAttribute.class_)\n        # Get tokens.\n        tokens = []\n        while in_stream.incrementToken():\n            tokens.append(term.term())\n        # Filter tokens.\n        self.tokens = self.filter(tokens)\n        # Setup iterator.\n        self.iter = iter(self.tokens)\n\n    def filter(self, tokens):\n        seq = self.seq\n        return [split for token in tokens for split in seq.findall(token)]\n\n    def incrementToken(self):\n        try:\n            self.term.setTermBuffer(next(self.iter))\n        except StopIteration:\n            return False\n        return True\n\n\nclass NumberToWordFilter(lucene.PythonTokenFilter):\n    num_map = {0: "zero", 1: "one", 2: "two", 3: "three", 4: "four", 5: "five", 6: "six", 7: "seven", 8: "eight", 9: "nine", 10: "ten", 11: "eleven", 12: "twelve", 13: "thirteen", 14: "fourteen", 15: "fifteen", 16: "sixteen", 17: "seventeen", 18: "eighteen", 19: "nineteen", 20: "twenty", 30: "thirty", 40: "forty", 50: "fifty", 60: "sixty", 70: "seventy", 80: "eighty", 90: "ninety", 100: "hundred", 1000: "thousand", 1000000: "million"}\n    is_num = re.compile(r"^\d+$")\n\n    def __init__(self, in_stream):\n        lucene.PythonTokenFilter.__init__(self, in_stream)\n        term = self.term = self.addAttribute(lucene.TermAttribute.class_)\n        # Get tokens.\n        tokens = []\n        while in_stream.incrementToken():\n            tokens.append(term.term())\n        # Filter tokens.\n        self.tokens = self.filter(tokens)\n        # Setup iterator.\n        self.iter = iter(self.tokens)\n\n    def filter(self, tokens):\n        num_map = self.num_map\n        is_num = self.is_num\n        final = []\n        for token in tokens:\n            if not is_num.match(token):\n                final.append(token)\n                continue\n            # Reverse digits from token.\n            digits = token.lstrip('0')[::-1]\n            if not digits:\n                # We have a zero.\n                final.append(num_map[0])\n                continue\n            # Group every 3 digits and iterate over digit groups in reverse\n            # so that groups are yielded in the original order and in each\n            # group: 0 -&gt; ones, 1 -&gt; tens, 2 -&gt; hundreds\n            groups = [digits[i:i+3] for i in xrange(0, len(digits), 3)][::-1]\n            scale = len(groups) - 1\n            result = []\n            for oth in groups:\n                l = len(oth)\n                if l == 3 and oth[2] != '0':\n                    # 2 -&gt; x\n                    # 1 -&gt; .\n                    # 0 -&gt; .\n                    result.append(num_map[int(oth[2])])\n                    result.append(num_map[100])\n                if l &gt;= 2:\n                    if oth[1] == '1':\n                        # 1 -&gt; 1\n                        # 0 -&gt; x\n                        result.append(num_map[int(oth[1::-1])])\n                    else:\n                        if oth[1] != '0':\n                            # 1 -&gt; x (x &gt;= 2)\n                            # 0 -&gt; x\n                            result.append(num_map[int(oth[1]) * 10])\n                        if oth[0] != '0':\n                            result.append(num_map[int(oth[0])])\n                elif oth[0] != '0':\n                    # 0 -&gt; x\n                    result.append(num_map[int(oth[0])])\n                # Add scale modifier.\n                s = scale\n                if s % 2:\n                    result.append(num_map[1000])\n                while s &gt;= 2:\n                    result.append(num_map[1000000])\n                    s -= 2\n                scale -= 1\n            final.extend(result)\n        return final \n\n\n    def incrementToken(self):\n        try:\n            self.term.setTermBuffer(next(self.iter))\n        except StopIteration:\n            return False\n        return True\n
639	templateIds = IdHelper.NormalizeGuid(templateIds);\n
640	carrot.title=my_title&amp;carrot.snippet=my_title,my_description\n
641	sim(query, doc) = sum(t in terms(query), freq(t, query) * w(t, doc))\n
642	http://localhost:8983/solr/core0/select/?q=pasticceria\n
643	TopScoreDocCollector collector = TopScoreDocCollector.create(10, true);\nsearcher.search(query, collector);\nTopDocs topDocs = collector.topDocs();\nint numResults = collector.getTotalHits();\n
644	# Delete old version to make sure new settings are applied\ncurl -XDELETE "localhost:9200/dates-test/"\necho\n# Create a new index with proper mapping \n# See http://www.elasticsearch.org/guide/reference/index-modules/analysis/pathhierarchy-tokenizer.html\ncurl -XPUT "localhost:9200/dates-test" -d '{\n    "mappings": {\n        "doc": {\n            "properties": {\n                "name": {"type": "string"},\n                "object-available": {\n                    "type": "nested",\n                    "properties" : {\n                        "end" : {\n                            "type" : "date"\n                        },\n                        "start" : {\n                            "type" : "date"\n                        }\n                    }\n                }\n            }\n        }\n    }\n}'\necho\n# Put some test data\ncurl -XPUT "localhost:9200/dates-test/doc/1" -d '{\n    "name": "Record 1",\n    "object-available":[\n        {"start":"2012-01-01", "end":"2012-02-03"},\n        {"start":"2012-05-05", "end":"2012-12-31"}\n    ]\n}\n'\ncurl -XPUT "localhost:9200/dates-test/doc/2" -d '{\n    "name": "Record 2",\n    "object-available":[\n        {"start":"2012-02-01", "end":"2012-04-20"},\n        {"start":"2012-04-25", "end":"2012-11-30"}\n    ]\n}\n'\ncurl -XPOST "localhost:9200/dates-test/_refresh"\necho\necho Test for the range 2011-12-01 - 2012-02-05. Should find only 1st record \ncurl -XPOST "localhost:9200/dates-test/doc/_search?pretty=true" -d '{\n    "query": {\n        "nested": {\n            "path": "object-available",\n            "query": {\n                "bool": {\n                    "must": [\n                        {\n                            "range": {\n                                "start": {\n                                    "from": "2011-12-01",\n                                    "to": "2012-02-05"\n                                }\n                            }\n                        },\n                        {\n                            "range": {\n                                "end": {\n                                    "from": "2011-12-01",\n                                    "to": "2012-02-05"\n                                }\n                            }\n                        }\n\n                    ]\n                }\n            }\n        }\n    }\n}'\necho\necho Test for the range 2012-01-20 - 2012-12-01. Should find only 2nd record \ncurl -XPOST "localhost:9200/dates-test/doc/_search?pretty=true" -d '{\n    "query": {\n        "nested": {\n            "path": "object-available",\n            "query": {\n                "bool": {\n                    "must": [\n                        {\n                            "range": {\n                                "start": {\n                                    "from": "2012-01-20",\n                                    "to": "2012-12-01"\n                                }\n                            }\n                        },\n                        {\n                            "range": {\n                                "end": {\n                                    "from": "2012-01-20",\n                                    "to": "2012-12-01"\n                                }\n                            }\n                        }\n\n                    ]\n                }\n            }\n        }\n    }\n}'\necho\necho Test for the range 2012-04-01 - 2013-01-01. Should find both record \ncurl -XPOST "localhost:9200/dates-test/doc/_search?pretty=true" -d '{\n    "query": {\n        "nested": {\n            "path": "object-available",\n            "query": {\n                "bool": {\n                    "must": [\n                        {\n                            "range": {\n                                "start": {\n                                    "from": "2012-04-01",\n                                    "to": "2013-01-01"\n                                }\n                            }\n                        },\n                        {\n                            "range": {\n                                "end": {\n                                    "from": "2012-04-01",\n                                    "to": "2013-01-01"\n                                }\n                            }\n                        }\n\n                    ]\n                }\n            }\n        }\n    }\n}'\n
645	Counter clock = Counter.newCounter(true);\nTimeLimitingCollector collector = new TimeLimitingCollector(c, clock, 10);\ncollector.setBaseline(0);  \nnew Thread() {\n   public void run() {\n      clock.addAndGet(1);  // will kill the indexSearcher.search(...) after 10 ticks (10 seconds)\n      Thread.sleep(1000);  // try-catch is necessary here, yes\n   }\n}.start();\nindexSearcher.search(query, collector);\n
646	Directory dir      = FSDirectory.open( new File("index" ));\nDirectory dir_taxo = FSDirectory.open( new File("index-taxo" ));\nIndexWriter writer = newIndexWriter(dir);\nTaxonomyWriter taxo = new DirectoryTaxonomyWriter(dir_taxo, OpenMode.CREATE);\nFacetFields ff= new FacetFields(taxo);\n\n//for all documents:\nd=new Document();\nList&lt;CategoryPath&gt;=new ArrayList&lt;CategoryPath&gt;();    \n\nfor (all fields in doc)\n{\n    d.addField( ....)\n}\nfor (all categories in doc)\n{ \n    CategoryPath cp = new CategoryPath(field, value);\n    categories.add( cp);\n    taxo.addCategory(cp); //not sure if necessary\n}\n\nff.addFields(d, categories);\nw.addDocument( d );\n
647	 class MyAnalyzer extends Analyzer {\n  @Override\n  protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n    Tokenizer source = new LowerCaseTokenizer(version, reader);\n    return new TokenStreamComponents(source, new PorterStemFilter(source));\n  }\n}\n
648	from doc in docs\n     select new\n     {\n         ...\n         Token_Length = doc.TokenLength\n     }\n
649	class CodeValue{\n    static searchable ={\n        only:['value', 'description']\n        value boost: 2.0\n    }\n    String value\n    String description\n    static belongsTo = [codeset: CodeSet]\n}\nclass CodeSet{\n    static searchable ={\n        only:['name', 'description']\n        name boost: 2.0\n    }\n\n    String name\n    String description\n    static hasMany = [codeValues:CodeValue]\n}\n
650	  Map&lt;String,String&gt; args = new HashMap&lt;String, String&gt;();\n  CustomFilterFactory factory = new CustomFilterFactory(args);\n
651	Query qx2 = NumericRangeQuery.newIntRange("x2", 1, 1, true, true);\nQuery matchAll = new MatchAllDocsQuery();\nQuery qnotx2 = new BooleanQuery();\nquery.add(matchAll, Occur.MUST);\nquery.add(qx2, Occur.MUST_NOT);\nQuery qx1 = NumericRangeQuery.newIntRange("x1", 1, 1, true, true);\nQuery query = new BooleanQuery();\nquery.add(qx1, Occur.MUST);\nquery.add(qnotx2, Occur.MUST);\nTopDocs topDocs = searcher.search(query, null, 1); \n
652	Map&lt;String,Analyzer&gt; analyzerList = new HashMap&lt;String,Analyzer&gt;();\nanalyzerList.put("stemmedText", new EnglishAnalyzer(Version.LUCENE_44));\nanalyzerList.put("unstemmedText", new StandardAnalyzer(Version.LUCENE_44));\nPerFieldAnalyzerWrapper analyzer = new PerFieldAnalyzerWrapper(new StandardAnalyzer(Version.LUCENE_44), analyzerList);\n
653	org.apache.lucene.search.Query allQuery = new org.apache.lucene.search.MatchAllDocsQuery();\nfullTextQuery = fullTextSession.createFullTextQuery(allQuery, this.type); \nresults = tab.getQueryFiltersForSearch(fullTextQuery).setSort(sort).list();\n
654	String dateOneString = DateTools.dateToString(dateOne, DateTools.Resolution.MILLISECOND);\nString dateTwoString = DateTools.dateToString(dateTwo, DateTools.Resolution.MILLISECOND);\nTermRangeQuery dateQuery = new TermRangeQuery("lastUpdateDate", dateTwoString,  dateOneString, true, true);\n
655	IndexReader reader = IndexReader.Open(indexDirectoryPath);\n
656	Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_CURRENT);\n\n// Store the index in memory:\n//Directory directory = new RAMDirectory();\n\nStore an index on disk\nDirectory directory = FSDirectory.open(indexfilesDirPathOnYourServer);\nIndexWriterConfig config = new IndexWriterConfig(Version.LUCENE_CURRENT, analyzer);\nIndexWriter iwriter = new IndexWriter(directory, config);\nDocument doc = new Document();\nString title = getTitle();\ndoc.add(new Field("fieldname", text, TextField.TYPE_STORED));\niwriter.addDocument(doc);\niwriter.close();\n
657	#!/bin/bash\n\nexport ELASTICSEARCH_ENDPOINT="http://localhost:9200"\n\n# Create indexes\n\ncurl -XPUT "$ELASTICSEARCH_ENDPOINT/play" -d '{\n    "mappings": {\n        "type": {\n            "properties": {\n                "kol_tags": {\n                    "properties": {\n                        "scored": {\n                            "type": "nested",\n                            "properties": {\n                                "name": {\n                                    "type": "string",\n                                    "index": "not_analyzed"\n                                }\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}'\n\n# Index documents\ncurl -XPOST "$ELASTICSEARCH_ENDPOINT/_bulk?refresh=true" -d '\n{"index":{"_index":"play","_type":"type"}}\n{"kol_tags":{"scored":[{"name":"Core Grower","score":36},{"name":"Connectivity","score":42}]}}\n{"index":{"_index":"play","_type":"type"}}\n{"kol_tags":{"scored":[{"name":"Connectivity","score":34},{"name":"Connectivity","score":42}]}}\n{"index":{"_index":"play","_type":"type"}}\n{"kol_tags":{"scored":[{"name":"Core Grower","score":36}]}}\n{"index":{"_index":"play","_type":"type"}}\n{"kol_tags":{"scored":[{"name":"Connectivity","score":36}]}}\n'\n\n# Do searches\n\ncurl -XPOST "$ELASTICSEARCH_ENDPOINT/_search?pretty" -d '\n{\n    "query": {\n        "filtered": {\n            "filter": {\n                "bool": {\n                    "should": [\n                        {\n                            "nested": {\n                                "path": "kol_tags.scored",\n                                "filter": {\n                                    "bool": {\n                                        "must": [\n                                            {\n                                                "term": {\n                                                    "name": "Core Grower"\n                                                }\n                                            },\n                                            {\n                                                "range": {\n                                                    "score": {\n                                                        "gte": 1,\n                                                        "lte": 100\n                                                    }\n                                                }\n                                            }\n                                        ]\n                                    }\n                                }\n                            }\n                        },\n                        {\n                            "nested": {\n                                "path": "kol_tags.scored",\n                                "filter": {\n                                    "bool": {\n                                        "must": [\n                                            {\n                                                "term": {\n                                                    "name": "Connectivity"\n                                                }\n                                            },\n                                            {\n                                                "range": {\n                                                    "score": {\n                                                        "gte": 35,\n                                                        "lte": 65\n                                                    }\n                                                }\n                                            }\n                                        ]\n                                    }\n                                }\n                            }\n                        }\n                    ],\n                    "must_not": [\n                        {\n                            "nested": {\n                                "path": "kol_tags.scored",\n                                "filter": {\n                                    "bool": {\n                                        "must": [\n                                            {\n                                                "term": {\n                                                    "name": "Connectivity"\n                                                }\n                                            },\n                                            {\n                                                "not": {\n                                                    "range": {\n                                                        "score": {\n                                                            "gte": 35,\n                                                            "lte": 65\n                                                        }\n                                                    }\n                                                }\n                                            }\n                                        ]\n                                    }\n                                }\n                            }\n                        }\n                    ]\n                }\n            }\n        }\n    }\n}\n'\n\ncurl -XPOST "$ELASTICSEARCH_ENDPOINT/_search?pretty" -d '\n{\n    "filter": {\n        "nested": {\n            "path": "kol_tags.scored",\n            "filter": {\n                "or": [\n                    {\n                        "and": [\n                            {\n                                "terms": {\n                                    "kol_tags.scored.name": [\n                                        "Core Grower"\n                                    ]\n                                }\n                            },\n                            {\n                                "range": {\n                                    "kol_tags.scored.score": {\n                                        "gte": 1,\n                                        "lte": 100\n                                    }\n                                }\n                            }\n                        ]\n                    },\n                    {\n                        "and": [\n                            {\n                                "terms": {\n                                    "kol_tags.scored.name": [\n                                        "Connectivity"\n                                    ]\n                                }\n                            },\n                            {\n                                "range": {\n                                    "kol_tags.scored.score": {\n                                        "gte": 35,\n                                        "lte": 65\n                                    }\n                                }\n                            }\n                        ]\n                    }\n                ]\n            }\n        }\n    }\n}\n'\n
658	var Restaurant = from u in objCtx.Restaurants\n                       select u;\n\nif (!String.IsNullOrEmpty(SearchString))\n{\n    Restaurant = Restaurant.Where(s =&gt; s.Name.Contains(SearchString));\n}\n\n// Second table.\nvar Restaurant2 = from u in objCtx.Restaurants2\n                       select u;\n\n\nif (!String.IsNullOrEmpty(SearchString))\n{\n    Restaurant2 = Restaurant2.Where(s =&gt; s.Name.Contains(SearchString));\n}\n\nreturn View(Restaurant.Union(Restaurant2).ToList());\n
659	MATCH (item:Item)\nWHERE (item.title = "foo bar" AND item.body =~ "ba.*") OR item.title = "bat"\nRETURN item\n
660	&lt;cache-container\n    name="hibernateSearch"\n    default-cache="LuceneIndexesData"\n    jndi-name="java:jboss/infinispan/hibernateSearch"\n    start="EAGER"&gt;\n\n    &lt;transport lock-timeout="60000" /&gt;\n\n    &lt;replicated-cache name="LuceneIndexesMetadata"\n        mode="SYNC"\n        start="EAGER"&gt;\n        &lt;binary-keyed-jdbc-store\n            datasource="java:jboss/datasources/PicketLinkDatasource"\n            shared="true"\n            passivation="false"\n            purge="false"&gt;\n            &lt;property name="createTableOnStart"&gt;true&lt;/property&gt;\n            &lt;binary-keyed-table&gt;\n                &lt;id-column name="ID_COLUMN" type="VARCHAR(255)" /&gt;\n                &lt;data-column name="DATUM" type="BLOB" /&gt;\n            &lt;/binary-keyed-table&gt;\n        &lt;/binary-keyed-jdbc-store&gt;\n    &lt;/replicated-cache&gt;\n\n    &lt;replicated-cache name="LuceneIndexesData"\n        mode="SYNC"\n        start="EAGER"&gt;\n        &lt;binary-keyed-jdbc-store\n            datasource="java:jboss/datasources/PicketLinkDatasource"\n            shared="true"\n            passivation="false"\n            purge="false"&gt;\n            &lt;property name="createTableOnStart"&gt;true&lt;/property&gt;\n            &lt;binary-keyed-table&gt;\n                &lt;id-column name="ID_COLUMN" type="VARCHAR(255)" /&gt;\n                &lt;data-column name="DATUM" type="BLOB" /&gt;\n            &lt;/binary-keyed-table&gt;\n        &lt;/binary-keyed-jdbc-store&gt;\n    &lt;/replicated-cache&gt;\n\n    &lt;replicated-cache name="LuceneIndexesLocking"\n        mode="SYNC"\n        start="EAGER" /&gt;\n\n&lt;/cache-container&gt;\n
661	private static class ModelAnalyzer extends Analyzer\n{\n    @Override\n    protected TokenStreamComponents createComponents(String string, Reader reader)\n    {\n        Tokenizer tokenizer = new NGramTokenizer(Version.LUCENE_4_9, reader, 3, 20)\n        {\n            @Override\n            protected boolean isTokenChar(int c)\n            {\n                return Character.isLetterOrDigit(c);\n            }\n        };\n        return new TokenStreamComponents(tokenizer,\n            new LowerCaseFilter(Version.LUCENE_4_9, tokenizer));\n    }\n}\n
662	parser.setDefaultOperator(Operator.AND);\nif(searchWord.contains(" ")){\n    searchWord= searchWordreplace(" ", "?");\n}\n
663	String indexableDateString = DateTools.dateToString(theDate, DateTools.Resolution.MINUTE);\ndoc.add(new StringField("importantDate", indexableDateString, Field.Store.YES));\n...\nTopDocs results = indexSearcher.search(new TermRangeQuery(\n    "importantDate",\n    new BytesRef(DateTools.dateToString(lowDate, DateTools.Resolution.MINUTE)),\n    new BytesRef(DateTools.dateToString(highDate, DateTools.Resolution.MINUTE)),\n    true,\n    false\n));\n...\nField dateField = resultDocument.getField("importantDate")\nDate retrievedDate = DateTools.stringToDate(dateField.stringValue());\n
664	POST test/scores/_search\n{\n  "sort":{\n    "_script":{\n      "lang":"groovy",\n      "script" : "doc['priority1'].value == 0 ? doc['priority2'].value : doc['priority1'].value",\n      "type" : "number",\n      "order" : "asc"\n    }\n  }\n}\n
665	User.where(name: /?ob/)\n# or\nUser.as(:u).where("u.name =~ '?ob`")\n
666	Explanation explain = indexSearcher.Explain(parsedQuery, hits.ScoreDocs[0].Doc);\n
667	POST /test_index/_search?search_type=count\n{\n   "aggs": {\n      "males_18_and_over": {\n         "filter": {\n            "and": [\n               { "term": { "Gender": "M" } },\n               { "range": { "Age": { "gte": 18 } } } \n            ]\n         },\n         "aggs": {\n            "last_names": {\n               "terms": {\n                  "field": "LastName"\n               },\n               "aggs": {\n                  "max_age": {\n                     "top_hits": {\n                        "sort": [\n                           {\n                              "Age": {\n                                 "order": "desc"\n                              }\n                           }\n                        ],\n                        "size": 1\n                     }\n                  }\n               }\n            }\n         }\n      }\n   }\n}\n
668	  StringReader stringReader = new StringReader("abcd");\n  NGramTokenizer tokenizer = new NGramTokenizer(1, 2);\n  tokenizer.setReader(stringReader);\n  tokenizer.reset();\n  CharTermAttribute termAtt = tokenizer.getAttribute(CharTermAttribute.class);\n  while (tokenizer.incrementToken()) {\n    String token = termAtt.toString();\n    System.out.println(token);\n  }\n
669	MATCH (author:Author )-[:WROTE]-&gt;(article:Article) \nWHERE article.id =~ "(?i)Art1052668.*" \nRETURN author, article.date\n
670	public class MyDrillSideways extends DrillSideways {\n\n  public MyDrillSideways(IndexSearcher searcher, FacetsConfig config, TaxonomyReader taxoReader) {\n    super(searcher, config, taxoReader);\n  }\n\n  @Override\n  protected Facets buildFacetsResult(FacetsCollector drillDowns, FacetsCollector[] drillSideways, String[] drillSidewaysDims) throws IOException {\n\n    String longRangeFacetDim = "mySpecialLongRangeDim";\n    Facets drillDownFacets = new FastTaxonomyFacetCounts(taxoReader, config, drillDowns);\n\n    boolean foundLongRangeInDrillSideways = false;\n    Map&lt;String, Facets&gt; drillSidewaysFacets = new HashMap&lt;&gt;();\n    if (drillSideways != null) {\n      for (int i = 0; i &lt; drillSideways.length; i++) {\n        String sidewaysDim = drillSidewaysDims[i];\n        FacetsCollector drillSideway = drillSideways[i];\n\n        Facets sidewaysFacets;\n        if (sidewaysDim.equals(longRangeFacetDim)) {\n          foundLongRangeInDrillSideways = true;\n          sidewaysFacets = new LongRangeFacetCounts(...,drillSideway,...);\n        } else {\n          sidewaysFacets = new FastTaxonomyFacetCounts(taxoReader, config, drillSideway);\n        }\n        drillSidewaysFacets.put(sidewaysDim, sidewaysFacets);\n      }\n    }\n\n    if (!foundLongRangeInDrillSideways) {\n      Facets facetsTime = new LongRangeFacetCounts(..., FacetsCollector, ...);\n      drillSidewaysFacets.put(longRangeFacetDim, facetsTime);\n    }\n\n    return new MultiFacets(drillSidewaysFacets, drillDownFacets);\n  }\n}\n
671	http://localhost:8983/solr/simple/select?q=test~1&amp;defType=edismax&amp;qf=fullText\n
672	&lt;dataConfig&gt;\n  &lt;dataSource type="JdbcDataSource" /&gt;\n  &lt;document&gt;\n    &lt;entity name="PARENT" query="select * from PARENT"&gt;\n      &lt;field column="id" /&gt;\n      &lt;field column="desc" /&gt;\n      &lt;field column="type_s" /&gt;\n      &lt;entity child="true" name="CHILD" query="select * from CHILD where parent_id='${PARENT.id}'"&gt;\n        &lt;field column="id" /&gt;\n        &lt;field column="desc" /&gt;\n        &lt;field column="type_s" /&gt;\n      &lt;/entity&gt;\n    &lt;/entity&gt;\n  &lt;/document&gt;\n&lt;/dataConfig&gt;\n
673	if (!str.StartsWith("{") &amp;&amp; !str.EndsWith("}"))\n    id = String.Format("{{{0}}}", str);\n
674	String solrDir = "C:/Program Files/Apache Software Foundation/Tomcat 7.0/webapps/solr/new_core/";\n\n//this solr Directory is home and specified core.\n\n//solrParams.set("qt", "/dataimport");\n//solrParams.set("command", "full-import");\n\n//above two line for importing data one time.\n\nCoreContainer container = new CoreContainer(solrDir);\ncontainer.load();\nEmbeddedSolrServer server = new EmbeddedSolrServer(container, "collection1");\n\nModifiableSolrParams solrParams = new ModifiableSolrParams();\nsolrParams.add(CommonParams.Q, "*:*");\n\nQueryResponse queryResponse = server.query(solrParams);\nfor (SolrDocument document : queryResponse.getResults()) {\n  System.out.println(document);\n}\n
675	Neo4jGraph neo4j = (Neo4jGraph) neo4j;\n
676	MoreLikeThis mlt = new MoreLikeThis(indexreader);\nQuery query = mlt.like(someReader, "contents");\nHits hits = indexsearcher.search(query);\n
677	    @Override\n    public String objectToString(Object value) {\n        if(value != null) {\n            String templateType = null;  \n            if(value instanceof Set) {\n                StringBuilder strBuild = new StringBuilder();\n                Set&lt;TemplateAttribute&gt; setOfTempAttr = (Set&lt;TemplateAttribute&gt;) value;\n                for(TemplateAttribute tempValue : setOfTempAttr) {\n                    int productID = tempValue.getProduct().getProductId();\n                    Set&lt;TemplateAttributeValue&gt; setOfTempAttrValues = tempValue.getTemplateAttrValues();\n\n                    for (TemplateAttributeValue taValue : setOfTempAttrValues) {\n                        int entityID = taValue.getId().getEntityId();\n                        if(entityID == productID) {\n                            strBuild.append(" " + taValue.getAttrValue());\n                        }\n\n                    }\n                }\n                return strBuild.toString();\n            } else if(value instanceof String) {\n                templateType = (String) value;\n            }\n            return templateType;\n        }\n        return null;\n}\n
678	&lt;property name="hibernateProperties"&gt;\n    &lt;props&gt;\n        ...\n        &lt;prop key="hibernate.search.default.indexBase"&gt;/var/lucene/indexes&lt;/prop&gt;\n        &lt;prop key="hibernate.search.default.directory_provider"&gt;filesystem&lt;/prop&gt;\n        &lt;prop key="hibernate.search.lucene_version"&gt;LUCENE_CURRENT&lt;/prop&gt;\n    &lt;/props&gt;\n&lt;/property&gt;\n
679	Sort sort = new Sort(\n   new SortField(\n      null,                    // or specify a field\n      SortField.Type.DOC, \n      true\n   )\n);\n
680	Terms termFreqVector = indexReader.getTermVector(doc, field);\nTermsEnum te = termFreqVector.iterator(null);\n
681	    private void button1_Click(object sender, EventArgs e)\n    {\n        //Step 1 parse your term to a tree\n        var pt = ParseTerm.Parse(textBox1.Text, null);\n        //Step 2 find the term we care about\n        pt = FindTermNode(pt, "\"better\"");\n        //Step 3 Walk the tree backwards and make the term\n        var res = CreateANDTerm(pt);\n        textBox2.Text = res;\n    }\n\n    enum direction\n    { left, right}\n\n    private string CreateANDTerm(ParseTerm inPt)\n    {  \n        direction fromDirection;\n        var originPt = inPt;\n        StringBuilder retStr = new StringBuilder();\n        retStr.Append("("+originPt.myexpression+")");\n        var currentPt = inPt;\n        var nextPt = currentPt.parent;\n\n        do\n        {\n            if (nextPt.left == currentPt)\n            {\n                fromDirection = direction.left;\n            }\n            else\n            {\n                fromDirection = direction.right;\n            }\n\n            if (nextPt.relation == ParseTerm.RelationType.RT_AND)\n            {\n                if (fromDirection == direction.left)\n                {\n                    retStr.Append(" AND (" + nextPt.right.myexpression + ")");\n                }\n                else\n                {\n                    retStr.Append(" AND (" + nextPt.left.myexpression + ")");\n                }\n            }\n            currentPt = nextPt;\n            nextPt = currentPt.parent;\n        } while (nextPt != null);\n        return retStr.ToString();\n    }\n\n    private ParseTerm FindTermNode(ParseTerm pt,string inStr)\n    {\n        if (pt.relation == ParseTerm.RelationType.RT_TERM &amp;&amp; pt.searchstring == inStr)\n        {\n            return pt;\n        }\n        else\n        {\n            if(pt.left != null &amp;&amp; pt.left.myexpression.Contains(inStr))\n            {\n                var retPT = FindTermNode(pt.left, inStr);\n                if (retPT != null)\n                    return retPT;                  \n            }\n            if (pt.right != null &amp;&amp; pt.right.myexpression.Contains(inStr))\n            {\n                var retPT = FindTermNode(pt.right, inStr);\n                if (retPT != null)\n                    return retPT;                   \n            }\n            return null;\n        }\n    }\n
682	fl=*,firstmatch:if(and(termfreq(field1,'ipod'),termfreq(field2,'black')),1,0),\nsecondmatch:if(termfreq(field3,'apple computer')),1,0),\nthirdmatch:if(termfreq(field4,'working'),1,0)\n
683	&lt;filter class="EdgeNGramFilterFactory" minGramSize="2" maxGramSize="15" side="back" /&gt;\n
684	for (GroupDocs&lt;BytesRef&gt; group : d) {\n    for (ScoreDoc scoredoc : group.scoreDocs) {\n        Document doc = is.doc(scoredoc.doc);\n        //Do stuff\n    }\n}\n
685	final FieldType type = new FieldType();\n            type.setNumericPrecisionStep(4);\n            type.setStored(true);\n            type.setIndexed(true);\n            type.setNumericType(FieldType.NumericType.LONG);\n            doc.add(new LongField("BirthDate", parseDate("1969/01/31 16:17:18").getTime(), type));\n
686	#  guarantee(result == EXCEPTION_CONTINUE_EXECUTION) failed: Unexpected result from topLevelExceptionFilter\n
687	Field field = new StoredField("myfield", "content");\n
688	    Directory directory = null;\n    IndexWriter writer = null;\n    try {\n        directory = FSDirectory.open(new File(args[0]));\n        IndexWriterConfig iwConf = new IndexWriterConfig(Version.LUCENE_48,\n                new KeywordAnalyzer());\n        iwConf.setOpenMode(IndexWriterConfig.OpenMode.CREATE);\n        writer = new IndexWriter(directory, iwConf);\n        Document document = new Document();\n        document.add(new StringField("title", "Cat", Store.YES));\n        writer.addDocument(document);\n        writer.commit();\n        writer.close();\n    } catch (IOException e) {\n        e.printStackTrace();\n    }\n
689	public static void read() throws IOException {\n    System.out.println("Etape1");\n    File indexDirectory = new File("D:/index/DEV_IdxDOSSIER/data/index");\n    IndexReader r = IndexReader.open(FSDirectory.open(indexDirectory));\n    System.out.println("Etape2");\n    int num = r.numDocs();\n    int nbrUA = 0 ;\n    for (int i = 0; i &lt; num; i++) {\n        Document d = r.document(i);\n        System.out.println("DC_KEY: " + d.get("DC_KEY"));\n        try {\n            FileWriter fw = new FileWriter("D:\\index\\test.txt", true);\n            BufferedWriter output = new BufferedWriter(fw);\n            if (d.get("DC_KEY") != null) {\n                output.write(d.get("DC_KEY") + "\r\n");\n                System.out.println("fichier mis à jour");\n            } else {\n                System.out.println("Le DC_KEY est null c'est une Unité d'Archive");\n                nbrUA++;\n            }\n            output.flush();\n            output.close();\n\n        } catch (IOException ioe) {\n            System.out.println("the fonction write didn't Work, here is the Error");\n            ioe.printStackTrace();\n        } catch (NullPointerException ioe) {\n            System.out.println("Erreur : pointeur null");\n            System.out.println("the fonction write didn't Work, here is the Error");\n            ioe.printStackTrace();\n        }\n        System.out.println("nombre de document traité : " + (i + 1) + "\r\n");\n    }\n    r.close();\n    System.out.println("nombre d'Unité d'Archive : " + nbrUA + "\r\n");\n}\n
690	JARS+=$(ANALYZERS_JAR)          # many language analyzers \n
691	 {"query"=&gt;\n          {"filtered"=&gt;\n            {"query"=&gt;{{"match_all"=&gt;{}},\n             "filter"=&gt;{\n               "bool"=&gt;{\n                 "must"=&gt;[\n                    {"term"=&gt;{"domain_id"=&gt;7721}}, \n                    {"range"=&gt;{"goals"=&gt;{"gte"=&gt;0}}}\n                    "nested"=&gt;{"path"=&gt;"by_traffic_source", \n                        "filter"=&gt;{\n                          {"term"=&gt;{"by_traffic_source.source"=&gt;"organic"}}\n                        }\n                      }\n                    }\n                  ]},\n\n               }\n              }\n            },\n         "aggs"=&gt;{"crawl_pages"=&gt;{"terms"=&gt;{"field"=&gt;"crawl_page_id", "size"=&gt;200}}}}\n
692	SrndQuery query = QueryParser.parse(queryString);\nBasicQueryFactory factory = new BasicQueryFactory(1000 /*maxBasicQueries*/);\nQuery luceneQuery = query.makeLuceneQueryField("myDefaultField", factory);\ntopDocs = searcher.search(luceneQuery, 1000);\n
693	var parsed = query.Replace(" ", "?");\n
694	//From TopGroupsShardRequestFactory class in Solr\n\n// If group.format=simple group.offset doesn't make sense\n    Grouping.Format responseFormat = rb.getGroupingSpec().getResponseFormat();\n    if (responseFormat == Grouping.Format.simple || rb.getGroupingSpec().isMain()) {\n      sreq.params.remove(GroupParams.GROUP_OFFSET);\n    }\n
695	curl http://localhost:9200/my_idx/my_type/_search?q=Doe*\n
696	/* The latest pre-built standalone PDFBox jar file and the javaloader package are assumed to be in the same folder as the following component */\ncomponent{\n\n    function init( javaLoaderPath="javaloader.JavaLoader" ){\n        if( !server.KeyExists( "_pdfBoxLoader" ) ){\n            var paths=[];\n            paths.append( GetDirectoryFromPath( GetCurrentTemplatePath() ) &amp; "pdfbox-app-1.8.11.jar" );\n            server._pdfBoxLoader=New "#javaLoaderPath#"( paths );\n        }\n        variables.reader=server._pdfBoxLoader.create( "org.apache.pdfbox.pdmodel.PDDocument" );\n        variables.stripper=server._pdfBoxLoader.create( "org.apache.pdfbox.util.PDFTextStripper" );\n        return this;\n    }\n\n    string function extractText( required string pdfPath, numeric startPage=0, numeric endPage=0 ){\n        if( Val( startPage ) )\n            stripper.setStartPage( startPage );\n        if( Val( endPage ) )\n            stripper.setEndPage( endPage );\n        var pdf=reader.load( pdfPath );\n        var text=stripper.getText( pdf );\n        reader.close();\n        return text;\n    }\n\n}\n
697	Query query = parser.parse("\"" + QueryParser.escape(doc.get("title")) + "\"");\n
698	import java.nio.CharBuffer;\nimport java.nio.charset.Charset;\n\nimport org.apache.lucene.analysis.TokenStream;\nimport org.apache.lucene.analysis.util.FilteringTokenFilter;\nimport org.apache.lucene.analysis.tokenattributes.CharTermAttribute;\n\npublic final class ByteLengthFilter extends FilteringTokenFilter {\n\n  private final int min;\n  private final int max;\n  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n\n  public ByteLengthFilter(TokenStream in, int min, int max) {\n    super(in);\n    if (min &lt; 0) {\n      throw new IllegalArgumentException("minimum length must be greater than or equal to zero");\n    }\n    if (min &gt; max) {\n      throw new IllegalArgumentException("maximum length must not be greater than minimum length");\n    }\n    this.min = min;\n    this.max = max;\n  }\n\n  @Override\n  public boolean accept() {\n    final int len = Charset.forName("UTF-8").encode(CharBuffer.wrap(termAtt)).remaining();\n    return (len &gt;= min &amp;&amp; len &lt;= max);\n  }\n}\n
699	class CustomizedScoreProvider extends CustomScoreProvider {\n\n    public CustomizedScoreProvider(LeafReaderContext reader) {\n            super(reader);\n            // TODO Auto-generated constructor stub\n        }\n\n    public  float customScore(int doc, float subQueryScore,float valSrcScores[]){\n\n     try {\n\n         subQueryScore+=4; // I only added this for testing , \n     } catch(Exception e) { \n         e.printStackTrace();\n            }\n    return subQueryScore;\n             }\n    }\n\nclass CustomizedScoreQuery extends CustomScoreQuery{\n\n\npublic CustomizedScoreQuery(Query subQuery,IndexReader ireader) {\n        super(subQuery);\n        // TODO Auto-generated constructor stub\n    }\npublic CustomizedScoreProvider getCustomScoreProvider (LeafReaderContext reader){\n    CustomizedScoreProvider i=new CustomizedScoreProvider(reader);\n     return (i);\n}\n}\n
700	// assuming "query" returns a TopDocs\nvar dashDocs = query "+dash_username:mm +dash_description:Appointment" sort by "dash_ActivityId"\nvar activityDocs = query "+dash_username:mm +dash_description:Appointment" sort by "activity_Id"\n\nvar dashDocsEnum = dashDocs.ScoreDocs.GetEnumerator()\nforeach(var activityDocID in activityDocs.ScoreDocs)\n{\n    if(dashDocsEnum.Current==null)\n        break;\n\n    var activityId = GetId(activityDocId.td, "activity_id");\n    var dashActivityId = GetId(dashDocsEnum.Current.td, "dash_activityid");\n\n    if(dashActivityId&lt;activityId)\n    {\n        // spin Dash forward to catch up with Activity\n        while(dashActivityId&lt;activityId)\n        {\n            if(!dashDocsEnum.MoveNext())\n                break;\n            dashActivityId = GetId(dashDocsEnum.Current.td, "dash_activityid");\n        }\n    }\n\n    while(dashActivityId==activityId)\n    {\n        // at this point we have an Activity and a matched Dash\n        var fullActivity = GetActivity(activityDocId.td);\n        var fullDashActivity = GetDash(dashDocsEnum.Current.td);\n\n        // do something with Activity and Dash\n\n        if(!dashDocsEnum.MoveNext())\n            break;\n        dashActivityId = GetId(dashDocsEnum.Current.td, "dash_activityid");\n    }\n}\n
701	QueryParser myQueryParser = new QueryParser(myFieldName, new StandardAnalyzer());\nQuery query = myQueryParser.parse(QueryParserBase.escape(myDoc.get(myFieldName)));\n
702	VaultIndexes vIndexes = new VaultIndexes();\nprivate void btnRefreshIndex_Click(object sender, RoutedEventArgs e)\n{\n    vIndexes.refreshIndexes();\n}\n
703	FSDirectory luceneDirectory = FSDirectory.open(new File(INDEX_DIRECTORY).toPath());\nIndexWriterConfig conf = new IndexWriterConfig();\ntry (IndexWriter indexWriter = new IndexWriter(directory, conf)) {\n    for (File file : new File(Files).listFiles()) {\n    ...\n
704	@Query("field_name:?0")\n@Highlight(prefix="&lt;strong&gt;",postfix="&lt;/strong&gt;")\npublic HighlightPage&lt;YOUR_ENTITY_CLASS&gt; yourSomeMethod(String textToBeSearched);\n
705	FastVectorHighlighter highlighter = new FastVectorHighlighter(true, true);\nString highlightedResult = highlighter.getBestFragment(\n        highlighter.getFieldQuery(myQuery),\n        indexReader,\n        docId,\n        CONTENT_FIELD_NAME,\n        50 //char length of fragment\n    );\n
706	  &lt;!--&lt;field name="manu" type="text_general" indexed="true" stored="true" omitNorms="false"/&gt;--&gt;\n   &lt;field name="manu" type="text_synonyms_brand" indexed="true" stored="true" omitNorms="false"/&gt;\n   &lt;!--&lt;field name="category" type="text_general" indexed="true" stored="true" omitNorms="false"/&gt;--&gt;\n   &lt;field name="category" type="text_synonyms_category" indexed="true" stored="true" omitNorms="false"/&gt;\n
707	IndexWriterConfig conf= new IndexWriterConfig (Version.LUCENE_45,new FrenchAnalyzer(Version.LUCENE_45,FrenchAnalyzer.getDefaultStopSet()));\n
708	fl = File('index')\nindexDir = SimpleFSDirectory(fl)\n
709	// keep highlighted entries in sequence\n$highlights = [];\n\nforeach ($hits as $doc) {\n    // if we have a highlighting element for this document\n    if (!empty($result['highlighting'][$doc['id']])) {\n        // keep the highlighted text\n        $highlights[] = $result['highlighting'][$doc['id']];\n    } else {\n        // .. and if we don't, add a null element so indices match\n        $highlights[] = null; \n    }\n} \n
710	{!frange u=0 l=0}\n    sub(fieldCount,\n        sum(termfreq(field, 'term2'), \n            termfreq(field, 'term3'),\n            termfreq(field, 'term4')\n           )\n        )\n
711	@OneToMany(mappedBy = "item", fetch= FetchType.EAGER)\n@ContainedIn\nprivate Set&lt;CriticalComponent&gt; criticalComponents = new HashSet&lt;&gt;();\n
712	&lt;fieldType name="text_path" class="solr.TextField" positionIncrementGap="100"&gt;\n  &lt;analyzer&gt;\n    &lt;tokenizer class="solr.PathHierarchyTokenizerFactory" delimiter="\" replace="/"/&gt;\n  &lt;/analyzer&gt;\n&lt;/fieldType&gt;\n
713	private Query parseQueryForSong(String artist, String title, String album) throws ParseException {\n    String[] artistArr = artist.split(" ");\n    String[] titleArr = sanitizePhrase(title).split(" ");\n    String[] albumArr = sanitizePhrase(album).split(" ");\n\n    BooleanQuery.Builder mainQueryBuilder = new BooleanQuery.Builder();\n    BooleanQuery.Builder albumQueryBuilder = new BooleanQuery.Builder();\n    PhraseQuery artistQuery = new PhraseQuery("artist", artistArr);\n\n    for (String titleWord : titleArr) {\n        if (!titleWord.isEmpty()) {\n            mainQueryBuilder.add(new TermQuery(new Term("title", titleWord)), BooleanClause.Occur.SHOULD);\n        }\n    }\n\n    for (String albumWord : albumArr) {\n        if (!albumWord.isEmpty()) {\n            albumQueryBuilder.add(new TermQuery(new Term("album", albumWord)), BooleanClause.Occur.SHOULD);\n        }\n    }\n\n    mainQueryBuilder.add(artistQuery, BooleanClause.Occur.MUST);\n    mainQueryBuilder.add(albumQueryBuilder.build(), BooleanClause.Occur.MUST);\n\n    StandardAnalyzer analyzer = new StandardAnalyzer();\n    Query mainQuery = new QueryParser("title", analyzer).parse(mainQueryBuilder.build().toString());\n\n    return mainQuery;\n}\n
714	import static org.apache.commons.lang.StringUtils.*;\n\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport org.apache.lucene.index.Term;\nimport org.apache.lucene.search.Query;\nimport org.compass.core.CompassQuery;\nimport org.compass.core.engine.SearchEngineException;\nimport org.compass.core.engine.SearchEngineQuery;\nimport org.compass.core.impl.DefaultCompassQuery;\nimport org.compass.core.lucene.engine.LuceneSearchEngineQuery;\nimport org.compass.core.lucene.engine.queryparser.QueryParserUtils;\n\npublic class SearchTermExtractor {\n\npublic String extract(CompassQuery compassQuery, final String fieldName) {\n    SearchEngineQuery searchEngineQuery = ((DefaultCompassQuery)compassQuery).getSearchEngineQuery();\n    Query luceneQuery = ((LuceneSearchEngineQuery)searchEngineQuery).getQuery();\n\n    final List&lt;String&gt; suggestedTerms = new ArrayList&lt;String&gt;();\n\n    // Use a visitor to extract the terms\n    QueryParserUtils.visit(luceneQuery, new QueryParserUtils.QueryTermVisitor() {\n        @Override\n        public Term replaceTerm(Term term) throws SearchEngineException {\n            if (fieldName.equals(term.field())) {\n                suggestedTerms.add(term.text());\n            }\n            // Just return the original term, we're not trying to modify the query itself\n            return term;\n        }\n    });\n\n    return join(suggestedTerms, ' '); // join is from Commons Lang StringUtils\n}\n\n}\n
715	TopDocs results = searcher.Search(new TermQuery(new Term("MyIDField", Id)), 1);\nint internalId = results.scoreDocs[0].doc;\n
716	public virtual bool IsNewFragment(Token token)\n{\n    bool isNewFrag = token.EndOffset() &gt;= (fragmentSize * currentNumFrags);\n    if (isNewFrag)\n    {\n        currentNumFrags++;\n    }\n\n    return isNewFrag;\n}\n
717	    private static final int FETCH_SIZE = 100;\n    private static final int BATCH_SIZE = 1000;\n\n    //Scrollable results will avoid loading too many objects in memory\n    ScrollableResults scroll = query.scroll(ScrollMode.FORWARD_ONLY);\n    int batch = 0;\n    scroll.beforeFirst();\n    while (scroll.next()) {\n        batch++;\n\n        index(scroll.get(0)); //index each element\n\n        if (batch % BATCH_SIZE == 0) {\n            //flushToIndexes(); //apply changes to indexes\n            //optimize();\n            //clear(); //free memory since the queue is processed\n        }\n    }\n
718	&lt;entity name="fileItems"  rootEntity="false" dataSource="dbSource" query="select path from file_paths"&gt;\n  &lt;field column="path" name="link"/&gt;\n  &lt;entity name="tika-test" processor="TikaEntityProcessor" url="${fileItems.path}" dataSource="fileSource"&gt;\n    &lt;field column="title" name="title" meta="true"/&gt;\n    &lt;field column="Creation-Date" name="date_published" meta="true"/&gt;\n  &lt;/entity&gt;\n&lt;/entity&gt;\n
719	public StartsWithQuery Prefix(string prefix, string[] fields, Dictionary&lt;string,string&gt; filterFields = null )\n        {\n           if(!string.IsNullOrEmpty(prefix))\n           {\n\n\n               var parser = new MultiFieldQueryParser(Version.LUCENE_29, fields, new KeywordAnalyzer());\n               var boolQuery = new BooleanQuery();\n\n               boolQuery.Add(parser.Parse(prefix + "*"), BooleanClause.Occur.MUST);\n               if (filterFields != null)\n               {\n                   foreach (var field in filterFields)\n                   {\n                        boolQuery.Add(new TermQuery(new Term(field.Key, field.Value)), BooleanClause.Occur.MUST);\n\n                   }\n               }\n\n           }\n\n            return this;\n        }\n
720	public class MyQueue : Lucene.Net.Util.PriorityQueue&lt;int&gt;\n{\n    public MyQueue(int MaxSize) : base()\n    {\n        Initialize(MaxSize);\n    }\n\n    public override bool LessThan(int a, int b)\n    {\n        return a &lt; b;\n    }\n}\n\nint queueSize = 3;\nMyQueue pq = new MyQueue(queueSize);\npq.InsertWithOverflow(1);\npq.InsertWithOverflow(9);\npq.InsertWithOverflow(8);\npq.InsertWithOverflow(3);\npq.InsertWithOverflow(5);\n\nint i1 = pq.Pop();\nint i2 = pq.Pop();\nint i3 = pq.Pop();\n
721	RAMDirectory dir = new RAMDirectory();\nIndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true);\nfor (int i = 0; i &lt; 20; i++)\n{\n    Document doc = new Document();\n    doc.Add(new Field("field1", "some text " + i.ToString(), Field.Store.YES, Field.Index.ANALYZED));\n    doc.Add(new Field("ID", i.ToString(), Field.Store.YES, Field.Index.ANALYZED));\n    writer.AddDocument(doc);\n}\nwriter.Close();\n\nIndexReader reader = IndexReader.Open(dir);\n\nLucene.Net.Search.Similar.MoreLikeThisQuery mltq = new Lucene.Net.Search.Similar.MoreLikeThisQuery("text", new string[] { "field1" }, new WhitespaceAnalyzer());\n\nBooleanQuery bq = new BooleanQuery();\nbq.Add(new MatchAllDocsQuery(), BooleanClause.Occur.MUST);\nbq.Add(new TermQuery(new Term("ID","15")),BooleanClause.Occur.MUST_NOT);\nFilter filter = new CachingWrapperFilter(new QueryWrapperFilter(bq));\n\nTopDocs td =  new IndexSearcher(reader).Search(mltq, filter, 100);\nDebug.Assert(td.TotalHits == 19);\n\nreader.Close();\n
722	static searchable = {\n    topics component: true\n}\n
723	public static void main(String[] args) throws MalformedURLException, SolrServerException { \n\n        SolrServer solr = new HttpSolrServer("http://localhost:8080/solr");\n        ModifiableSolrParams parameters = new ModifiableSolrParams();\n          parameters.set("q", "*:*"); //query everything thanks to user1452132!\n          parameters.set("facet", true);//without this I cant select all\n          parameters.set("fl", "id");//send back just the id values\n          parameters.set("wt", "json");//Id like this in json format please\n\n        QueryResponse response = solr.query(parameters);\n            System.out.println(response); \n    }\n
724	static searchable = {\n    id: name 'fooId'\n}\n
725	BoolQueryBuilder boolQuery = QueryBuilders.boolQuery();\nboolQuery.should(QueryBuilders.termsQuery("lang", "en"));\nboolQuery.should(QueryBuilders.termsQuery("location", "en"));\n\nBoolQueryBuilder titleBoolQuery = QueryBuilders.boolQuery();\ntitleBoolQuery.should(QueryBuilders.termsQuery("title", new String[]{"americanlegion", "conversion"}));\ntitleBoolQuery.must(QueryBuilders.termQuery("title", "conversion"));\n\nboolQuery.should(titleBoolQuery);\n
726	@Indexed(interceptor=BookIndexingInterceptor.class)\npublic class Book extends BaseEntity{\n
727	from doc in docs\nfrom text in doc.Tests\nselect new { Text = text }\n
728	Query subQuery = new TermRangeQuery("myField", "aaaaaaaaa", null, true, false);\n
729	&lt;lib dir="${solr.install.dir:../../..}/custom-libs/" regex="solr-.*\.jar" /&gt;\n
730	/*\n   Sets the number of other words permitted between words \n   in query phrase.If zero, then this is an exact phrase search.  \n*/\n public void setSlop(int s) { slop = s; }\n
731	[\n    'q' =&gt; "{!join from=id to=service_id score=none}uri:\\$serviceUri* AND -deleted:true",\n    'fq' =&gt; "{!cache=false}category:monthly_volume AND type:\"$type\" AND timestamp:[$strDateStart TO $strDateEnd]",\n    'alt' =&gt; 'json',\n    'max-results' =&gt; 1000,\n    'sort' =&gt; 'timestamp ASC',\n    'statsFields' =&gt; 'stats.field=value&amp;stats.facet=timestamp',\n]\n
732	org.hibernate.search.FullTextQuery query = s.createFullTextQuery( query, Book.class );\norg.apache.lucene.search.Sort sort = new Sort(\nnew SortField("title", SortField.STRING));\nquery.setSort(sort);\nList results = query.list();\n
733	curl http://localhost:8983/solr/demo/query -d 'q=*:*&amp;fl=title,score'\n
734	var results = query.Page(page, itemsPerPage).GetResults();\n
735	Field subjectField = new TextField("subject", myEmailSubject, Field.Store.YES);\n
736	IndexWriter idxw = new IndexWriter(_directory, analyzer, false, IndexWriter.MaxFieldLength.UNLIMITED\n
737	FullTextQuery jpaQuery = ftem.createFullTextQuery(query, ProductModel.class);\njpaQuery.setProjection( FullTextQuery.THIS, "Image_url" );\nList results = jpaQuery.list();\nObject[] firstResult = (Object[]) results.get(0);\nProductModel productModel = firstResult[0];\nString imageUrl = firstResult[1];\n
738	Query query = NumericRangeQuery.newIntRange("id", offer.getId(), offer.getId(), true, true);\nindexWriter.deleteDocuments(query);\n
739	@Entity\n@AnalyzerDefs({\n    @AnalyzerDef(name = "keyword",\n        tokenizer = @TokenizerDef(factory = KeywordTokenizerFactory.class)\n    )\n})\n@Indexed\npublic class YourEntity {\n    @Fields({\n        @Field, // your default field with default analyzer if you need it\n        @Field(name = "propertyKeyword", analyzer = @Analyzer(definition = "keyword")) \n    })\n    private String property;\n}\n
740	1. For index phase, you can use a Custom Analyzer with a character filter to map whitespaces to underscores/emptystring.\n   eg:If you map whitespaces to emptystring, your data will be stored as:\n    Demo Site 001 ---&gt; DemoSite001\n    001 Demo Site ---&gt; 001DemoSite\n     "charFilters":[\n    {\n       "name":"map_dash",\n       "@odata.type":"#Microsoft.Azure.Search.MappingCharFilter",\n       "mappings":[" =&gt;"]\n    }\n\n\n   In query phase, \n      Step 1. Parse the query and substitute whitespace with the same identifier, as used in the index phase.\n          So , search query "Demo S" translates to  ---&gt; "DemoS"\n      Step 2. Do a wildcard  search for the new query string\n          search = DemoS*\n
741	&lt;filter class="solr.SnowballPorterFilterFactory" language="English" protected="protwords.txt"/&gt;\n
742	FullTextSession fullTextSession = Search.getFullTextSession(session);\nfullTextSession.createIndexer().startAndWait();\n
743	Query luceneQuery = mythQB\n    .simpleQueryString()\n    .onField("history")\n    .withAndAsDefaultOperator()\n    .matching("storm tree")\n    .createQuery();\n
744	org.apache.lucene.search.Query querypono1 = qb2.simpleQueryString().onField("poNo").matching(poNumber).createQuery();\n
745	POST /images_index/_update_by_query\n{\n    "script":{\n        "source": "ctx._source['images_count'] = ctx._source['images'].length"\n    }\n}\n
746	search=machineTag:/.*high\%20machine.*/&amp;$count=true&amp;$top=100&amp;$skip=0&amp;$filter=machineTag%20ne%20null&amp;queryType=full\n
747	&lt;analyzer&gt;\n  &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;\n  &lt;filter class="solr.ShingleFilterFactory"/&gt;\n&lt;/analyzer&gt;\n
748	&lt;!-- removes session ids from urls (such as jsessionid and PHPSESSID) --&gt;\n&lt;regex&gt;\n  &lt;pattern&gt;([;_]?((?i)l|j|bv_)?((?i)sid|phpsessid|sessionid)=.*?)(\?|&amp;amp;|#|$)&lt;/pattern&gt;\n  &lt;substitution&gt;$4&lt;/substitution&gt;\n&lt;/regex&gt;\n
749	Dim indexWriter = New IndexWriter(indexDir, New Standard.StandardAnalyzer(), True)\n
750	Parallel.ForEach(docs, d =&gt; { writer.Add(d,analyzer) });\n
751	// Called when an object is saved\npublic function save(Doctrine_Connection $conn = null) {\n    $conn = $conn ? $conn : $this-&gt;getTable()-&gt;getConnection();\n    $conn-&gt;beginTransaction();\n    try {\n        $ret = parent::save($conn);\n\n        $this-&gt;updateLuceneIndex();\n\n        $conn-&gt;commit();\n\n        return $ret;\n    } catch (Exception $e) {\n        $conn-&gt;rollBack();\n        throw $e;\n    }\n}\n\npublic function updateLuceneIndex() {\n    $index = $this-&gt;getTable()-&gt;getLuceneIndex();\n\n    // remove existing entries\n    foreach ($index-&gt;find('pk:' . $this-&gt;getId()) as $hit) {\n        $index-&gt;delete($hit-&gt;id);\n    }\n\n    $doc = new Zend_Search_Lucene_Document();\n\n    // store job primary key to identify it in the search results\n    $doc-&gt;addField(Zend_Search_Lucene_Field::UnIndexed('pk', $this-&gt;getId()));\n\n    // index job fields\n    $doc-&gt;addField(Zend_Search_Lucene_Field::unStored('title', Utils::stripAccent($this-&gt;getTitle()), 'utf-8'));\n    $doc-&gt;addField(Zend_Search_Lucene_Field::unStored('summary', Utils::stripAccent($this-&gt;getSummary()), 'utf-8'));\n\n    // add job to the index\n    $index-&gt;addDocument($doc);\n    $index-&gt;commit();\n}\n\n// Called when an object is deleted\npublic function delete(Doctrine_Connection $conn = null) {\n    $index = $this-&gt;getTable()-&gt;getLuceneIndex();\n\n    foreach ($index-&gt;find('pk:' . $this-&gt;getId()) as $hit) {\n        $index-&gt;delete($hit-&gt;id);\n    }\n\n    return parent::delete($conn);\n}\n
752	&lt;fieldType name="text" class="solr.TextField" positionIncrementGap="100" omitNorms="false"&gt;\n  &lt;analyzer&gt;\n    &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;\n    &lt;filter class="solr.StandardFilterFactory"/&gt;\n    &lt;filter class="solr.LowerCaseFilterFactory"/&gt;\n    &lt;filter class="solr.StopFilterFactory"/&gt;\n    &lt;filter class="solr.PorterStemFilterFactory"/&gt;\n &lt;/analyzer&gt;\n&lt;/fieldType&gt;\n
753	\npublic class TokenCombiner extends TokenFilter {\n  private final StringBuilder sb = new StringBuilder();\n  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n  private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n  private final char separator;\n  private boolean consumed; // true if we already consumed\n\n  protected TokenCombiner(TokenStream input, char separator) {\n    super(input);\n    this.separator = separator;\n  }\n\n  @Override\n  public final boolean incrementToken() throws IOException {\n    if (consumed) {\n      return false; // don't call input.incrementToken() after it returns false\n    }\n    consumed = true;\n\n    int startOffset = 0;\n    int endOffset = 0;\n\n    boolean found = false; // true if we actually consumed any tokens\n    while (input.incrementToken()) {\n      if (!found) {\n        startOffset = offsetAtt.startOffset();\n        found = true;\n      }\n      sb.append(termAtt);\n      sb.append(separator);\n      endOffset = offsetAtt.endOffset();\n    }\n\n    if (found) {\n      assert sb.length() &gt; 0; // always: because we append separator\n      sb.setLength(sb.length() - 1);\n      clearAttributes();\n      termAtt.setEmpty().append(sb);\n      offsetAtt.setOffset(startOffset, endOffset);\n      return true;\n    } else {\n      return false;\n    }\n  }\n\n  @Override\n  public void reset() throws IOException {\n    super.reset();\n    sb.setLength(0);\n    consumed = false;\n  }\n}\n
754	public sealed class NewWhitespaceAnalyzer : Analyzer\n{\n    public override TokenStream TokenStream(System.String fieldName, System.IO.TextReader reader)\n    {\n        return new LowerCaseFilter(new WhitespaceTokenizer(reader));\n    }\n\n    public override TokenStream ReusableTokenStream(System.String fieldName, System.IO.TextReader reader)\n    {\n        SavedStreams streams = (SavedStreams) GetPreviousTokenStream();\n        if (streams == null)\n        {\n            streams = new SavedStreams();\n            SetPreviousTokenStream(streams);\n            streams.tokenStream = new WhiteSpaceTokenizer(reader);\n            streams.filteredTokenStream = new LowerCaseFilter(streams.tokenStream);\n        }\n        else\n        {\n            streams.tokenStream.Reset(reader);\n        }\n        return streams.filteredTokenStream;\n    }\n}\n
755	TermEnum terms = indexReader.Terms(new Term("category"));\n// enumerate the terms\n
756	Query query = new QueryParser(Version.LUCENE_35, FIELD_AUTOCOMPLETE, analyzer).parse("Mado*");\n
757	DateTime EarliestDate = session.Query&lt;Movie&gt;()                      \n                         .Where(x =&gt; x.State == "NY")\n                         .Min(x =&gt; x.ReleaseDate);\n
758	public static final Version luceneVersion = Version.LUCENE_40;\n\nIndexWriter getIndexWriter(){\n\n    Directory indexDir = FSDirectory.open(new File( INDEX_PATH ));\n\n    IndexWriterConfig luceneConfig = new IndexWriterConfig(\n                luceneVersion, new StandardAnalyzer(luceneVersion));\n\n    return(new IndexWriter(indexDir, luceneConfig));\n   }\n
759	http://solr:8983/solr/admin/luke?numTerms=0\n
760	&lt;str name="pf"&gt;Title^100 Directors^10&lt;/str&gt;\n&lt;str name="qf"&gt;Title Directors&lt;/str&gt;\n
761	Filter LE = new QueryWrapperFilter(new TermQuery(new Term(field, "ZZZOCB9X9Y".ToLowerInvariant())));\n
762	function(head, req){\n  var filter = function(key){ \n    if (!req.query.q){\n      return key; // list all\n    }\n    if (!req.query.q.match('^[\d-]+$'){\n      return; // don't allow regex injections\n    }\n    var match = key.match('.*' + req.query.q + '.*');\n    if (match) return match[0];\n  }\n  start({'headers': {'Content-Type': 'text/plain'}});\n  var num = null;\n  while(row=getRow()){\n    num = filter(row.key);\n    if (num){\n      send(num + '\n');\n    }\n  }\n}\n
763	var query = queryParser.Parse(QueryParser.Escape(strQuery));\n
764	public static void addTChild() {\n    Session session = null;\n    try {\n        session = HibernateUtil.getSession();\n        session.beginTransaction();\n        TParent tParent = (TParent)session.get(TParent.class, 6L);\n        TChild tChild = new TChild(tParent, "Auto added " + new Date().toString(), 'y');\n        Long id  = (Long)session.save(tChild);\n        tChild = (TChild)session.get(TChild.class, id);\n        tParent.getTChildSet().add(tChild);\n        session.save(tParent);\n\n        /*\n         * Manually update the index.\n         * Necessary due to fact that the index field updated\n         * is not a DB table field and hence will not cause an index update\n         */\n        FullTextSession fullTextSession = Search.getFullTextSession(session);\n        fullTextSession.index(tParent);\n\n        session.getTransaction().commit();\n    } catch (Exception ex) {\n        session.getTransaction().rollback();\n        ex.printStackTrace();\n    } finally{\n        if(session != null) {\n            session.close();\n        }\n    }\n}\n
765	// do what you have to do to populate this variable with the search term\nvar searchTerm='search term here'; \nvar pageName = location.pathname+location.search;\npageName += (location.search) ? '&amp;' : '?';\npageName += 'q='+searchTerm;\n_gaq.push(['_trackPageview', pageName])\n
766	$lower = '"general doctor"';\n
767	&lt;field name="timestamp" type="date" indexed="true" stored="true" default="NOW" multiValued="false"/&gt;\n
768	public class Demo {\n\n    static Directory dir;\n\n    IndexWriter indexWriter;\n\n    public boolean openIndex() {\n        try {\n            dir = new RAMDirectory();\n            Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_44);\n            IndexWriterConfig iwc = new IndexWriterConfig(Version.LUCENE_44, analyzer);\n            indexWriter = new IndexWriter(dir, iwc);\n\n            return true;\n        } catch (Exception e) {\n            return false;\n        }\n    }\n\n    public void finish() {\n        try {\n            indexWriter.close();\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n    }\n\n    public void addDoc() {\n        Document doc = new Document();\n        doc.add(new TextField("field1", "val1", Field.Store.YES));\n        doc.add(new TextField("field2", "val2", Field.Store.YES));\n        doc.add(new TextField("field3", "val3", Field.Store.YES));\n        doc.add(new TextField("field4", "val4", Field.Store.YES));\n\n        try {\n            indexWriter.addDocument(doc);\n            indexWriter.commit();\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n    }\n\n    private static void printResults(Query query) throws IOException {\n        IndexReader reader = DirectoryReader.open(dir);\n        IndexSearcher searcher = new IndexSearcher(reader);\n        TopDocs result = searcher.search(query, 5);\n        System.out.println("# of results:" + result.totalHits);\n        for (ScoreDoc doc : result.scoreDocs) {\n            System.out.println(doc);\n        }\n\n        reader.close();\n    }\n\n    public static void main(String[] args) throws IOException, ParseException {\n        Demo demo = new Demo();\n        demo.openIndex();\n        demo.addDoc();\n        demo.finish();\n\n\n        BooleanQuery booleanQuery = new BooleanQuery();\n        Query query1 = new TermQuery(new Term("field1", "val1"));\n        Query query2 = new TermQuery(new Term("field2", "val2"));\n        Query query3 = new TermQuery(new Term("field3", "val3"));\n        Query query4 = new TermQuery(new Term("field4", "val4"));\n        booleanQuery.add(query1, BooleanClause.Occur.SHOULD);\n        booleanQuery.add(query2, BooleanClause.Occur.SHOULD);\n        booleanQuery.add(query3, BooleanClause.Occur.SHOULD);\n        booleanQuery.add(query4, BooleanClause.Occur.SHOULD);\n        printResults(booleanQuery);\n\n        Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_44);\n        MultiFieldQueryParser multiQueryParser = new MultiFieldQueryParser(Version.LUCENE_44,\n                    new String[] {"field1", "field2", "field3", "field4"}, analyzer);\n        multiQueryParser.setDefaultOperator(QueryParser.Operator.OR);\n\n        Query mfQuery = multiQueryParser.parse("field1:val1 field2:val2 field3:val3 field4:val4");\n        printResults(mfQuery);\n\n        Query mfQuery2 = multiQueryParser.parse("field1:val* field2:x field3:y field4:z");\n        printResults(mfQuery2);\n\n    }\n}\n
769	&lt;fieldType name="text_sku" class="solr.TextField" omitNorms="false"&gt;\n  &lt;analyzer&gt;\n    &lt;tokenizer class="solr.WhitespaceTokenizerFactory" /&gt;\n    &lt;filter class="solr.WordDelimiterFilterFactory" catenateAll="1" splitOnNumerics="1" preserveOriginal="1"/&gt;\n    &lt;filter class="solr.LowerCaseFilterFactory"/&gt;\n  &lt;/analyzer&gt;\n&lt;/fieldType&gt;\n
770	Query q = new QueryParser(Version.LUCENE_40, "contents", analyzer).parse(s);\nsearcher.search(q, collector);\nScoreDoc[] hits = collector.topDocs().scoreDocs;\n
771	        SearchDocument doc = new SearchDocument\n        {\n            UniqueKey = String.Format("{0}_{1}_{2}",\n                    moduleInfo.ModuleDefinition.DefinitionName, moduleInfo.PortalID, item.ItemId),\n            AuthorUserId = item.AssignedUserId,\n            ModifiedTimeUtc = item.LastModifiedOnDate.ToUniversalTime(),\n            Title = item.ItemName,\n            Body = item.ItemDescription,\n            Url = "",\n            CultureCode = "en-US",\n            Description = "DotNetNuclear Search Content Item",\n            IsActive = true,\n            ModuleDefId = moduleInfo.ModuleDefID,\n            ModuleId = item.ModuleId,\n            PortalId = moduleInfo.PortalID,\n            TabId = tab\n        };\n
772	private static Version VERSION;\n\npublic static void main(String[] args) throws IOException, ParseException {\n    //prepare\n    VERSION = Version.LUCENE_4_9;\n    Directory dir = new RAMDirectory();\n    IndexWriterConfig config = new IndexWriterConfig(VERSION, new SimpleAnalyzer(VERSION));\n    IndexWriter writer = new IndexWriter(dir, config);\n\n    String countryCode = "DE";\n\n    //index\n    Document doc = new Document();\n    doc.add(new TextField("countryCode", countryCode, Field.Store.YES));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexSearcher search = new IndexSearcher(DirectoryReader.open(dir));\n    //lookup\n    Query query = new QueryParser(VERSION, "countryCode", new SimpleAnalyzer(VERSION)).parse(countryCode);\n\n    TopDocs docs = search.search(query, 1);\n    System.out.println(docs.totalHits);\n}\n
773	while ((byteRef = iterator.next()) != null) {\n    System.out.println(byteRef.utf8ToString() + " - " + iterator.docFreq());\n}\n
774	    String searchable;\n    org.apache.lucene.search.Query query = qb.bool()\n                              .should( qb.keyword().onField("title").ignoreFieldBridge().matching(searchable).createQuery() )\n                              .should( qb.keyword().onField("isbnNo").ignoreAnalyzer().matching(searchable).createQuery() )\n                            .createQuery();\n
775	var query = context.GetQueryable&lt;MovieSearchResultItem&gt;()\n    .Where(result =&gt; result.Body.Contains(txtQuery)\n        || result.Title.Contains(txtQuery)\n        || result.OtherField.Contains(txtQuery)).ToList();\n
776	export LD_LIBRARY_PATH=$PREFIX/lib:$PREFIX/jre/lib:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=$PREFIX/jre/lib/amd64:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=$PREFIX/jre/lib/amd64/server:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=$PREFIX/lib/python2.7/site-packages:$LD_LIBRARY_PATH\n
777	package org.apache.lucene.index;\n\nimport java.io.IOException;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\n\nimport org.apache.lucene.codecs.Codec;\nimport org.apache.lucene.store.Directory;\nimport org.apache.lucene.store.IOContext;\nimport org.apache.lucene.store.SimpleFSDirectory;\n\npublic class GenSegmentInfo {\n    public static void main(String[] args) throws IOException {\n        Codec codec = Codec.getDefault();\n        Path myPath = Paths.get("/tmp/index");\n        Directory directory = new SimpleFSDirectory(myPath);\n\n        //launch this the first time with random segmentID value\n        //then with java debug, get the right segment ID\n        //by putting a breakpoint on CodecUtil#checkIndexHeaderID(...)\n        byte[] segmentID = {88, 55, 58, 78, -21, -55, 102, 99, 123, 34, 85, -38, -70, -120, 102, -67};\n\n        SegmentInfo info = codec.segmentInfoFormat().read(directory, "_1rpt",\n                segmentID, IOContext.READ);\n        info.setCodec(codec);\n        SegmentInfos infos = new SegmentInfos();\n        SegmentCommitInfo commit = new SegmentCommitInfo(info, 1, -1, -1, -1);\n        infos.add(commit);\n        infos.commit(directory);\n    }\n}\n
778	List&lt;issue&gt; issues=db.search("searchJson/newSearch")\n.limit(10).includeDocs(true)\n.query("name:\"web*\"", issue.class);\n
779	public class CustomAnalyzer extends Analyzer {\n\n    protected TokenStreamComponents createComponents(String s) {\n        // provide your own tokenizer, that will split input string as you want it\n        final Tokenizer standardTokenizer = new MyStandardTokenizer();\n\n        TokenStream tok = new StandardFilter(standardTokenizer);\n        // make everything lowercase, remove if not needed\n        tok = new LowerCaseFilter(tok);\n        //provide stopwords if you want them\n        tok = new StopFilter(tok, stopwords);\n        return new TokenStreamComponents(standardTokenizer, tok);\n    }\n\n    private class MyStandardTokenizer extends Tokenizer {\n\n        public boolean incrementToken() throws IOException {\n            //mimic the logic of standard analyzer and add your rules\n            return false;\n        }\n    }\n}\n
780	defType=edismax&amp;pf2=title^4\n
781	IsBundle==false OR RelationshipID==x\n
782	 Map&lt;String,Serializable&gt; params = new HashMap&lt;&gt;();\n params.put("query", "/x:path/to:node/pa:th");\n NodeRef nodeRef = nodeLocatorService.getNode("xpath",null,params);\n
783	tokenizer = @TokenizerDef(factory = JapaneseTokenizerFactory.class,\n                params = {@Parameter(name = "mode", value = "findFeatures"),\n                        }),\n
784	 public static String fieldFlags(Field fld, FieldInfo info) {\n    ...\n    Number numeric = null;\n    if (fld == null) {\n      ...\n    } else {\n      ...\n      numeric = fld.numericValue();\n    }\n    ...\n    if (numeric != null) {\n      flags.append("#");\n      // try faking it\n      if (numeric instanceof Integer) {\n        flags.append("i32");\n      } else if (numeric instanceof Long) {\n        flags.append("i64");\n      } else if (numeric instanceof Float) {\n        flags.append("f32");\n      } else if (numeric instanceof Double) {\n        flags.append("f64");\n      } else if (numeric instanceof Short) {\n        flags.append("i16");\n      } else if (numeric instanceof Byte) {\n        flags.append("i08");\n      } else if (numeric instanceof BigDecimal) {\n        flags.append("b^d");\n      } else if (numeric instanceof BigInteger) {\n        flags.append("b^i");\n      } else {\n        flags.append("???");\n      }\n      ...\n    }\n
785	&lt;copyField source="spellcheck_en" dest="spellcheck_low" /&gt;\n&lt;copyField source="spellcheck_en" dest="spellchecksearch" /&gt;\n
786	ID        Title              Categories\n\n"MyDoc1", "Hello world!",    "/programming/beginner/samples"\n"MyDoc2", "Prove that P=NP", "/programming/advanced/samples"\n
787	Document _document = new Document();\n_document.add(new Field("type", document.getType().getBytes(), Store.YES);\n// Or document.add(new Field("type", document.getType().getBytes(), Store.NO);\n
788	private Query CreateFilteredQuery (string account, string folder, Query criteria)\n{\n     BooleanQuery bq = new BooleanQuery();\n     bq.Add(new TermQuery (new Lucene.Net.Index.Term ("account", account)), BooleanClause.Occur.MUST);\n     bq.Add(new TermQuery (new Lucene.Net.Index.Term ("folder", folder)), BooleanClause.Occur.MUST);\n     bq.Add(criteria, BooleanClause.Occur.MUST);\n     return bq;\n}\n\n\nQuery filteredQuery = CreateFilteredQuery ("fake@fake.com", "inbox", myQueryParser.Parse (criteria));\nvar hits = myIndexSearcher.Search (filteredQuery);\n
789	localhost:8080/solr/select/?q=greekbailout&amp;wt=json&amp;sort=date asc&amp;rows=N\n
790	&lt;requestHandler name="/relevant_by_one_field" class="solr.SearchHandler"&gt;\n   &lt;lst name="defaults"&gt;\n      &lt;str name="defType"&gt;edismax&lt;/str&gt;\n      &lt;str name="qf"&gt;title&lt;/str&gt;\n   &lt;/lst&gt;\n&lt;/requestHandler&gt;\n
791	START node=node(*) \nMATCH node-[rel]-() \nRETURN id(node) as id, count(rel) as count \nORDER BY count DESC\n
792	private void search_btnActionPerformed(java.awt.event.ActionEvent evt) {                                           \n    str=search_txb.getText().split(" ");\n    // initialize nodes to a new array with the same size as str\n    nodes = new Node[str.length]; \n    for(int i=0;i&lt;str.length;++i)\n    ......\n\n}\n
793	package tikatest01;\n\nimport java.io.File;\nimport org.apache.lucene.document.Document;\nimport org.apache.lucene.document.Field.Store;\nimport org.apache.lucene.document.TextField;\nimport org.apache.lucene.index.IndexWriter;\nimport org.apache.tika.Tika;\n\npublic class LuceneIndexer {\n\n    private final Tika tika;\n    private final IndexWriter writer;\n\n    public LuceneIndexer(Tika tika, IndexWriter writer) {\n        this.tika = tika;\n        this.writer = writer;\n    }\n\n    public void indexDocument(File file) throws Exception {\n        Document document = new Document();\n        document.add(new TextField(\n                "filename", file.getName(), Store.YES));\n        document.add(new TextField(\n                "fulltext", tika.parseToString(file), Store.NO));\n        writer.addDocument(document);\n    }\n}\n
794	public abstract class AnalyzerView\n{\n    public abstract string Name { get; }\n\n    public virtual string GetView(TokenStream tokenStream,out int numberOfTokens)\n    {\n        StringBuilder sb = new StringBuilder();\n        numberOfTokens = 0;\n        while (tokenStream.IncrementToken())\n        {\n            numberOfTokens++;\n            sb.Append(GetTokenView(tokenStream));\n        }\n\n        return sb.ToString();\n    }\n\n    protected abstract string GetTokenView(TokenStream tokenStream);\n}\n
795	    boolean ascending = false;\n    SortField idSortField = new SortField("id", SortField.Type.LONG, ascending);\n\n    SortingMergePolicy sortingMP = new SortingMergePolicy(\n            iwc.getMergePolicy(), new Sort(idSortField));\n    iwc.setMergePolicy(sortingMP);\n
796	try (Directory dir = new RAMDirectory()) {\n    // use dir here, it will be automatically closed at the end of this block.\n}\n// exception catching omitted\n
797	Analyzer analisador = new BrazilianAnalyzer(Version.LUCENE_4_9);\nQueryParser parser = new QueryParser(Version.LUCENE_4_9, "Texto", analisador);\n
798	import org.apache.lucene.document.Document;\nimport org.apache.lucene.index.LeafReader;\nimport org.apache.lucene.index.LeafReaderContext;\nimport org.apache.lucene.search.SimpleCollector;\n\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.List;\n\n\nfinal class AllDocumentsCollector extends SimpleCollector {\n\n  private final List&lt;Document&gt; documents;\n  private LeafReader currentReader;\n\n  public AllDocumentsCollector(final int numDocs) {\n    this.documents = new ArrayList&lt;&gt;(numDocs);\n  }\n\n  public List&lt;Document&gt; getDocuments() {\n    return Collections.unmodifiableList(documents);\n  }\n\n  @Override\n  protected void doSetNextReader(final LeafReaderContext context) {\n    currentReader = context.reader();\n  }\n\n  @Override\n  public void collect(final int doc) throws IOException {\n    documents.add(currentReader.document(doc));\n  }\n\n  @Override\n  public boolean needsScores() {\n    return false;\n  }\n}\n
799	private static boolean check(Set cache, String ref, String ip, String ue, String mail) {\n    return Sets.powerSet(ImmutableSet.of(0, 1, 2, 3)).stream().map(set -&gt; {\n        BlacklistKey key = new BlacklistKey("*", "*", "*", "*");\n        for (Integer idx : set) {\n            switch (idx) {\n                case 0:\n                    key.setReferrer(ref);\n                    break;\n                case 1:\n                    key.setIp(ip);\n                    break;\n                case 2:\n                    key.setUserAgent(ue);\n                    break;\n                case 3:\n                    key.setEmail(mail);\n            }\n        }\n        return key;\n    }).anyMatch(keys::contains);\n}\n
800	Iterator iter = FrenchAnalyzer.getDefaultStopSet().iterator();\nwhile(iter.hasNext()) {\n    char[] stopWord = (char[]) iter.next();\n    stopWords.add(new String (stopWord));\n}\n
801	_query_:"{!edismax fq=$fq1 qf=$qf1 v=$condQuery}"\nOR\n_query_:"{!edismax fq=$fq2 qf=$qf2 v=$condQuery}"\n
802	TYPE:"hr:hrdoctype" AND =@cm\:name:"E"\n
803	http://host:port/solr/core/update?stream.body=&lt;delete&gt;&lt;query&gt;*:*&lt;/query&gt;&lt;/delete&gt; \n
804	FSDirectory dir = FSDirectory.GetDirectory("C:\\temp\\CFSTEST");\nIndexWriter writer = new IndexWriter(dir, new CJKAnalyzer());\nwriter.SetUseCompoundFile(true);\n\nDocument document = new Document();\n\ndocument.Add(new Field(\n    "text",\n    "プーケット",\n    Field.Store.YES,\n    Field.Index.ANALYZED));\nwriter.AddDocument(document);\n\ndocument.GetField("text").SetValue("another doc");\nwriter.AddDocument(document);\n\nwriter.Optimize(1, true);\nwriter.Close();\n
805	public class PaddedIntegerBridge implements StringBridge {\n\n    private int PADDING = 5;\n\n    public String objectToString(Object object) {\n        String rawInteger = ( (Integer) object ).toString();\n        if (rawInteger.length() &gt; PADDING)\n            throw new IllegalArgumentException( "Try to pad on a number too big" );\n        StringBuilder paddedInteger = new StringBuilder( );\n        for ( int padIndex = rawInteger.length() ; padIndex &lt; PADDING ; padIndex++ ) {\n            paddedInteger.append('0');\n        }\n        return paddedInteger.append( rawInteger ).toString();\n    }\n}\n
806	@Override\nprotected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n    Tokenizer source = new LetterTokenizer(Version.LUCENE_44, reader);              \n    TokenStream filter = new LowerCaseFilter(Version.LUCENE_44, source);                \n    filter = new StopFilter(Version.LUCENE_44, filter, stopWords);                  \n    filter = new PorterStemFilter(filter);\n    return new TokenStreamComponents(source, filter);\n} \n
807	http://localhost:8983/solr/coreMikisa/select/?q=*%3A*&amp;version=2.2&amp;start=0&amp;rows=10&amp;indent=on&amp;stats=true&amp;stats.field=Price \n
808	Directory dir = new RAMDirectory();\n\nSimilarity similarity = new DefaultSimilarity() {\n  @Override\n  public float computeNorm(String fld, FieldInvertState state) {\n    return state.getBoost();\n  }\n\n  @Override\n  public float coord(int overlap, int maxOverlap) {\n    return 1f / maxOverlap;\n  }\n\n  @Override\n  public float idf(int docFreq, int numDocs) {\n    return 1f;\n  }\n\n  @Override\n  public float queryNorm(float sumOfSquaredWeights) {\n    return 1f;\n  }\n\n  @Override\n  public float tf(float freq) {\n    return freq == 0f ? 0f : 1f;\n  }\n};\nIndexWriterConfig iwConf = new IndexWriterConfig(Version.LUCENE_35,\n    new WhitespaceAnalyzer(Version.LUCENE_35));\niwConf.setSimilarity(similarity);\nIndexWriter iw = new IndexWriter(dir, iwConf);\nDocument doc = new Document();\nField field = new Field("text", "", Store.YES, Index.ANALYZED);\ndoc.add(field);\nfor (String value : Arrays.asList("a b c", "c d", "a b d", "a c d")) {\n  field.setValue(value);\n  iw.addDocument(doc);\n}\niw.commit();\niw.close();\n\nIndexReader ir = IndexReader.open(dir);\nIndexSearcher searcher = new IndexSearcher(ir);\nsearcher.setSimilarity(similarity);\nBooleanQuery q = new BooleanQuery();\nq.add(new TermQuery(new Term("text", "a")), Occur.SHOULD);\nq.add(new TermQuery(new Term("text", "b")), Occur.SHOULD);\nq.add(new TermQuery(new Term("text", "d")), Occur.SHOULD);\n\nTopDocs topDocs = searcher.search(q, 100);\nSystem.out.println(topDocs.totalHits + " results");\nScoreDoc[] scoreDocs = topDocs.scoreDocs;\nfor (int i = 0; i &lt; scoreDocs.length; ++i) {\n  int docId = scoreDocs[i].doc;\n  float score = scoreDocs[i].score;\n  System.out.println(ir.document(docId).get("text") + " -&gt; " + score);\n  System.out.println(searcher.explain(q, docId));\n}\nir.close();\n
809	item[] = fragments[0]\nfragment = item[0]\n
810	Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_36);\nanalyzer = new ShingleAnalyzerWrapper(analyzer, 1, 5); //wrapper that adds a ShingleFilter to the analyzer\nQueryParser parser = new StandardQueryParser(analyzer);  \nQuery query = parser.parse(query, defaultField);\nsearcher.search(query, 10);\n
811	IndexReader[] readers = new IndexReader[size];\n// Initialize readers\nMultiReader multiReader = new MultiReader(readers);\n\nIndexSearcher searcher = new IndexSearcher(multiReader);\nHits results = searcher.search(new TermQuery(t));\nfor (int i = 0; i &lt; results.length(); i++) {\n    int docID = results.id(i);\n    multiReader.deleteDocument(docID);\n}\nmultiReader.commit(); // Check if this throws an exception.\nmultiReader.close();\nsearcher.close();\n
812	var luceneAnalyzer = new KeywordAnalyzer();\n\nvar query1 = new QueryParser("Topic", luceneAnalyzer).Parse("hello");\nvar query2 = new QueryParser("Topic", luceneAnalyzer).Parse("world");\n\nBooleanQuery filterQuery = new BooleanQuery();\nfilterQuery.Add(query1, BooleanClause.Occur.MUST);\nfilterQuery.Add(query1, BooleanClause.Occur.MUST);\n\nTopDocs results = searcher.Search(filterQuery);\n
813	String filepath = pageContext.getServletContext().getRealPath("/") + "WEB-INF" + java.io.File.separator + "index";\n
814	Query q = parser.getFuzzyQuery(field, term, minSimilarity);\n
815	var parser = new QueryParser("contents", new StandardAnalyzer());\nQuery query = parser.Parse(Query);\nSimpleHTMLFormatter formatter = new SimpleHTMLFormatter(config.HighlightFormatterPrefix, config.HighlightFormatterSuffix);\nQueryScorer fragmentScorer = new QueryScorer(query,"contents");\nHighlighter highlighter = new Highlighter(formatter, fragmentScorer);\nhighlighter.SetTextFragmenter(new SimpleFragmenter(100));\nTokenStream tokenStream = new SimpleAnalyzer().TokenStream(config.MainContentFieldName, new StringReader(field.StringValue()));\n\nreturn highlighter.GetBestFragments(tokenStream, field.StringValue(), 2, ".");\n
816	var searcher = new IndexSearcher(indexPath);\n\n// get document at position 0\nvar doc = searcher.Doc( 0 );\n
817	public void indexRecord(CouchDBRecord rec) {\n    Document doc = new Document();\n    doc.add(new Field("device_id", rec.device_id, Store.YES, Index.NOT_ANALYZED));\n    doc.add(new Field("college_name",  rec.college_name, Store.YES, Index.ANALYZED));\n    doc.add(new Field("college_year", rec.college_year.toString(), Store.YES, Index.NOT_ANALYZED));\n    this.writer.addDocument(doc);\n}\n
818	Term curTerm = iReader.Term();\nbool hasNext = true;\nwhile (curTerm != null &amp;&amp; hasNext)\n{\n    //do whatever you need with the current term....\n    hasNext = iReader.Next();\n    curTerm = iReader.Term();\n}\n
819	.../lucene-3.4.0/tmp $ ls\nlucene-core-3.4.0.jar   lucene-demo-3.4.0.jar\n.../lucene-3.4.0/tmp $ export CLASSPATH=./lucene-core-3.4.0.jar:./lucene-demo-3.4.0.jar\n.../lucene-3.4.0/tmp $ echo $CLASSPATH\n./lucene-core-3.4.0.jar:./lucene-demo-3.4.0.jar\n.../lucene-3.4.0/tmp $ java org.apache.lucene.demo.IndexFiles -docs .\nIndexing to directory 'index'...\nadding ./lucene-core-3.4.0.jar\nadding ./lucene-demo-3.4.0.jar\n1485 total milliseconds\n.../lucene-3.4.0/tmp $\n
820	 &lt;param name="excerptProviderClass" value="org.apache.jackrabbit.core.query.lucene.DefaultHTMLExcerpt"/&gt;\n
821	 BooleanQuery topQuery = new BooleanQuery();\n topQuery.add(new TermQuery(...), BooleanClause.Occur.Must);\n etc.\n
822	Field field = new Field(field_name, Float.toString(weight), Store.YES, Index.NOT_ANALYZED_NO_NORMS);&lt;br&gt;\n
823	Query q = new TermQuery(new Term("name", "Jack"));\n
824	public static class MultiTermUseIdfOfSearchTerm&lt;Q extends Query&gt; extends TopTermsRewrite&lt;BooleanQuery&gt; {\n\n    //public static final class MultiTermUseIdfOfSearchTerm extends TopTermsRewrite&lt;BooleanQuery&gt; {\n        private final Similarity similarity;\n\n        /**\n         * Create a TopTermsScoringBooleanQueryRewrite for\n         * at most &lt;code&gt;size&lt;/code&gt; terms.\n         * &lt;p&gt;\n         * NOTE: if {@link BooleanQuery#getMaxClauseCount} is smaller than\n         * &lt;code&gt;size&lt;/code&gt;, then it will be used instead.\n         */\n        public MultiTermUseIdfOfSearchTerm(int size) {\n            super(size);\n            this.similarity = new DefaultSimilarity();\n\n        }\n\n        @Override\n        protected int getMaxSize() {\n            return BooleanQuery.getMaxClauseCount();\n        }\n\n        @Override\n        protected BooleanQuery getTopLevelQuery() {\n            return new BooleanQuery(true);\n        }\n\n        @Override\n        protected void addClause(BooleanQuery topLevel, Term term, float boost) {\n            final Query tq = new ConstantScoreQuery(new TermQuery(term));\n            tq.setBoost(boost);\n            topLevel.add(tq, BooleanClause.Occur.SHOULD);\n        }\n\n        protected float getQueryBoost(final IndexReader reader, final MultiTermQuery query)\n                throws IOException {\n            float idf = 1f;\n            float df;\n            if (query instanceof PrefixQuery)\n            {\n                PrefixQuery fq = (PrefixQuery) query;\n                df = reader.docFreq(fq.getPrefix());\n                if(df&gt;=1)\n                {\n                    idf = (float)Math.pow(similarity.idf((int) df, reader.numDocs()),2);\n                }\n            }\n            return idf;\n        }\n\n        @Override\n        public BooleanQuery rewrite(final IndexReader reader, final MultiTermQuery query) throws IOException {\n            BooleanQuery  bq = (BooleanQuery)super.rewrite(reader, query);\n\n            float idfBoost = getQueryBoost(reader, query);\n            Iterator&lt;BooleanClause&gt; iterator = bq.iterator();\n            while(iterator.hasNext())\n            {\n                BooleanClause next = iterator.next();\n                next.getQuery().setBoost(next.getQuery().getBoost() * idfBoost);\n            }\n            return bq;\n        }\n\n    }\n
825	&lt;analyzer&gt;\n  &lt;charFilter class="solr.HTMLStripCharFilterFactory"/&gt;\n  &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;\n&lt;/analyzer&gt;\n
826	URIEncoding="UTF-8" useBodyEncodingForURI = "false"\n
827	$query = new Zend_Search_Lucene_Search_Query_MultiTerm();\n$query-&gt;addTerm(new Zend_Search_Lucene_Index_Term('en', 'language'), true);\n$query-&gt;addTerm(new Zend_Search_Lucene_Index_Term('us', 'country'), true);\n$hits  = $index-&gt;find($query);\n
828	 public SearchManager(string indexName)\n {\n   SearchIndexName = indexName;\n   Database database = Factory.GetDatabase("master");\n   var item = Sitecore.Context.Site.StartPath;\n   SiteRoot = database.GetItem(item);\n}\n[...]\npublic SearchResultCollection Search(string searchString)\n{\n  //Getting index from the web.config\n  var searchIndex = Sitecore.Search.SearchManager.GetIndex(SearchIndexName);\n  using(IndexSearchContext context = searchIndex.CreateSearchContext())\n  {\n     SearchHits hits = context.Search(searchString, new SearchContext(SiteRoot));\n
829	TokenStream ts = analyzer.tokenStream(field, new StringReader(text));\nCharTermAttribute charTermAttribute = ts.addAttribute(CharTermAttribute.class);\n\n\nwhile (ts.incrementToken()) {\n    String term = charTermAttribute.toString();\n    //term contains your token\n}\n
830	http://localhost:8983/solr/select/?q=tree\n  &amp;defType=edismax&amp;qf=essence^4.0+keywords^2.0+allSearchable^1.0\n
831	&lt;script&gt;&lt;![CDATA[\n        function addfield(row){\n            var fieldName = row.get('double_value');\n    // Round it\n            row.put(fieldName, double_rounded);\n            return row;\n        }\n]]&gt;&lt;/script&gt;\n
832	public class Gatanizer extends TokenFilter {\n\n    private final CharTermAttribute termAttribute = addAttribute(CharTermAttribute.class);\n\n    /**\n     * Construct a token stream filtering the given input.\n     */\n    protected Gatanizer(TokenStream input) {\n        super(input);\n    }\n\n    @Override\n    public boolean incrementToken() throws IOException {\n        if (input.incrementToken()) {\n\n            final char[] buffer = termAttribute.buffer();\n            final int length = termAttribute.length();\n\n            String tokenString = new String(buffer, 0, length);\n            tokenString = StringUtils.removeEnd(tokenString, "gatan");\n\n            termAttribute.setEmpty();\n            termAttribute.append(tokenString);\n\n            return true;\n        }\n\n        return false;\n    }\n\n}\n
833	final Map&lt;String, Integer&gt; country2count = new HashMap&lt;String, Integer&gt;();\n    for (final ScoreDoc hit : hits) {\n        final int docId = hit.doc;\n        if (!reader.isDeleted(docId)) {\n            // Get the document from docId\n            final Document document = searcher.doc(docId);\n            // Get the country\n            final String country = document.get("from");\n\n            if(country2count.containsKey(country)){\n                int prevCount = country2count.get(country);\n                country2count.put(country, ++prevCount);\n            }else{\n                country2count.put(country, 1);\n            }\n        }\n    }\n
834	durability_feature=11\n
835	private final KStemmer stemmer = new KStemmer();\n// char[] term = ...\nstemmer.stem(term, len);\n
836	for (int docNum=0; docNum &lt; reader.maxDoc(); docNum++) {\n    if (reader.isDeleted(docNum)) {\n        continue;\n    }\n    TermFreqVector tfv = reader.getTermFreqVector(docNum, "fieldName");\n    ...\n}\n
837	start n=node:topic('name:(keyword1* AND keyword2*)') MATCH (n)  with n order by length(split(n.name," ")) asc limit 20 return n\n
838	export JAVA_HOME=/usr/lib/jvm/jdk1.7.0_75\n
839	tomcat home/webapps/ROOT/WEB-INF/classes/META-INF/portal-log4j-ext.xml file I used to triage various issues on bootup related to clustering.\n\n&lt;?xml version="1.0"?&gt;\n&lt;!DOCTYPE log4j:configuration SYSTEM "log4j.dtd"&gt;\n\n&lt;log4j:configuration xmlns:log4j="http://jakarta.apache.org/log4j/"&gt;\n\n&lt;category name="com.liferay.portal.cluster"&gt;\n&lt;priority value="TRACE" /&gt;\n&lt;/category&gt;\n\n&lt;category name="com.liferay.portal.license"&gt;\n&lt;priority value="TRACE" /&gt;\n&lt;/category&gt;\n
840	MATCH (p:Person) WHERE p.name='abc'\n
841	HttpSolrClient solrClient =  new HttpSolrClient("http://localhost:8983/solr/core1"\n
842	s(q, d) = q * d / (||q|| * ||d||)\n
843	int n;\n//Set this variable to the # of terms\nString nextTerm;\nTerm[] termL2 = new Term[n];\nfor (int i = 0; i &lt; n; i++) {\n    nextTerm = blahblah; //here you set your next term\n    termL2[i] = new Term("abstract", nextTerm);\n}\nmQuery.add(termL2);\n
844	StandardAnalyzer analyzer - new StandardAnalyzer()\nDirectory index = FSDirectory.open(new File("...").toPath());\nIndexWriterConfig config = new IndexWriterConfig(analyzer);\naddDoc(writer, "...", "...");\naddDoc(writer, "...", "...");\naddDoc(writer, "...", "...");\n// documents need to be read from the data source..\n// only add once, or else your docs will be duplicated as you continue to use the system\nwriter.close();\n
845	  public class CIAIAnalyser : Analyzer\n{\n    public override TokenStream TokenStream(string fieldName, System.IO.TextReader reader)\n    {\n        StandardTokenizer tokenizer = new StandardTokenizer(Lucene.Net.Util.Version.LUCENE_29, reader);\n\n        tokenizer.SetMaxTokenLength(255);\n        TokenStream stream = new StandardFilter(tokenizer);\n        stream = new LowerCaseFilter(stream);\n        return new ASCIIFoldingFilter(stream);\n\n    }\n\n}\n
846	 TokenStream ts= analyzer.tokenStream("myfield", reader);\n            // The Analyzer class will construct the Tokenizer, TokenFilter(s), and CharFilter(s),\n            //   and pass the resulting Reader to the Tokenizer.\n            OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n            CharTermAttribute charTermAttribute = ts.addAttribute(CharTermAttribute.class);\n\n            try {\n                ts.reset(); // Resets this stream to the beginning. (Required)\n                while (ts.incrementToken()) {\n                    // Use AttributeSource.reflectAsString(boolean)\n                    // for token stream debugging.\n                    System.out.println("token: " + ts.reflectAsString(true));\n                    String term = charTermAttribute.toString();\n                    System.out.println(term);\n\n                }\n                ts.end();   // Perform end-of-stream operations, e.g. set the final offset.\n            } finally {\n                ts.close(); // Release resources associated with this stream.\n            }\n
847	import org.apache.lucene.index.{ Term, IndexReader }\nimport org.apache.lucene.search.{ TermQuery, Query }\nimport org.apache.lucene.search.function.{ CustomScoreProvider, CustomScoreQuery }\n\nclass CustomizedScoreProvider(reader: IndexReader) extends \n    CustomScoreProvider(reader) {\n\n    protected override def customScore(doc: Int, \n        subQueryScore: Float, valSrcScores: Array[Float]): Float = {\n        try {\n            // subQueryScore is the default score you get from  \n            // the original Query\n            val currentDocument = reader.document(doc)\n\n            // get the value of two field1, field2, \n            // make sure the two fields are stored since you have to retrieve the value\n            val field1Value = currentDocument.get("field1")\n            val field2Value = currentDocument.get("field2")\n\n            // write your own logical to compute the extra score, assuming 0 here\n            val extraScore = 0F\n\n            // ignore the valSrcScores here, the original calculation      \n            // is modifiedScore = subQueryScore*valSrcScores[0]*..\n            subQueryScore + extraScore\n        } catch {\n            case _: Exception =&gt; subQueryScore\n        }\n    }\n}\n\n/**\n * Create a CustomScoreQuery over input subQuery.\n * @param subQuery the sub query whose scored is being customized. Must not be null.\n */\nclass CustomizedScoreQuery(val subQuery: Query, reader: IndexReader) \n    extends CustomScoreQuery(subQuery) {\n    protected override def getCustomScoreProvider(reader: IndexReader) = {\n        try {\n            new CustomizedScoreProvider(reader)\n        } catch {\n            case _: Exception =&gt; super.getCustomScoreProvider(reader)\n        }\n    }\n}\n\n\nobject CustomizedScoreQueryTest extends App {\n\n    val termQuery = new TermQuery(new Term("field", "xxx"))\n    // get indexSearch, indexReader and  wrap the original query\n    ...\n    val wrappedQuery = new CustomizedScoreQuery(termQuery, indexReader)\n    val topDocs = indexSearch.search(wrappedQuery, 5)\n    ...\n\n}\n
848	Document doc=new Document();\n//Note, changed these field names.\ndoc.add(new Field("title", event.getTitle(), Field.Store.YES, Field.Index.ANALYZED));\ndoc.add(new Field("place", event.getPlace(), Field.Store.YES, Field.Index.ANALYZED));\ndoc.add(new Field("description", event.getDescription(), Field.Store.YES, Field.Index.ANALYZED));\ndoc.add(new Field("table", yourTableName, Field.Store.YES, Field.Index.NO));\n
849	BooleanClause firstClause = boolQuery.clauses.get(0);\n//change the first clause to use a different query.\nfirstClause.setQuery(myNewSubquery);\n//Set the BooleanClause.Occur, if you wish\nfirstClause.setOccur(BooleanClause.Occur.MUST);\n
850	terms: while ((currentTerm = iter.next()) != null) {\n\n  String word = currentTerm.utf8ToString();\n  int len = word.length();\n  if (len &lt; 3) {\n    continue; // too short we bail but "too long" is fine...\n  }\n\n  if (!isEmpty) {\n    for (TermsEnum te : termsEnums) {\n      if (te.seekExact(currentTerm)) {\n        continue terms;\n      }\n    }\n  }\n\n  // ok index the word\n  Document doc = createDocument(word, getMin(len), getMax(len));\n  writer.addDocument(doc);\n
851	function (doc) {\n    if (doc.type = 'myType') {\n        emit(doc.name, doc.name);\n    }\n}\n
852	&lt;field name="key_phrases" type="key_phrases" indexed="true" stored="false" multiValued="true"/&gt;\n&lt;copyField source="content" dest="key_phrases"/&gt;\n&lt;fieldType name="key_phrases" class="solr.TextField" sortMissingLast="true" omitNorms="true"&gt;\n&lt;analyzer&gt;\n  &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;\n    &lt;filter class="solr.ShingleFilterFactory"\n            minShingleSize="2" maxShingleSize="5"\n            outputUnigramsIfNoShingles="true"\n    /&gt;\n    &lt;filter class="solr.KeepWordFilterFactory"\n            words="keep_phrases.txt" ignoreCase="true"/&gt;\n    &lt;filter class="solr.LowerCaseFilterFactory"/&gt;\n  &lt;/analyzer&gt;\n&lt;/fieldType&gt;\n
853	Analyzer analyzer = new SnowballAnalyzer(Lucene.Net.Util.Version.LUCENE_30, "English")\n
854	public void commit() throws Exception {\n    indexWriter.commit();\n    if (indexReader != null)\n        indexReader.close();\n    indexReader = DirectoryReader.open(this.indexWriter, true);\n}\n
855	&lt;fieldtype name="subword" class="solr.TextField"&gt;\n      &lt;analyzer type="query"&gt;\n          &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;\n          &lt;filter class="solr.WordDelimiterFilterFactory"\n                generateWordParts="1"\n                generateNumberParts="1"\n                catenateWords="0"\n                catenateNumbers="0"\n                catenateAll="0"\n                preserveOriginal="1"\n                /&gt;\n          &lt;filter class="solr.LowerCaseFilterFactory"/&gt;\n          &lt;filter class="solr.StopFilterFactory"/&gt;\n      &lt;/analyzer&gt;\n      &lt;analyzer type="index"&gt;\n          &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;\n          &lt;filter class="solr.WordDelimiterFilterFactory"\n                generateWordParts="1"\n                generateNumberParts="1"\n                catenateWords="1"\n                catenateNumbers="1"\n                catenateAll="0"\n                preserveOriginal="1"\n                /&gt;\n          &lt;filter class="solr.LowerCaseFilterFactory"/&gt;\n          &lt;filter class="solr.StopFilterFactory"/&gt;\n      &lt;/analyzer&gt;\n    &lt;/fieldtype&gt;\n
856	QueryParser parser = new QueryParser("text", new StandardAnalyzer());\nQuery query = parser.parse("default", "myfield:ID:123456:789");\n
857	using System;\nusing System.Linq;\nusing System.Collections.Generic;\nusing Microsoft.VisualStudio.TestTools.UnitTesting;\n\nusing Lucene.Net.Search;\nusing Lucene.Net.Index;\nusing Lucene.Net.Analysis.Standard;\nusing Lucene.Net.QueryParsers;\nusing Lucene.Net.Documents;\nusing Lucene.Net.Store;\n\nnamespace SO_answers\n{\n    [TestClass]\n    public class UnitTest1\n    {\n        [TestMethod]\n        public void TestShopping()\n        {\n            var item = new Dictionary&lt;string, string&gt;\n            {\n                {"field1", "value1" },\n                {"field2", "value2" },\n                {"field3", "value3" }\n            };\n\n            var writer = CreateIndex();\n            Add(writer, item);\n            writer.Flush(true, true, true);\n\n            var searcher = new IndexSearcher(writer.GetReader());\n            var result = Search(searcher, item);\n\n            Assert.AreEqual(1, result.Count);\n\n            writer.Dispose();\n        }\n\n        private List&lt;string&gt; Search(IndexSearcher searcher, Dictionary&lt;string, string&gt; values)\n        {\n            var query = new BooleanQuery();\n            foreach (var termQuery in values.Select(kvp =&gt; new TermQuery(new Term(kvp.Key, kvp.Value.ToLowerInvariant()))))\n                query.Add(new BooleanClause(termQuery, Occur.MUST));\n\n            return Search(searcher, query);\n        }\n\n        private List&lt;string&gt; Search(IndexSearcher searcher, Query query)\n        {\n            var sortField = new SortField("creationDate", SortField.LONG, true);\n            var inverseSort = new Sort(sortField);\n            var results = searcher.Search(query, null, 100, inverseSort); // exception thrown here\n\n\n            var result = new List&lt;string&gt;();\n            var matches = results.ScoreDocs;\n            foreach (var item in matches)\n            {\n                var id = item.Doc;\n                var doc = searcher.Doc(id);\n                result.Add(doc.GetField("creationDate").StringValue);\n            }\n            return result;\n        }\n\n        IndexWriter CreateIndex()\n        {\n            var directory = new RAMDirectory();\n\n            var analyzer = new StandardAnalyzer(Lucene.Net.Util.Version.LUCENE_30);\n            var writer = new IndexWriter(directory, analyzer, new IndexWriter.MaxFieldLength(1000));\n\n            return writer;\n        }\n        void Add(IndexWriter writer, IDictionary&lt;string, string&gt; values)\n        {\n            var document = new Document();\n            foreach (var kvp in values)\n                document.Add(new Field(kvp.Key, kvp.Value.ToLowerInvariant(), Field.Store.YES, Field.Index.ANALYZED));\n            document.Add(new NumericField("creationDate", Field.Store.YES, true).SetLongValue(DateTime.UtcNow.Ticks));\n\n            writer.AddDocument(document);\n        }\n    }\n}\n
858	{!xmlparser}\n&lt;SpanNot fieldName="headline"&gt;\n  &lt;Include&gt;\n    &lt;SpanTerm&gt;new&lt;/SpanTerm&gt;\n  &lt;/Include&gt;\n  &lt;Exclude fieldName="headline"&gt;\n    &lt;SpanTerm&gt;york&lt;/SpanTerm&gt;\n  &lt;/Exclude&gt;\n  &lt;Pre&gt;0&lt;/Pre&gt;\n  &lt;Post&gt;1&lt;/Post&gt;\n&lt;/SpanNot&gt;\n
859	https://[service name].search.windows.net/indexes/[index name]?api-version=[api-version]\n
860	hibernate.search.default.directory_provider = filesystem\nhibernate.search.default.indexBase = /usr/lucene/indexes\n
861	 &lt;charFilter class="solr.PatternReplaceCharFilterFactory" pattern="([^a-z])" replacement=""/&gt;\n&lt;tokenizer class="solr.PatternTokenizerFactory" pattern=";\s*" /&gt;\n&lt;filter class="solr.PatternReplaceFilterFactory" pattern="([^a-z])" replacement="" replace="all"/&gt;\n
862	FullTextEntityManager ftem=Search.getFullTextEntityManager(entityManager);\nftem.createIndexer().startAndWait();\n
863	FullTextQuery fullTextQuery =  ftem.createFullTextQuery(query, Kash.class);\nfullTextQuery.setSort(new Sort(new SortField("yes.YeshName_for_sort", SortField.STRING, true)));\n
864	var sort = new Sort(new SortField(TimestampFieldName, SortField.LONG, true))\n
865	Field idField = new StringField("document_id", idOne);\ndoc.add(idField);\n
866	&lt;field name="text_en" type="text_general" indexed="true" stored="true"/&gt;\n
867	defType=edismax&amp;q=exactName:"Bob Rivers"^100 bob rivers&amp;q.op=OR\n
868	QueryParser parser = new QueryParser(VERSION, "text", new StandardAnalyzer(VERSION));\nQuery query = parser.Parse("\"" + query.keywords + "\"");\n
869	CollectionStatistics collectionStats = indexSearcher.collectionStatistics("Body");\nlong token_count = collectionStats.sumTotalTermFreq();\n
870	QueryParser parser = new QueryParser("Body", new EnglishAnalyzer());\nQuery query = parser.parse(topic);\nTopDocs hits = iSearcher.search(query, 1000);\nfor (int i=0; i&lt;hits.scoreDocs.length; i++){\n     Terms termVector = iSearcher.getIndexReader().getTermVector(hits.scoreDocs[i].doc, "Body");\n     Document doc = iSearcher.doc(hits.scoreDocs[i].doc);\n     documentsList.put(doc, termVector);\n}\n
871	&lt;analyzer type="index"&gt;\n      &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;\n      &lt;filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt" /&gt;\n      &lt;filter class="solr.LowerCaseFilterFactory"/&gt;\n    &lt;/analyzer&gt;\n
872	import org.apache.lucene.search.similarities.BasicStats;\nimport org.apache.lucene.search.similarities.SimilarityBase;\n\npublic class MySimilarity extends SimilarityBase {\n\n    @Override\n    protected float score(BasicStats stats, float termFreq, float docLength) {\n        double tf = 1 + (Math.log(termFreq) / Math.log(2));\n        double idf = Math.log((stats.getNumberOfDocuments() + 1) / stats.getDocFreq()) / Math.log(2);\n        float dotProduct = (float) (tf * idf);\n        return dotProduct;\n    }\n\n}\n
873	Query fieldOccursQuery = new TermQuery(new Term(queryFieldName, "[* TO *]"));\n
874	&lt;searchComponent name="suggest" class="solr.SuggestComponent"&gt;\n  &lt;lst name="suggester"&gt;\n    &lt;str name="name"&gt;mySuggester&lt;/str&gt;\n    &lt;str name="lookupImpl"&gt;FuzzyLookupFactory&lt;/str&gt;     \n    &lt;str name="dictionaryImpl"&gt;DocumentDictionaryFactory&lt;/str&gt;     \n    &lt;str name="field"&gt;cat&lt;/str&gt;\n    &lt;str name="weightField"&gt;price&lt;/str&gt;\n    &lt;str name="suggestAnalyzerFieldType"&gt;string&lt;/str&gt;\n  &lt;/lst&gt;\n  &lt;lst name="suggester"&gt;\n    &lt;str name="name"&gt;altSuggester&lt;/str&gt;\n    &lt;str name="dictionaryImpl"&gt;DocumentExpressionDictionaryFactory&lt;/str&gt;\n    &lt;str name="lookupImpl"&gt;FuzzyLookupFactory&lt;/str&gt;\n    &lt;str name="field"&gt;product_name&lt;/str&gt;\n    &lt;str name="weightExpression"&gt;((price * 2) + ln(popularity))&lt;/str&gt;\n    &lt;str name="sortField"&gt;weight&lt;/str&gt;\n    &lt;str name="sortField"&gt;price&lt;/str&gt;\n    &lt;str name="storeDir"&gt;suggest_fuzzy_doc_expr_dict&lt;/str&gt;\n    &lt;str name="suggestAnalyzerFieldType"&gt;text_en&lt;/str&gt;\n  &lt;/lst&gt; \n&lt;/searchComponent&gt;\n
875	&lt;str name="mlt.fl"&gt;one, two, three &lt;/str&gt;\n\n    &lt;str name="mlt.mintf"&gt;2&lt;/str&gt;\n\n    &lt;str name="rows"&gt;10&lt;/str&gt;\n\n    &lt;str name="mlt.mindf"&gt;2&lt;/str&gt;\n\n    &lt;str name="mlt.boost"&gt;true&lt;/str&gt;\n
876	sql_query = SELECT Objects.ID, Objects.Name AS name, \n        GROUP_CONCAT(Tags.Name) AS tags\n        FROM Objects\n        LEFT JOIN Composite ON (Objects.ID = ObjID) \n        LEFT JOIN Tags ON (Tags.ID = TagID)\n        GROUP BY Objects.ID ORDER BY NULL\n
877	        IndexReader reader = DirectoryReader.open(dir);\n        final Fields fields = MultiFields.getFields(reader);\n        final Iterator&lt;String&gt; iterator = fields.iterator();\n\n        long maxFreq = Long.MIN_VALUE;\n        String freqTerm = "";\n        while(iterator.hasNext()) {\n            final String field = iterator.next();\n            final Terms terms = MultiFields.getTerms(reader, field);\n            final TermsEnum it = terms.iterator();\n            BytesRef term = it.next();\n            while (term != null) {\n                final long freq = it.totalTermFreq();\n                if (freq &gt; maxFreq) {\n                    maxFreq = freq;\n                    freqTerm = term.utf8ToString();\n                }\n                term = it.next();\n            }\n        }\n\n        System.out.println(freqTerm + " " + maxFreq);\n
878	&amp;sort=ms(2012-12-01T18:37:36.282Z, myDateField_dt) asc\n
879	        IndexReader reader = DirectoryReader.open(dir);\n        final Fields fields = MultiFields.getFields(reader);\n        final Iterator&lt;String&gt; iterator = fields.iterator();\n\n        while(iterator.hasNext()) {\n            final String field = iterator.next();\n            final Terms terms = MultiFields.getTerms(reader, field);\n            final TermsEnum it = terms.iterator(null);\n            BytesRef term = it.next();\n            while (term != null) {\n                System.out.println(term.utf8ToString());\n                term = it.next();\n            }\n        }\n
880	CREATE KEYSPACE test with replication = {\n    'class' : 'SimpleStrategy', 'replication_factor' : '1' \n}; \n\nCREATE TABLE test.users (\n    id bigint PRIMARY KEY,\n    name text,\n    surname text\n);\n\nCREATE CUSTOM INDEX test_users_idx ON test.users() \nUSING 'com.stratio.cassandra.lucene.Index'\nWITH OPTIONS = {\n    'refresh_seconds': '1',\n    'schema': '{\n        fields: {\n            name: {type: "string"},\n            surname:{type:"string"}\n        }\n    }'\n};\n\nINSERT INTO test.users(id, name, surname) VALUES (1, 'Hans', 'Albers');\nINSERT INTO test.users(id, name, surname) VALUES (2, 'Quintina', 'Koch');\nINSERT INTO test.users(id, name, surname) VALUES (3, 'Orlando', 'Schwarz');\nINSERT INTO test.users(id, name, surname) VALUES (4, 'Federico', 'Hans');\nINSERT INTO test.users(id, name, surname) VALUES (5, 'Berenice', 'Schwarz');\nINSERT INTO test.users(id, name, surname) VALUES (6, 'Zaida', 'Koch');\n\nSELECT * FROM test.users WHERE expr(test_users_idx,'{\n    query: {\n        type : "boolean", \n        should : [\n            {type: "match", field: "name", value: "Hans", boost: 1.5},\n            {type: "match", field: "surname", value: "Hans", boost: 1.0}\n        ]\n    }\n}');\n
881	javax.persistence.Query persistenceQuery = fullTextEntityManager.createFullTextQuery(query, typeParameterClass);\nquery.setProjection(ProjectionConstants.ID, [other fields...])\npersistenceQuery.setMaxResults(100);\n
882	?q=*&amp;wt=json&amp;rows=1000&amp;fq={!geofilt%20pt=36.722484,-4.371908%20sfield=location%20d=50}&amp;sort=geodist()+asc\n
883	FieldType type = new FieldType();\ntype.setTokenized(true);\ntype.setStoreTermVectors(true);\n...\ndocument.add(new Field("fieldName", someString, type));\n
884	@AnalyzerDef(name = "sort_analyzer",\n   tokenizer = @TokenizerDef(factory = KeywordTokenizerFactory.class),\n   filters = {\n       @TokenFilterDef(factory = ASCIIFoldingFilterFactory.class),\n       @TokenFilterDef(factory = LowerCaseFilterFactory.class)\n   }\n)\npublic class FeatureMeta {\n\n    @Id\n    @GeneratedValue(strategy=GenerationType.AUTO)\n    private Long id;\n\n    @Column(unique=true)\n    private String uri;\n\n    @Column\n    @Field\n    @Field(name = "name_sort", analyzer = @Analyzer(definition = "sort_analyzer"))\n    private String name;\n\n    @Field\n    @Column\n    private String businessDesc;\n\n    @Field\n    @Column\n    private String logicalDesc;\n\n    .\n    .\n\n}\n
885	public class IgnoreStopWordsAnalyzer extends StopwordAnalyzerBase {\n\n    public IgnoreStopWordsAnalyzer() {\n        super(Version.LUCENE_36, null);\n    }\n\n    @Override\n    protected ReusableAnalyzerBase.TokenStreamComponents createComponents(final String fieldName, final Reader reader) {\n        final StandardTokenizer src = new StandardTokenizer(Version.LUCENE_36, reader);\n        TokenStream tok = new StandardFilter(Version.LUCENE_36, src);\n        tok = new LowerCaseFilter(Version.LUCENE_36, tok);\n        tok = new StopFilter(Version.LUCENE_36, tok, this.stopwords);\n        return new ReusableAnalyzerBase.TokenStreamComponents(src, tok);\n    }\n}\n
886	q = "calories:[0 TO 300]",\nsort = "abs(sub(100,calories)) asc"\n
887	bf=div(1,abs(sub(100,calories)))\ndefType=edismax\nq=(food_group:"Proteins"^100 OR food_group:"Dairy" OR food_group:"Grains")\n
888	q=name:"Bob Smith" AND description:"database expert" AND ( owner:"glynn" OR group_owner:"staff" OR world_owner:"everyone")\n
889	...\ngetParameters : function Contract_getParameters() {\n    var param = "query=";\n    var query = "{\"datatype\":\"some-prefix:contract\",";\n    if (this.widgets.filter.value == "inactive")\n        query += "\"prop_some-prefix_isCoordinated\":\"false\"}";\n    else\n        query += "\"prop_some-prefix_isCoordinated\":\"true\"}";\n    param += encodeURIComponent(query);\n\n    return param;\n},\n...\n
890	TopDocs hits = searcher.search(new WildcardQuery(new Term(AppConstants.NAME, "*")), 20);\n\n            if (null == hits.scoreDocs || hits.scoreDocs.length &lt;= 0) {\n                System.out.println("No Hits Found");\n                return;\n            }\n\n    for (ScoreDoc hit : hits.scoreDocs) {\n        Document doc = searcher.doc(hit.doc);\n        .......build a list of names or whatever or cross check with DB table etc etc......\n    }\n
891	IndexHits&lt;Node&gt; hits = graphDb.index().forNodes("node_auto_index").query("title:wars");\nwhile (hits.hasNext()) {\n  Node node = hits.next();\n  float weight = hits.currentScore();\n}\n
892	 Map = revisions =&gt; from revision in revisions\n                    where (revision.AuditedType == "typeA")\n                    select\n                    new\n                    {\n                        revision.ChangeTimestamp,\n                        Changes_Which = revision.Changes.Select(chnage =&gt; change.Which)\n                    };\n
893	q=*:* AND -_query_:"{!geofilt d=100}"&amp;pt=41.93825,-93.38989&amp;sfield=site_opplocation\n
894	solrClient = new HttpSolrClient.Builder("http://localhost:8983/solr/swcm").build();\n
895	Terms terms = MultiFields.getTerms(indexReader, "text");\nTermsEnum termsEnum = terms.iterator();\nList&lt;Term&gt; matchingTerms = new ArrayList&lt;Term&gt;();\ntermsEnum.seekCeil(new BytesRef("app"));\nwhile (termsEnum.term().utf8ToString().startsWith("app")) {\n    matchingTerms.add(new Term("text", termsEnum.term()));\n    termsEnum.next();\n}\nSystem.out.println(matchingTerms);\n
896	@Entity @Indexed\npublic class Child {\n...\n\n\nQuery searchQuery = queryBuilder.keyword().onField("parent.name").matching("test").createQuery();\nFullTextQuery fullTextQuery = fullTextEntityManager.createFullTextQuery(searchQuery, Child.class);\nfullTextQuery.setProjection("name");\n
897	    private static class CustomAnalyzer extends Analyzer{\n    @Override\n    protected TokenStreamComponents createComponents(String fieldName) {\n        final int flags = GENERATE_WORD_PARTS|CATENATE_WORDS|CATENATE_NUMBERS|CATENATE_ALL|PRESERVE_ORIGINAL;\n        Tokenizer tokenizer = new StandardTokenizer();\n        return new TokenStreamComponents(tokenizer,new WordDelimiterGraphFilter(tokenizer, flags, null ));\n    }\n}\n
898	# retrieve all those that have a lowercase letter followed by a uppercase letter\nq=email:/.*[a-z][A-Z].*/\n\n# retrieve all those that have a uppercase letter followed by a lowercase letter\nq=email:/.*[A-Z][a-z].*/\n
899	magnolia.repositories.jackrabbit.config=WEB-INF/config/repo-conf/jackrabbit-bundle-derby-search.xml\n
900	TokenStream tokenStream = new StandardAnalyzer().tokenStream("field",new StringReader(textFile));\n
901	return (doc['field'].value != null? doc['field'].value.length(): 0);\n
902	# pseudo code\ndocument = {files: [], content: []}\n\nfor file in files:\n    document[files].append(file.name)\n\n    tika = solr.tika(extractOnly=true, read(file.name))\n    document[content].append(tika[content])\n\nsolr.add(document)\nsolr.commit()\n
903	export JCC_CFLAGS="-v;-fno-strict-aliasing;-Wno-write-strings;-D__STDC_FORMAT_MACROS"\n
904	baseDir="\\CLDServer2\RemoteK2Depot"\n
905	    fq={!frange l=2 u=3}geodist()\n
906	AnalyzingSuggester suggester = new AnalyzingSuggester(dir, "sugest", new StandardAnalyzer(CharArraySet.EMPTY_SET));\nsuggester.build(new HighFrequencyDictionary(indexReader, "tokens", 0));\nList&lt;LookupResult&gt; lookupResults = suggester.lookup("a", false, 20);\n
907	FullTextEntityManager em = /*...*/;\nAnalyzer analyzer = em.getSearchFactory()\n    .getAnalyzer(Document.class);\nhighlightText(query, analyzer, fieldName, text);\n
908	@Field(analyze = Analyze.NO, store = Store.YES)\n@IndexedEmbedded\nprivate Set&lt;String&gt; keywords = new HashSet&lt;String&gt;();\n
909	//This is what the default stop word list is in case you want to use or filter this\nvar defaultStopWords = StopAnalyzer.ENGLISH_STOP_WORDS_SET;\n\n//create a new StandardAnalyzer with custom stop words\nvar sa = new StandardAnalyzer(\n    Version.LUCENE_29, //depends on your version\n    new HashSet&lt;string&gt; //pass in your own stop word list\n    {\n        "hello",\n        "world"\n    });\n
910	@TokenFilterDef(\n    factory = SynonymFilterFactory.class,\n    params = {\n        @Parameter(name = "ignoreCase", value = "true"),\n        @Parameter(name = "expand", value = "true"),\n        @Parameter(name = "synonyms", value = "synonymsfile"),\n        @Parameter(name = "format", value = "wordnet") // Add this\n    }\n)\n
911	.../suggest?suggest=true&amp;suggest.q=name&amp;suggest.dictionary=suggest_nl\n
912	q=Summer&amp;defType=edismax&amp;qf=series\n
913	&lt;field name="content_type" type="text_general"&gt;\n
914	fq=start_date:[NOW/DAY-10DAYS TO NOW]\n
915	QueryBuilder animalQueryBuilder = fullTextEntityManager.getSearchFactory().buildQueryBuilder().forEntity( Animal.class ).get();\nQueryBuilder breederQueryBuilder = fullTextEntityManager.getSearchFactory().buildQueryBuilder().forEntity( Breeder.class ).get();\n\nQuery locationQuery = breederQueryBuilder.spatial()\n    .onField( "breederLocation" )\n    .within( 10.0, DistanceUnit.KM )\n    .ofLatitude( 42.0 ).andLongitude( 42.0 )\n    .createQuery();\nQuery speciesQuery = animalQueryBuilder.keyword()\n    .onField( "species" )\n    .matching( &lt;some species&gt; );\nQuery termsQuery = animalQueryBuilder.simpleQueryString()\n    .onFields(\n        "animalName", "breeder.company.legalName",\n        "breeder.firstName", "breeder.lastName"\n    )\n    .withAndAsDefaultOperator()\n    .matching( "Boston Terrier" );\n\nQuery booleanQuery = animalQueryBuilder.bool()\n    .must( termsQuery )\n    .must( animalQueryBuilder.bool()\n        .should( locationQuery )\n        .should( speciesQuery )\n        .createQuery()\n    )\n    .createQuery();\n\nFullTextQuery fullTextQuery = fullTextEntityManager.createFullTextQuery( booleanQuery, Animal.class, Breeder.class );\n
916	Analyzer anal = new StandardAnalyzer();\nTokenStream ts = anal.tokenStream("title", query); // string query\nCharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\nts.reset();\nwhile (ts.incrementToken()) {\n    System.out.println(termAtt.toString());\n}\nanal.close();\n
917	List&lt;Person&gt; myQueryMethod(&lt;some params&gt;) {\n    FullTextEntityManager em = ...;\n    Query luceneQuery = ...;\n    FullTextQuery query = em.createFullTextQuery( luceneQuery, WriteablePerson.class );\n    query.setProjection( org.hibernate.search.engine.ProjectionConstants.ID );\n    List&lt;Object[]&gt; projections = query.getResultList();\n    return loadResults( Person.class, projections );\n}\n\n&lt;T&gt; List&lt;T&gt; loadResults(Class&lt;T&gt; clazz, List&lt;Object[]&gt; idProjections) {\n    List&lt;Serializable&gt; ids = new ArrayList&lt;&gt;( idProjections.size() );\n    for ( Object[] projection : idProjections ) {\n        ids.add( (Serializable) projection[0] );\n    }\n    return em.unwrap( Session.class ).byMultipleIds( clazz )\n        .with( CacheMode.&lt;pick a cache mode&gt; ) // May be ommitted\n        .withBatchSize( &lt;pick a batch size&gt; ) // May be ommitted\n        .multiLoad( ids );\n}\n
918	BooleanQuery bothQuery = new BooleanQuery();\n\n                                         // field, value\nTermQuery idQuery1 = new TermQuery(new Term("ID1", "id1"));\nTermQuery idQuery2 = new TermQuery(new Term("ID2", "id2"));\n\nbothQuery.add(new BooleanClause(idQuery1, BooleanClause.Occur.MUST));\nbothQuery.add(new BooleanClause(idQuery2, BooleanClause.Occur.MUST));\n\nTopDocs topDocs = searcher.search(bothQuery, 1);\nDocument doc = searcher.doc(topDocs.scoreDocs[0].doc)\n
919	var docs = search.luceneSearch("@cm\\:name:\"png\"");\n
920	&lt;fieldType name="text" class="solr.TextField"&gt;\n  &lt;analyzer&gt;\n    &lt;tokenizer class="solr.SimplePatternTokenizerFactory" pattern="[^,]+"/&gt;\n  &lt;/analyzer&gt;\n&lt;/fieldType&gt;\n
921	 @Entity\n @Indexed\n public class C {\n     @Id\n     @GeneratedValue\n     private Long id;\n\n     @OneToOne(mappedBy = "a")\n     // you might need a prefix if some fields are common between a and b\n     @IndexedEmbedded\n     private A a;\n\n     @OneToOne(mappedBy = "b")\n     // you might need a prefix if some fields are common between a and b\n     @IndexedEmbedded\n     private B b;\n }\n
922	Tokenizer tokenizer = new KeywordTokenizer(reader);\nTokenStream result = new StopFilter(GlobalVariables.LuceneVersion, tokenizer, StopAnalyzer.ENGLISH_STOP_WORDS_SET);\nCharArraySet stopWords = new CharArraySet(GlobalVariables.LuceneVersion, 1, true)\nresult = new  StopFilter(GlobalVariables.LuceneVersion, result, stopWords);\nresult = new LowerCaseFilter(GlobalVariables.LuceneVersion, result);\nreturn new TokenStreamComponents(tokenizer, result);\n
923	"sort": {\n  "_script": {\n    "order": "desc",\n    "type": "number",\n    "script": {\n      "source": \n         "def m = /my_regex_here/.matcher(doc['description'].value);\n          if(m.matches()) {\n            return 1\n          } else {\n            return 0\n          }"\n    }\n  }\n}\n
924	&lt;fieldType name="text" class="solr.TextField" /&gt;\n
925	foreach ($this-&gt;_terms as $termId =&gt; $term) {\n     if (array_key_exists($termId,$this-&gt;_weights)) {\n         $score += $reader-&gt;getSimilarity()-&gt;tf($this-&gt;_termsFreqs[$termId][$docId]) *\n                   $this-&gt;_weights[$termId]-&gt;getValue() *\n                   $reader-&gt;norm($docId, $term-&gt;field);\n     }\n}\n
926	&lt;fieldtype name="syn" class="solr.TextField"&gt;\n  &lt;analyzer&gt;\n      &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;\n      &lt;filter class="solr.SynonymFilterFactory" synonyms="syn.txt" ignoreCase="true" expand="false"/&gt;\n  &lt;/analyzer&gt;\n&lt;/fieldtype&gt;\n
927	IndexSearcher[] searchers = new IndexSearcher[2];\nsearchers[0] = new IndexSearcher(searchDirOne);\nsearchers[1] = new IndexSearcher(searchDirTwo);\n\nMultiSearcher searcher = new MultiSearcher(searchers);\n\nQuery query = QueryParser.Parse("foo","bar" , new StandardAnalyzer());\n\nHits hits = searcher.Search(query);\n
928	searchable do\n  string :name, :stored =&gt; true\nend\n
929	curl 'http://localhost:8080/solr/update/csv?commit=true&amp;f.features.split=true&amp;f.features.separator=%7C&amp;f.cat.split=true&amp;f.cat.separator=%7C' --data-binary @input.csv -H 'Content-type:text/plain; charset=utf-8'\n
930	    QueryParser qp = new QueryParser("keyWord", new StandardAnalyzer());\n    Query q = qp.Parse("+(keyWord:chicken KeyWord:parmesan) title:\"Chicken Parmesan\"~12^5.0");\n    var hits = searcher.Search(q);\n
931	public class MyAnalyzer extends Analyzer {\n\n    private Set customStopSet; \n    public static final String[] STOP_WORDS = ...;\n\n    private boolean doStemming = false;\n    private boolean doStopping = false;\n\n    public JavaSourceCodeAnalyzer(){\n            super();\n            customStopSet = StopFilter.makeStopSet(STOP_WORDS);\n    }\n\n    public void setDoStemming(boolean val){\n            this.doStemming = val;\n    }\n\n    public void setDoStopping(boolean val){\n            this.doStopping = val;\n    }\n\n    public TokenStream tokenStream(String fieldName, Reader reader) {\n\n            // First, convert to lower case\n            TokenStream out = new  LowerCaseTokenizer(reader);\n\n            if (this.doStopping){\n                    out = new StopFilter(true, out, customStopSet);\n            }\n\n            if (this.doStemming){\n                    out = new PorterStemFilter(out);\n            }\n\n            return out;\n    }\n}\n
932	public class NameDuplicateTests {\n    private Analyzer analyzer;\n    private IndexSearcher searcher;\n    private IndexReader reader;\n    private QueryParser qp;\n\n    private final static Multimap&lt;String, String&gt; firstNameSynonyms;\n    static {\n        firstNameSynonyms = HashMultimap.create();\n        List&lt;String&gt; robertSynonyms = ImmutableList.of("Bob", "Bobby", "Robert");\n        for (String name: robertSynonyms) {\n            firstNameSynonyms.putAll(name, robertSynonyms);\n        }\n        List&lt;String&gt; willSynonyms = ImmutableList.of("William", "Will", "Bill", "Billy");\n        for (String name: willSynonyms) {\n            firstNameSynonyms.putAll(name, willSynonyms);\n        }\n    }\n\n    public static Analyzer createAnalyzer() {\n        return new Analyzer() {\n            @Override\n            public TokenStream tokenStream(String fieldName, Reader reader) {\n                TokenStream tokenizer = new WhitespaceTokenizer(reader);\n                if (fieldName.equals("firstName")) {\n                    tokenizer = new SynonymFilter(tokenizer, new SynonymEngine() {\n                        @Override\n                        public String[] getSynonyms(String s) throws IOException {\n                            return firstNameSynonyms.get(s).toArray(new String[0]);\n                        }\n                    });\n                }\n                return tokenizer;\n            }\n        };\n    }\n\n\n    @Before\n    public void setUp() throws Exception {\n        Directory dir = new RAMDirectory();\n        analyzer = createAnalyzer();\n\n        IndexWriter writer = new IndexWriter(dir, analyzer, IndexWriter.MaxFieldLength.UNLIMITED);\n        ImmutableList&lt;String&gt; firstNames = ImmutableList.of("William", "Robert", "Bobby", "Will", "Anton");\n        ImmutableList&lt;String&gt; lastNames = ImmutableList.of("Robert", "Williams", "Mayor", "Bob", "FunkyMother");\n\n        for (int id = 0; id &lt; firstNames.size(); id++) {\n            Document doc = new Document();\n            doc.add(new Field("id", String.valueOf(id), Field.Store.YES, Field.Index.NOT_ANALYZED));\n            doc.add(new Field("firstName", firstNames.get(id), Field.Store.YES, Field.Index.ANALYZED));\n            doc.add(new Field("lastName", lastNames.get(id), Field.Store.YES, Field.Index.NOT_ANALYZED));\n            writer.addDocument(doc);\n        }\n        writer.close();\n\n        qp = new QueryParser(Version.LUCENE_30, "firstName", new WhitespaceAnalyzer());\n        searcher = new IndexSearcher(dir);\n        reader = searcher.getIndexReader();\n    }\n\n    @After\n    public void tearDown() throws Exception {\n        searcher.close();\n    }\n\n    @Test\n    public void testNameFilter() throws Exception {\n        search("+firstName:Bob +lastName:Williams");\n        search("+firstName:Bob +lastName:Wolliam~");\n    }\n\n    private void search(String query) throws ParseException, IOException {\n        Query q = qp.parse(query);\n        System.out.println(q);\n        TopDocs res = searcher.search(q, 3);\n        for (ScoreDoc sd: res.scoreDocs) {\n            Document doc = reader.document(sd.doc);\n            System.out.println("Found " + doc.get("firstName") + " " + doc.get("lastName"));\n        }\n    }\n}\n
933	public class MinTermLengthTokenFilter : TokenFilter\n{\n    private int minTermLength;\n    private TermAttribute termAtt;\n    public MinTermLengthTokenFilter(int maxTermLength, TokenStream input)\n        : base(input)\n    {\n        this.minTermLength = maxTermLength;\n        termAtt = (TermAttribute)AddAttribute(typeof(TermAttribute));\n    }\n\n    public override bool IncrementToken()\n    {\n        while (input.IncrementToken())\n        {\n            if (termAtt.TermLength() &gt;= minTermLength)\n            {\n                return true;\n            }\n        }\n        return false;\n    }        \n\n}\n\n\npublic class MinTermLengthAnalyzer : StandardAnalyzer\n{\n    private int minTermLength;\n    public MinTermLengthAnalyzer(int minTermLength)\n        :base()\n    {\n        this.minTermLength = minTermLength;\n    }\n\n    public override TokenStream TokenStream(string fieldName, TextReader reader)\n    {   \n        return new MinTermLengthTokenFilter(minTermLength, base.TokenStream(fieldName, reader));\n    }\n\n    public override TokenStream ReusableTokenStream(string fieldName, TextReader reader)\n    {\n        return new MinTermLengthTokenFilter(minTermLength, base.ReusableTokenStream(fieldName, reader));\n\n    }\n}\n
934	FullTextSession fts = Search.getFullTextSession(getSessionFactory().getCurrentSession());\n\nQuery q = fts.getSearchFactory().buildQueryBuilder()\n    .forEntity(Offer.class).get()\n    .keyword()\n    .onField("id")\n    .matching(myId)\n    .createQuery();\nObject[] dId = (Object[]) fts.createFullTextQuery(q, Offer.class)\n    .setProjection(ProjectionConstants.DOCUMENT_ID)\n    .uniqueResult();\n\nif(dId != null){\n\n    IndexReader indexReader = fts.getSearchFactory().getIndexReaderAccessor().open(Offer.class);\n\n    TermFreqVector freq = indexReader.getTermFreqVector((Integer) dId[0], "description");\n\n}\n
935	EntityManager em = entityManagerFactory.createEntityManager();\nFullTextEntityManager fullTextEntityManager = Search.getFullTextEntityManager(em);\nfullTextEntityManager.createIndexer().startAndWait();\n
936	Query mainQuery, filterQuery;\n\nBooleanQuery query = new BooleanQuery();\n\n// add the main query for scoring\nquery.add(mainQuery, Occur.SHOULD);\n\n// prevent the filter query to participate in the scoring\nfilter.setBoost(0);\n// make the filter query required\nquery.add(filterQuery, Occur.MUST);\n
937	/**\n * @author Omnaest\n */\npublic class SolrSimpleIndexingTest\n{\n  protected SolrServer solrServer = newSolrServerInstance();\n\n  @Test\n  public void testSolr() throws IOException,\n                        SolrServerException\n  {\n\n    {\n      SolrInputDocument solrInputDocument = new SolrInputDocument();\n      {\n        solrInputDocument.addField( "id", "0" );\n        solrInputDocument.addField( "text", "test1" );\n      }\n      this.solrServer.add( solrInputDocument );\n    }\n    {\n      SolrInputDocument solrInputDocument = new SolrInputDocument();\n      {\n        solrInputDocument.addField( "id", "1" );\n        solrInputDocument.addField( "text", "test2" );\n      }\n      this.solrServer.add( solrInputDocument );\n    }\n    this.solrServer.deleteByQuery( "text:([* TO *] -test2)" );\n    this.solrServer.commit();\n\n    /*\n     * Now your index does only contain the document with id=1 !!\n     */\n\n    QueryResponse queryResponse = this.solrServer.query( new SolrQuery().setQuery( "*:*" ) );\n    SolrDocumentList solrDocumentList = queryResponse.getResults();\n\n    assertEquals( 1, solrDocumentList.size() );\n    assertEquals( "1", solrDocumentList.get( 0 ).getFieldValue( "id" ) );\n  }\n\n  /**\n   * @return\n   */\n  private static CommonsHttpSolrServer newSolrServerInstance()\n  {\n    try\n    {\n      return new CommonsHttpSolrServer( "http://localhost:8983/solr" );\n    }\n    catch ( MalformedURLException e )\n    {\n      e.printStackTrace();\n      fail();\n    }\n    return null;\n  }\n}\n
938	my $content = "I am the law";\nmy $doc = Plucene::Document-&gt;new;\n$doc-&gt;add(Plucene::Document::Field-&gt;Text(content =&gt; $content));\n$doc-&gt;add(Plucene::Document::Field-&gt;Text(author =&gt; "Philip Johnson"));\n
939	QP qp = new QP("user", new SimpleAnalyzer());\nvar s = qp.Parse("user:RonaldMcDonald data:[aaa TO bbb]");\n
940	IndexWriter writer = new IndexWriter("Path to index here", new WhitespaceAnalyzer());\n
941	curl localhost:9200/_search?fields=id -d '{\n  "query" : {\n    "match_all" : {}\n  }\n}'\n
942	Analyzer analyzer = ...; // your analyzer\nTokenStream tokenStream = analyzer.tokenStream(null, new StringReader(searchText));\n
943	protected Document getDocument(File f) throws Exception {\n  Document doc = new Document();\n  Field contents = new Field("contents", new Scanner(f).useDelimiter("\\A").next(), Store.YES, Index.NO);  // you should actually close the scanner\n  Field filename = new Field("filename", f.getName(), Store.YES, Index.ANALYZED);\n  doc.add(contents);\n  doc.add(filename);\n  return doc;\n}\n
944	+name:$search +(share_level:public (+share_level:friends +friends_ids:$uid))\n\n$search = picnic\n$uid = 3 (jack)\n
945	q=filename_str:/.*123girlfriend.jpg/\n
946	 @Field("cat")\n   public void setCategory(String[] c){\n       this.categories = c;\n   }\n
947	Query q = qb.keyword()\n    .onField( "myField" )\n    .matching( "12345%SEPERATOR%4" )\n    .createQuery();\n
948	1) Query q = new QueryParser(Version.LUCENE_40, "title", analyzer).parse(querystr + "*");\n
949	using System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Text;\nusing System.Threading.Tasks;\nusing Lucene.Net.Analysis;\nusing Lucene.Net.Analysis.Standard;\nusing Lucene.Net.Documents;\nusing Lucene.Net.Index;\nusing Lucene.Net.Search;\nusing Lucene.Net.Store;\n\nnamespace ConsoleApplication2\n{\n    class Program\n    {\n        static void Main(string[] args)\n        {\n            RAMDirectory dir = new RAMDirectory();\n            var perFieldAnalyzer = new PerFieldAnalyzerWrapper(new StandardAnalyzer(Lucene.Net.Util.Version.LUCENE_30));\n            perFieldAnalyzer.AddAnalyzer("ExactTitle", new LowercaseKeywordAnalyzer());\n\n            IndexWriter indexWriter = new IndexWriter(dir, perFieldAnalyzer, IndexWriter.MaxFieldLength.UNLIMITED);\n\n            Document reportDoc = new Document();\n            Field exactTitleField = new Field("ExactTitle", \n                                                "Test Abc", \n                                                Field.Store.NO,\n                                                Field.Index.ANALYZED);\n\n            reportDoc.Add(exactTitleField);\n            indexWriter.AddDocument(reportDoc);\n            indexWriter.Commit();\n\n            IndexSearcher searcher = new IndexSearcher(indexWriter.GetReader());\n\n            var term = new Term("ExactTitle", "test abc"); //note: for this to work this way you need to always lower case the search too\n            var exactQuery = new TermQuery(term);\n\n            var hits = searcher.Search(exactQuery, null, 25, Sort.RELEVANCE);\n            Console.WriteLine(hits.TotalHits); // prints "1"\n            Console.ReadLine();\n\n            indexWriter.Close();\n\n        }\n\n        public class LowercaseKeywordAnalyzer : Analyzer\n        {\n\n            public override TokenStream TokenStream(string fieldName, System.IO.TextReader reader)\n            {\n                TokenStream tokenStream = new KeywordTokenizer(reader);\n                tokenStream = new LowerCaseFilter(tokenStream);\n                return tokenStream;\n            }\n        }\n    }\n}\n
950	var qobject = {\n        query:{\n            custom_score:{\n                query:{\n                    filtered:{\n                        query:{\n                            multi_match:{\n                                query: q,\n                                fields: ['title','description'],\n                            }\n                        },\n                        filter:{\n                            "and" : [\n                                range:{\n                                    price: { from: 0, to: max_price }\n                                },\n                                geo_distance:{\n                                    'distance': distance + 'mi',\n                                    'location':{\n                                        lat: lat,\n                                        lon: lon\n                                    }\n                                }\n                            ]\n                        }\n                    }\n                },\n                script: '_score + _source["price"] * 10'\n            }\n        }\n    }\n
951	    BooleanQuery query = new BooleanQuery();\n    Query origSearch = getOrigSearch(searchString);\n    Query refinement = makeRefinement();\n    query.add(origSearch, Occur.MUST);\n    query.add(refinement, Occur.MUST);\n    TopDocs topDocs = searcher.search(query, maxHits);\n
952	 &lt;fieldType name="uuid" class="solr.UUIDField" indexed="true" /&gt;\n
953	PhraseQuery phrase = new PhraseQuery();\nphrase.add(new Term("textfield", "name"));\nphrase.add(new Term("textfield", "website"));\n
954	&lt;filter class="solr.EdgeNGramFilterFactory" minGramSize="1" maxGramSize="25" side="front"/&gt;\n
955	org.apache.lucene.search.Query searchQuery = qb.bool()\n    .must(qb.keyword().onFields("title", "description").matching(searchContext).createQuery())\n    .must(qb.keyword().onField("inventory.id").matching(inventoryId).createQuery())\n    .createQuery();\n
956	for (int i = 0; i &lt; Queryy.length; i++) {\n    booleanQuery.add(Queryy[i], BooleanClause.Occur.MUST);\n}\n
957	&lt;elevate&gt;\n\n  &lt;query text="AAA"&gt;\n   &lt;doc id="A" /&gt;\n   &lt;doc id="B" /&gt;\n  &lt;/query&gt;\n\n  &lt;query text="ipod"&gt;\n    &lt;doc id="A" /&gt;\n\n        &lt;!-- you can optionally exclude documents from a query result --&gt;\n    &lt;doc id="B" exclude="true" /&gt;\n  &lt;/query&gt;\n\n&lt;/elevate&gt;\n
958	&lt;field name="text" type="text_general" indexed="true" stored="false" multiValued="true"/&gt;\n
959	public float lengthNorm(FieldInvertState state) {\n    final int numTerms;\n    if (discountOverlaps)\n        numTerms = state.getLength() - state.getNumOverlap();\n    else\n        numTerms = state.getLength();\n    return state.getBoost() * ((float) (1.0 / Math.sqrt(numTerms)));\n}\n
960	declare %private function app:filter($node as node(), $mode as xs:string) as xs:string? {\n    let $ignored-elements := doc('/db/system/config/db/apps/shakespeare/collection.xconf')//*:ignore/@qname/string()\n    let $ignored-elements := \n        for $ignored-element in $ignored-elements\n        let $ignored-element := substring-after($ignored-element, ':')\n        return $ignored-element\n    return\n        if (local-name($node/parent::*) = ('speaker', 'stage', 'head', $ignored-elements)) \n        then ()\n        else \n            if ($mode eq 'before') \n            then concat($node, ' ')\n            else concat(' ', $node)\n};\n
961	String q = " START n=node:node_auto_index({ids}) return n  "\nMap&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;();\nmap.put("ids", keyList); \n
962	0.23013961 = (MATCH) weight(NAME:value in 0) [DefaultSimilarity], result of:\n  0.23013961 = fieldWeight in 0, product of:\n    1.0 = tf(freq=1.0), with freq of:\n      1.0 = termFreq=1.0\n    0.30685282 = idf(docFreq=1, maxDocs=1)\n    0.75 = fieldNorm(doc=0)\n\nNAME = VALUE (boost 1.0)\n
963	public static void main(String[] args) throws IOException {\n    StandardAnalyzer analyzer = new StandardAnalyzer(Version.LUCENE_46);\n    String s = "test if #hash has been removed";\n    TokenStream stream = analyzer.tokenStream("field", new StringReader(s));\n    stream.reset();\n    CharTermAttribute termAtt = stream.addAttribute(CharTermAttribute.class);\n    while (stream.incrementToken()) {\n        System.out.println(termAtt.toString());\n    }\n    stream.end();\n    stream.close();\n}\n
964	# add_to_index(cls, index, key, value, entity)\n\nnames = ["Hello", "Bye"]\n\nfor n in names:\n    batch.add_to_index(neo4j.Node, "NAME", "name", n, mynode)\n\nbatch.submit()\n
965	//Assuming you have a lucene default QueryParser and IndexSearcher lying around.\ntry {\n    query = org.apache.lucene.queryparser.surround.parser.QueryParser.parse(queryString);\n} catch (ParseException pe) {\n    query = defaultParser.parse(queryString);\n}\n\nsearcher.search(query, numHits);\n
966	&lt;fieldType name="text_general_shingle" class="solr.TextField"&gt;\n    &lt;analyzer&gt;\n        &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;\n        &lt;filter class="solr.LowerCaseFilterFactory"/&gt;\n        &lt;filter class="solr.ShingleFilterFactory" maxShingleSize="4" outputUnigrams="false"/&gt;\n        &lt;filter class="solr.PatternReplaceFilterFactory" pattern="(\s+)" replacement="" replace="all" /&gt;\n    &lt;/analyzer&gt;\n&lt;/fieldType&gt;\n
967	"mappings" : ["ph=&gt;\s", "qu=&gt;\\u0020"]\n
968	{!frange l=A u=C}fieldname\n
969	include 'Zend/Loader/Autoloader.php';\n$autoloader = Zend_Loader_Autoloader::getInstance();\n$autoloader-&gt;setFallbackAutoloader(true);\n\nZend_Search_Lucene_Analysis_Analyzer::setDefault(\n    new Zend_Search_Lucene_Analysis_Analyzer_Common_Utf8Num_CaseInsensitive());\n@mkdir('/tmp/test-lucene');\n$index = Zend_Search_Lucene::create('/tmp/test-lucene');\n\nforeach (array('root/1/2/3', 'root/1', 'root/3/2/1', 'root/3/2/2') as $path) {\n    $path = str_replace('/', 'ß', $path);\n    $doc = new Zend_Search_Lucene_Document();\n    $doc-&gt;addField(Zend_Search_Lucene_Field::Keyword('path', $path));\n    $index-&gt;addDocument($doc);\n}\n\n$hits = $index-&gt;find(str_replace('/', 'ß', 'path:root/3/2*'));\nforeach($hits as $hit){\n    echo str_replace('ß', '/', $hit-&gt;getDocument()-&gt;getFieldValue('path')) . PHP_EOL;\n}\n
970	File[] files = FILES_TO_INDEX_DIRECTORY.listFiles(); //this is where you set the files to index\n\nSimpleFSDirectory d = new SimpleFSDirectory(FILES_TO_INDEX_DIRECTORY); //here you are setting the index directory\n
971	var dir = new SimpleFSDirectory(new DirectoryInfo(path));\nvar reader = IndexReader.Open(dir, true);\n
972	&lt;dynamicField name="*_sort_seq"  type="string"  indexed="true"  stored="false"  sortMissingLast="true"/&gt; \n
973	public static void searchIndex(String searchString) throws IOException, ParseException {\n    searchString = searchString.trim();\n    if (searchString.length &lt; 1)\n        return;\n    String[] words = searchString.split(" ");\n    if (words.length &gt; 1) {\n        for (String word : words)\n            searchIndex(word);\n    } else {\n          // Do normal stuff here\n    }\n}\n
974	TopDocs hits = indexSearcher.search(...);\nfor (ScoreDoc scoreDoc : hits.scoreDocs) {\n   Document doc = indexSearcher.doc(scoreDoc.doc);\n   if (...) {\n      indexSearcher.getIndexReader().deleteDocument(doc);\n   }\n}\n
975	$from = new Zend_Search_Lucene_Index_Term(20, 'ages');\n$to   = new Zend_Search_Lucene_Index_Term(30, 'ages');\n$query = new Zend_Search_Lucene_Search_Query_Range(\n             $from, $to, true // inclusive\n         );\n$hits  = $index-&gt;find($query);\n
976	@SearchableProperty(index=Index.NOT_ANALYZED)\nprivate String field_name;\n
977	        Collator collator = Collator.getInstance(new Locale("ar"));  \n        ICUCollationKeyAnalyzer analyzer = new\n        ICUCollationKeyAnalyzer(collator);\n        RAMDirectory ramDir = new RAMDirectory();   \n        IndexWriter writer = new IndexWriter(ramDir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);   \n        Document doc = new Document();   \n        doc.add(new Field("content","\u0633\u0627\u0628", Field.Store.YES,Field.Index.ANALYZED));   \n        writer.addDocument(doc);   \n        writer.close();\n
978	function toCanonical($text)\n{\n    $out = $text . ' ';\n    $step = $text;\n\n    $pattern = '/([A-Z])[\s\.-]([A-Z])([^a-z])/';\n    while (preg_match($pattern, $step)) {\n        $step = preg_replace($pattern, '$1$2$3', $step);\n        $out .= $step . ' ';\n    }\n\n    return $out;\n}\n\nfunction createDocument($id, $companyName)\n{\n    $canonicalName = toCanonical($companyName);\n\n    $document = new Zend_Search_Lucene_Document();\n    $document-&gt;addField(Zend_Search_Lucene_Field::UnIndexed('document_id', $id));\n    $document-&gt;addField(Zend_Search_Lucene_Field::Text('companyname', $companyName));\n    $document-&gt;addField(Zend_Search_Lucene_Field::UnStored('companyname_canonical', $canonicalName));\n\n}\n\n$index-&gt;addDocument(createDocument(1, 'XYZ Holdings'));\n$index-&gt;addDocument(createDocument(1, 'X.Y.Z. (Holding) Company'));\n
979	q=+keyword -ur:exclude&amp;rows=20\n
980	Directory index = FSDirectory.open(INDEX_DIR);\n
981	for (ScoreDoc sd : topDocs.scoreDocs) {\n  if (sd.doc == Integer.MAX_VALUE) break;\n  final Document d = searcher.doc(sd.doc);\n  final Fieldable f = d.getFieldable(name);\n  ...\n}\n
982	foreach(file in htmlfiles)\n{\n Document d = new Document();\n d.add(new Field("Category", GetCategoryName(...), Field.Store.YES,  Field.Index.NOT_ANALYZED));\nd.add(new Field("Contents", GetContents(...), Field.Store.YES, Field.Index.ANALYZED));\n\nwriter.addDocument(d);\nwriter.close();\n}\n
983	val config = new IndexWriterConfig(Version.LUCENE_40, ...)\nconfig.setOpenMode(IndexWriterConfig.OpenMode.APPEND)\nval indexWriter = new IndexWriter(FSDirectory.open(...), config)\nindexWriter.forceMerge(1)\nindexWriter.close\n
984	List&lt;ScoreDoc&gt; results = new ArrayList&lt;ScoreDoc&gt;();\n
985	myStdErrLog=/tmp/myProject/myProg.stderr.$(/bin/date +%Y%m%d.%H%M)\nwget -O $outputDir/check_status_update_index.txt ${statusCmd} 2&gt; ${myStdErrLog}\n\nif [[ ! -s ${myStdErrLog} ]] ; then\n   mail -s "error on myProg" me@myself.org &lt; ${myStdErrLog}\nfi\nrm ${myStdErrLog}\n
986	&lt;document name="items"&gt;\n    &lt;entity name="item" query="SELECT A.ID, A.TITLE, A.DESCRIPTION, A.DATE, A.SOURCE, B.COORDINATE , C.NAME FROM ITEM A, LOCATION B, COUNTRY C WHERE A.LOCATION_ID = B.ID AND B.COUNTRY_ID = C.ID"&gt;\n        &lt;field column="ID" name="id" /&gt;\n        &lt;field column="TITLE" name="title" /&gt;\n        &lt;field column="DESCRIPTION" name="description" /&gt;\n        &lt;field column="DATE" name="date" /&gt;\n        &lt;field column="SOURCE" name="source" /&gt;\n        &lt;field column="COORDINATE" name="coordinate" /&gt;     \n\n    &lt;/entity&gt;\n&lt;/document&gt;\n
987	&lt;fieldType name="descendent_path" class="solr.TextField"&gt;\n    &lt;analyzer type="index"&gt;\n        &lt;tokenizer class="solr.PathHierarchyTokenizerFactory" delimiter="|" /&gt;\n    &lt;/analyzer&gt;\n    &lt;analyzer type="query"&gt;\n        &lt;tokenizer class="solr.KeywordTokenizerFactory" /&gt;\n    &lt;/analyzer&gt;\n&lt;/fieldType&gt;\n
988	private static String getTagValue(NodeList list, String name) {\n    for (int i = 0; i &lt; list.getLength(); i++) {\n        Element e = (Element) list.item(i);\n        if (e.getAttribute("name").equals(name)) {\n            return e.getTextContent();\n        }\n    }\n    return null;\n}\n\npublic static void main(String[] args) throws Exception {\n    Document doc = DocumentBuilderFactory.newInstance().newDocumentBuilder()\n            .parse(new File("1.xml"));\n    NodeList fields = doc.getElementsByTagName("field");\n    String country = getTagValue(fields, "Country or Area");\n    String year = getTagValue(fields, "Year");\n}\n
989	TermsEnum termsEnum = atomicReader.terms("fieldName").iterator();\nBytesRef text = new BytesRef("searchTerm");\nif (termsEnum.seekExact(text, true)) {\n  ...\n}\n
990	curl -XPOST localhost:9200/crunchbase/person/1/_update -d '{\n    "script" : "ctx._source.field10 = ctx._source.field10"\n}'\n
991	PhraseQuery titleStartsWithSearchString = new PhraseQuery();\ntitleStartsWithSearchString.add(new Term("title","STARTS_WITH".toLowerCase())\ntitleStartsWithSearchString.add(new Term("title",searchString));\n
992	private final TermAttribute termAtt; // instance variable\n\ntermAtt = addAttribute(TermAttribute.class); // initialization in constructor \n\n....\n\n\n else if (bufferLength &gt; 1 &amp;&amp; buffer[0] == ',')\n        {\n\n            // strip the starting , off !\n            offsetAtt.SetOffset(offsetAtt.StartOffset + 1, offsetAtt.EndOffset);\n\n        // update the termAtt\n            termAtt.setTermBuffer("sub-content of the buffer");\n\n        }\n\n....\n
993	ps3 =&gt; playstation 3, ps 3\nps2 =&gt; playstation 2, ps 2\n
994	NumericField letterDateField = new NumericField("date_of_letter",Field.Store.YES,true);\nletterDateField.setLongValue(rs.getDate("DATE_OF_LETTER").getTime());\ndoc.add(letterDateField);\nNumericField receivedDateField = new NumericField("date_received",Field.Store.YES,true);\nreceivedDateField.setLongValue(rs.getDate("DATE_RECEIVED").getTime());\ndoc.add(receivedDateField);\n
995	Query query = parser1.parser("+sport +category:a")\nTotalHitCountCollector collector = new TotalHitCountCollector();\nsearch.search(query, collector); \nctr = collector.getTotalHits();\nquery = parser1.parser("+sport +category:b")\ncollector = new TotalHitCountCollector();\nsearch.search(query, collector); \nctr1 = collector.getTotalHits();\n
996	for (FieldInfo info : atomicReader.getFieldInfos().iterator()) {\n    String name = info.name;\n    //Whatever you need to do with the name.\n}\n
997	Criteria criteriaA = etSessionFactory().getCurrentSession().\n            createCriteria(EntityClassA.class);\ncriteriaA.add(Restrictions.ilike("id", keywords, MatchMode.ANYWHERE));\ncriteriaA.add(Restrictions.ilike("name", keywords, MatchMode.ANYWHERE));\ncriteriaA.add(Restrictions.ilike("age", keywords, MatchMode.ANYWHERE));\n\nCriteria criteriaB = etSessionFactory().getCurrentSession().\n            createCriteria(EntityClassB.class);\ncriteriaB.add(Restrictions.ilike("smth", keywords, MatchMode.ANYWHERE));\ncriteriaB.add(Restrictions.ilike("rows", keywords, MatchMode.ANYWHERE));\ncriteriaB.add(Restrictions.ilike("here", keywords, MatchMode.ANYWHERE));\n
998	BooleanQuery booleanQuery = new BooleanQuery()\nBooleanQuery searchTermQuery = new BooleanQuery();\nforeach (var searchTerm in searchTerms)\n{\n    var searchTermSegments = searchTerm.Split(new[] { " " }, StringSplitOptions.RemoveEmptyEntries);\n    if (searchTermSegments.Count() &gt; 1)\n    {\n        searchTermQuery.Clauses().Clear();\n        foreach (var SegTex in searchTermSegments)\n        {\n            searchTermQuery.Add( new FuzzyQuery(new Term("FieldName", SegTex.ToLower().Trim())),BooleanClause.Occur.MUST);\n        }\n        booleanQuery.Add(searchTermQuery, BooleanClause.Occur.MUST);\n    }\n    else\n    {\n        booleanQuery.Add(new FuzzyQuery(new Term("FieldName", searchTerm.ToLower().Trim())), BooleanClause.Occur.MUST);\n    }\n}\n
999	SpanFirstQuery query = new SpanFirstQuery(new SpanTermQuery(new Term(field, "box")), 1);\n
1000	{!frange l=188 u=188} sum(price,3)\n
1001	SortField primarySort = new SortField("field", SortField.Type.STRING);\nSort sort = new Sort(primarySort, SortField.FIELD_SCORE);\nsearcher.search(query, hits, sort);\n
1002	 List&lt;Get&gt; rowsToGet= new ArrayList&lt;Get&gt;();\n for (String id:resultsFromLucene)\n       rowsToGet.add(new Get(Bytes.toBytes(id)));  \n Result[] results = htable.get(rowsToGet);\n
1003	    Query query1 = new PrefixQuery(new Term("login", prefix));\n    booleanQuery.add(query1, BooleanClause.Occur.SHOULD);\n
1004	 QueryParser parser( _T(""), &amp;fAnalyzer);\n f_objQuery = parser.parse(strQuery.c_str());\n
1005	@FieldBridge(impl = com.myco.myapp.CollectionCountBridge.class)\n@ContainedIn\n@OneToMany(cascade=CascadeType.ALL, fetch=FetchType.LAZY, mappedBy="TParent")\npublic Set&lt;TChild&gt; getTChildSet() {\n     return this.TChildSet;\n}\n
1006	public class BuildTermDocumentMatrix {\npublic BuildTermDocumentMatrix(File index, File corpus) throws IOException{\n    reader = DirectoryReader.open(FSDirectory.open(index));\n    searcher = new IndexSearcher(reader);\n    this.corpus = corpus;\n    termIdMap = computeTermIdMap(reader);\n}   \n/**\n*  Map term to a fix integer so that we can build document matrix later.\n*  It's used to assign term to specific row in Term-Document matrix\n*/\nprivate Map&lt;String, Integer&gt; computeTermIdMap(IndexReader reader) throws IOException {\n    Map&lt;String,Integer&gt; termIdMap = new HashMap&lt;String,Integer&gt;();\n    int id = 0;\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms("contents");\n    TermsEnum itr = terms.iterator(null);\n    BytesRef term = null;\n    while ((term = itr.next()) != null) {               \n        String termText = term.utf8ToString();              \n        if (termIdMap.containsKey(termText))\n            continue;\n        //System.out.println(termText); \n        termIdMap.put(termText, id++);\n    }\n\n    return termIdMap;\n}\n\n/**\n*  build term-document matrix for the given directory\n*/\npublic RealMatrix buildTermDocumentMatrix () throws IOException {\n    //iterate through directory to work with each doc\n    int col = 0;\n    int numDocs = countDocs(corpus);            //get the number of documents here      \n    int numTerms = termIdMap.size();    //total number of terms     \n    RealMatrix tdMatrix = new Array2DRowRealMatrix(numTerms, numDocs);\n\n    for (File f : corpus.listFiles()) {\n        if (!f.isHidden() &amp;&amp; f.canRead()) {\n            //I build term document matrix for a subset of corpus so\n            //I need to lookup document by path name. \n            //If you build for the whole corpus, just iterate through all documents\n            String path = f.getPath();\n            BooleanQuery pathQuery = new BooleanQuery();\n            pathQuery.add(new TermQuery(new Term("path", path)), BooleanClause.Occur.SHOULD);\n            TopDocs hits = searcher.search(pathQuery, 1);\n\n            //get term vector\n            Terms termVector = reader.getTermVector(hits.scoreDocs[0].doc, "contents");\n            TermsEnum itr = termVector.iterator(null);\n            BytesRef term = null;\n\n            //compute term weight\n            while ((term = itr.next()) != null) {               \n                String termText = term.utf8ToString();              \n                int row = termIdMap.get(termText);\n                long termFreq = itr.totalTermFreq();\n                long docCount = itr.docFreq();\n                double weight = computeTfIdfWeight(termFreq, docCount, numDocs);\n                tdMatrix.setEntry(row, col, weight);\n            }\n            col++;\n        }\n    }       \n    return tdMatrix;\n}\n}   \n
1007	Query luceneQuery = queryBuilder.phrase().onField("name").sentence(keyword).createQuery();\norg.hibernate.search.FullTextQuery fullTextQuery = fullTextSession.createFullTextQuery(luceneQuery, Computer.class);\n
1008	http://domain-name.com:1981/solr/admin/collections?action=CREATE&amp;name=image&amp;numShards=5&amp;replicationFactor=2&amp;maxShardsPerNode=2\n
1009	val itemsIndex = neo4j.gds.index().forNodes("items")\nitemsIndex.add(itemNode, "user", userUUID)\nitemsIndex.add(itemNode, "item", itemNode.getProperty("uuid"))\nitemsIndex.add(itemNode, "modified", new ValueContext(getProperty("modified").asInstanceOf[Long] ).indexNumeric())\n
1010	&lt;dataConfig&gt;\n&lt;dataSource driver="org.hsqldb.jdbcDriver" url="jdbc:hsqldb:/temp/example/ex" user="sa" /&gt;\n&lt;document name="products"&gt;\n    &lt;entity name="item" query="select * from item"&gt;\n        &lt;field column="ID" name="id" /&gt;\n        &lt;field column="code" name="code" /&gt;\n\n        &lt;entity name="countryName" query="select name from countrytable where item_id='${item.ID}'"&gt;\n            &lt;field name="name" column="description" /&gt;\n        &lt;/entity&gt;\n        &lt;entity name="countryCode" query="select countryCode from countrytable where item_id='${item.ID}'"&gt;              \n        &lt;/entity&gt;\n    &lt;/entity&gt;\n&lt;/document&gt;\n
1011	 var queryParser = new QueryParser(Version.LUCENE_30, "content", analyzer);\n queryParser.AllowLeadingWildcard = true;\n var query = queryParser.Parse( "*" );\n
1012	TermStats[] stats = HighFreqTerms.gethighFreqTerms(reader, 10, "myContentField", new HighFreqTerms.DocFreqComparator());\nfor (TermStats stat : stats) {\n    System.out.println(stat.termtext.utf8ToString() + ",   docfreq:" + stat.docFreq);\n    //Or whatever else you want to do with them...\n}\n
1013	IndexReader reader1 = DirectoryReader.open(directory1);\nIndexReader reader2 = DirectoryReader.open(directory2);\nMultiReader multiReader = new MultiReader(reader1, reader2);\nIndexSearcher searcher = new IndexSearcher(multiReader);\n
1014	docs = solrReply['docs']\n
1015	QueryParser qParser = new QueryParser(Version.LUCENE_36,"file_name",new StandardAnalyzer(Version.LUCENE_36));\nQuery query = qParser.parse(filename);\n
1016	_query_:"{!frange l=1 u=2}termfreq(point_statusses,'FREE')" OR _query_:"{!frange l=2 u=2}termfreq(point_statusses,'IN_USE')"\n
1017	for(Product product: products) {\n    for(Store store: product.stores) {\n        Document doc = new Document();\n        doc.add(new TextField("productName", product.name, Field.Store.YES));\n        doc.add(new FloatField("price", price, Field.Store.YES));\n        doc.add(new TextField("store", store, Field.Store.YES));\n        doc.add(new TextField("location", location, Field.Store.YES));\n    }\n}\n
1018	fl=product_id,score&amp;start=0&amp;q=iphone&amp;json.nl=map&amp;wt=json&amp;fq=store_id:1&amp;rows=100\n
1019	qf=title^10 text\n
1020	private static final class SimpleCollector extends Collector {\n    final Set&lt;Integer&gt; docs = CollectionHelper.newHashSet();\n    int docBase;\n\n    @Override\n    public void setScorer(Scorer scorer) throws IOException {\n        // no scoring\n    }\n\n    @Override\n    public void collect(int doc) throws IOException {\n        docs.add(docBase + doc);\n    }\n\n    @Override\n    public void setNextReader(AtomicReaderContext context) throws IOException {\n        this.docBase = context.docBase;\n    }\n\n    @Override\n    public boolean acceptsDocsOutOfOrder() {\n        return true;\n    }\n\n}\n
1021	&lt;types&gt;\n  ...\n  &lt;fieldType name="random" class="solr.RandomSortField" /&gt;\n  ... \n &lt;/types&gt;\n &lt;fields&gt;\n  ...\n  &lt;dynamicField name="random*" type="random" indexed="true" stored="false"/&gt;\n  ...\n &lt;/fields&gt;\n
1022	Pattern PATTERN = Pattern.compile("(?&gt;\"[^\"]+\"+)|(?&gt;[^ ]+)+");\nMatcher match = PATTERN.matcher(motRecherche);\nmatch.reset();\nint iM = 0;\n\nwhile(match.find()){\n    if(iM &gt; 0){\n        query.append("%20AND%20");\n    }\n\n    String utf_encoded = CommonUtils.escapeSolrQuery(match.group(0));\n    query.append(":"+utf_encoded);\n    iM++;\n}\n
1023	&lt;add&gt;\n  &lt;doc&gt;\n    &lt;field name="URL"&gt;stackoverflow.com&lt;/field&gt;\n    &lt;field name="tag" boost="11.0"&gt;html&lt;/field&gt;\n    &lt;field name="tag" boost="9.0"&gt;php&lt;/field&gt;\n  &lt;/doc&gt;\n&lt;/add&gt;\n
1024	@Query("START n=node:WRInteraction({0}) WHERE n.pubMillis &gt;= {1} AND n.pubMillis &lt;= {2} RETURN n")\nIterable&lt;WRInteraction&gt; getInteractionsByTermAndDateRange(String query,\n        long startMillis, long endMillis);\n
1025	export CFLAGS=-Qunused-arguments\nexport CPPFLAGS=-Qunused-arguments\n
1026	findTroublesomeQuery(BooleanQuery query) {\n    for (BooleanClause clause : query.clauses()) {\n        Query subquery = clause.getQuery()\n        TopDocs docs = searchHoweverYouDo(subquery);\n        if  (doc.totalSize == 0) {\n            //If you want to dig down recursively...\n            if (subquery instanceof BooleanQuery)\n                findTroublesomeQuery(query);\n            else \n                log(query); //Or do whatever you want to keep track of it.\n        }\n    }\n}\n
1027	facet=true&amp;facet.field=state\n
1028	// your db connection should have autocommit=off at this point\ntry {\n  preparedStatement.executeQuery(...);\n\n  indexWriter1.addDocument(...);\n  indexWriter2.addDocument(...);\n\n  indexWriter1.prepareCommit();\n  indexWriter2.prepareCommit();\n  // at this point, chances of Lucene failing are pretty slim\n  // the only thing left for Lucene now is to rename the segments file\n\n  dbConnection.commit();      \n  // if the DB connection commit fails, rollback Lucene writers below\n\n  indexWriter1.commit();\n  indexWriter2.commit();\n} catch (Throwable t) {\n  indexWriter1.rollback();\n  indexWriter2.rollback();\n\n  dbConnection.rollback();\n}\n
1029	createGroups = (item, previousGroup) -&gt;\n  subroutine = (item, previousGroup) -&gt;\n    if typeof item is "object"\n      unless item.operator\n        if !item.left\n          previousGroup.push item  \n        else\n          previousGroup.push item.left\n        previousGroup\n      if item.operator is "AND"\n        currentGroup = subroutine(item.left, previousGroup)\n        currentGroup = subroutine(item.right, currentGroup)\n        currentGroup and groups.push(currentGroup)\n      if item.operator is "OR"\n        currentGroup = subroutine(item.left, previousGroup)\n        groups.push currentGroup\n        currentGroup and subroutine(item.right, [])\n    return\n\n  previousGroup = previousGroup or []\n  subroutine item, previousGroup\n  groups\n\ncreateGroups o\n
1030	Query query = new WildcardQuery(new Term("*", term));\n
1031	CharArraySet stemExclusionSet = new CharArraySet(VERSION, 1, true);\nstemExclusionSet.add("activation");\nEnglishAnalyzer englishAnalyzer = new EnglishAnalyzer(VERSION, STOPWORDS, stemExclusionSet);\n
1032	WikipediaTokenizer x = wtt.testSimple();\nlogger.info(x.hasAttributes());\nx.reset();\nwhile (x.incrementToken() == true) {\n    logger.info("Token found!");\n}\nx.end();\nx.close();\n
1033	TermQuery termQuery = new TermQuery(new Term("priority", "high"));\n
1034	Field contentFieldLower = new Field("content", "", YES, NOT_ANALYZED);\ndocument.add(contentFieldLower);\nField contentField = new Field("content", "", YES, ANALYZED);\ndocument.add(contentField);\nField contentFieldNotAnalysed = new Field("content", "", YES, NOT_ANALYZED);\ndocument.add(contentFieldNotAnalysed);\n
1035	4000000 = 0b1111010000100100000000\n3997696 = 0b1111010000000000000000\n\n2000000 = 0b111101000010010000000\n1966080 = 0b111100000000000000000\n
1036	HttpSolrServer httpSolrServer = new HttpSolrServer("http://localhost:8983/solr/collectionname/");\nQueryResponse response = httpSolrServer.query(new SolrQuery(), METHOD.POST);    \nSolrDocumentList solrDocumentList = queryResponse.getResults();\nsolrDocumentList.getNumFound();\nsolrDocumentList.getStart();\n
1037	bq=title_t:("Hockey Jersey")^100 name:Hockey\ Jersey*^1000\n
1038	Directory dir = &lt;your dir&gt;;\nIndexReader index = DirectoryReader.open(dir);\nIndexSearcher searcher = new IndexSearcher(index);\nsearcher.setSimilarity(new KmancSimilarity());\n\ncontinue your code...\n
1039	String countryCode = ....;  // known in advance\nQueryParser queryParser = new QueryParser(matchVersion, f, a);\nQuery cityNameQuery = queryParser.parse(inputWithCityName);\nQuery countryCodeQuery = queryParser.parse("+countrycode:" + countryCode);\n\nBooleanQuery result = new BooleanQuery();\nresult.add(new BooleanClause(cityNameQuery, MUST));\nresult.add(new BooleanClause(countryCodeQuery, MUST));\n
1040	http://127.0.0.1:5984/_fti/local/db/_design/foo/by_subject?q=%22foo-bar%22\n
1041	  &lt;!-- Similarity is the scoring routine for each document vs. a query.\n   A custom Similarity or SimilarityFactory may be specified here, but \n   the default is fine for most applications.  \n   For more info: http://wiki.apache.org/solr/SchemaXml#Similarity\n--&gt;\n  &lt;!--\n     &lt;similarity class="com.example.solr.CustomSimilarityFactory"&gt;\n       &lt;str name="paramkey"&gt;param value&lt;/str&gt;\n     &lt;/similarity&gt;\n    --&gt;\n
1042	SpellChecker spellchecker = new SpellChecker(spellIndexDirectory);\nspellchecker.indexDictionary(new LuceneDictionary(indexReader, "fieldname"));\nspellchecker.indexDictionary(new LuceneDictionary(indexReader, "anotherfield"));\n
1043	q.alt={!dismax}firstName:Rob lastName:Lewis city:Seattle\n
1044	&lt;component instance-scope="single-instance"\n           type="Orchard.Tasks.SweepGenerator"\n           service="Orchard.Tasks.ISweepGenerator"&gt;\n    &lt;properties&gt;\n        &lt;property name="Interval" value="00:01:00" /&gt;\n    &lt;/properties&gt;\n&lt;/component&gt;\n
1045	      case '\u00C0': // À  [LATIN CAPITAL LETTER A WITH GRAVE]\n      case '\u00C1': // Á  [LATIN CAPITAL LETTER A WITH ACUTE]\n      case '\u00C2': // Â  [LATIN CAPITAL LETTER A WITH CIRCUMFLEX]\n      case '\u00C3': // Ã  [LATIN CAPITAL LETTER A WITH TILDE]\n      case '\u00C4': // Ä  [LATIN CAPITAL LETTER A WITH DIAERESIS]\n      case '\u00C5': // Å  [LATIN CAPITAL LETTER A WITH RING ABOVE]\n      case '\u0100': // Ā  [LATIN CAPITAL LETTER A WITH MACRON]\n      case '\u0102': // Ă  [LATIN CAPITAL LETTER A WITH BREVE]\n      case '\u0104': // Ą  [LATIN CAPITAL LETTER A WITH OGONEK]\n      case '\u018F': // Ə  http://en.wikipedia.org/wiki/Schwa  [LATIN CAPITAL LETTER SCHWA]\n      case '\u01CD': // Ǎ  [LATIN CAPITAL LETTER A WITH CARON]\n      case '\u01DE': // Ǟ  [LATIN CAPITAL LETTER A WITH DIAERESIS AND MACRON]\n      case '\u01E0': // Ǡ  [LATIN CAPITAL LETTER A WITH DOT ABOVE AND MACRON]\n      case '\u01FA': // Ǻ  [LATIN CAPITAL LETTER A WITH RING ABOVE AND ACUTE]\n      case '\u0200': // Ȁ  [LATIN CAPITAL LETTER A WITH DOUBLE GRAVE]\n      case '\u0202': // Ȃ  [LATIN CAPITAL LETTER A WITH INVERTED BREVE]\n      case '\u0226': // Ȧ  [LATIN CAPITAL LETTER A WITH DOT ABOVE]\n      case '\u023A': // Ⱥ  [LATIN CAPITAL LETTER A WITH STROKE]\n      case '\u1D00': // ᴀ  [LATIN LETTER SMALL CAPITAL A]\n      case '\u1E00': // Ḁ  [LATIN CAPITAL LETTER A WITH RING BELOW]\n      case '\u1EA0': // Ạ  [LATIN CAPITAL LETTER A WITH DOT BELOW]\n      case '\u1EA2': // Ả  [LATIN CAPITAL LETTER A WITH HOOK ABOVE]\n      case '\u1EA4': // Ấ  [LATIN CAPITAL LETTER A WITH CIRCUMFLEX AND ACUTE]\n      case '\u1EA6': // Ầ  [LATIN CAPITAL LETTER A WITH CIRCUMFLEX AND GRAVE]\n      case '\u1EA8': // Ẩ  [LATIN CAPITAL LETTER A WITH CIRCUMFLEX AND HOOK ABOVE]\n      case '\u1EAA': // Ẫ  [LATIN CAPITAL LETTER A WITH CIRCUMFLEX AND TILDE]\n      case '\u1EAC': // Ậ  [LATIN CAPITAL LETTER A WITH CIRCUMFLEX AND DOT BELOW]\n      case '\u1EAE': // Ắ  [LATIN CAPITAL LETTER A WITH BREVE AND ACUTE]\n      case '\u1EB0': // Ằ  [LATIN CAPITAL LETTER A WITH BREVE AND GRAVE]\n      case '\u1EB2': // Ẳ  [LATIN CAPITAL LETTER A WITH BREVE AND HOOK ABOVE]\n      case '\u1EB4': // Ẵ  [LATIN CAPITAL LETTER A WITH BREVE AND TILDE]\n      case '\u1EB6': // Ặ  [LATIN CAPITAL LETTER A WITH BREVE AND DOT BELOW]\n      case '\u24B6': // Ⓐ  [CIRCLED LATIN CAPITAL LETTER A]\n      case '\uFF21': // Ａ  [FULLWIDTH LATIN CAPITAL LETTER A]\n        output[outputPos++] = 'A';\n        break;\n
1046	var writer = new IndexWriter(directory, analyzer, false, IndexWriter.MaxFieldLength.LIMITED);\n
1047	private class Make {\n@Fields({\n        @Field(name = "name", analyzer = @Analyzer(definition = "searchtokenanalyzer")),\n        @Field(name = "label", analyze = Analyze.NO)\n    })\n    private String name;\n}\n
1048	QueryParser queryParser = new QueryParser(Version.LUCENE_CURRENT, "firstName", new StandardAnalyzer(Version.LUCENE_CURRENT));\n    try {\n        Query q = queryParser.parse("Ameer");\n    } catch (ParseException e) {\n        // TODO Auto-generated catch block\n        e.printStackTrace();\n    }\n
1049	fq={!frange l=3}termfreq(text,"special")\n
1050	var parser = new QueryParser(Version.LUCENE_29, "testRoot", std);\n
1051	title=foo\ndate=100 // using numeric value for simplicity\n
1052	private static IndexWriterConfig iwc = new IndexWriterConfig(Version.LUCENE_43,\n        new StandardAnalyzer(Version.LUCENE_43));\niwc.setOpenMode(IndexWriterConfig.OpenMode.CREATE);\n
1053	1.3350155 = (MATCH) sum of:\n  0.7981777 = (MATCH) weight(content:"text ab" in 0) [DefaultSimilarity], result of:\n    0.7981777 = score(doc=0,freq=1.0 = phraseFreq=1.0\n), product of:\n      0.7732263 = queryWeight, product of:\n        2.0645385 = idf(), sum of:\n          0.7768564 = idf(docFreq=4, maxDocs=4)\n          1.287682 = idf(docFreq=2, maxDocs=4)\n        0.37452745 = queryNorm\n      1.0322692 = fieldWeight in 0, product of:\n        1.0 = tf(freq=1.0), with freq of:\n          1.0 = phraseFreq=1.0\n        2.0645385 = idf(), sum of:\n          0.7768564 = idf(docFreq=4, maxDocs=4)\n          1.287682 = idf(docFreq=2, maxDocs=4)\n        0.5 = fieldNorm(doc=0)\n  0.5368378 = (MATCH) weight(content:unique in 0) [DefaultSimilarity], result of:\n    0.5368378 = score(doc=0,freq=1.0 = termFreq=1.0\n), product of:\n      0.6341301 = queryWeight, product of:\n        1.6931472 = idf(docFreq=1, maxDocs=4)\n        0.37452745 = queryNorm\n      0.8465736 = fieldWeight in 0, product of:\n        1.0 = tf(freq=1.0), with freq of:\n          1.0 = termFreq=1.0\n        1.6931472 = idf(docFreq=1, maxDocs=4)\n        0.5 = fieldNorm(doc=0)\n
1054	 var luceneQuery = Session.Advanced.LuceneQuery&lt;Test.ReduceResult&gt;("Raven/DocumentsByEntityName")\n    .WhereEquals("Tag", "Customers");\n
1055	BooleanQuery finalQuery = new BooleanQuery();\nfinalQuery.add(q1, BooleanClause.Occur.MUST);\nfinalQuery.add(q2, BooleanClause.Occur.MUST);\nis.search(finalQuery, collector);\n
1056	md5: a24f73f70e3fcf6aa8fda67444981f78 *solr-4.10.2.tgz\nsha1: b913204d07212d7bb814afe4641992f22404a27d *solr-4.10.2.tgz\n\n-----BEGIN PGP SIGNATURE-----\nVersion: GnuPG v1\n\niEYEABECAAYFAlRMxb0ACgkQ8RmUH25o2mHNLgCfSCHKyvLACVEYEfDLmgh+Mv2W\nhvcAoJQcJ4AGiXNtM7ZRKVPu8HiIu1ic\n=39HO\n-----END PGP SIGNATURE-----\n
1057	Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_47);\nPrecedenceQueryParser luceneParser = new PrecedenceQueryParser(analyzer);\nluceneParser.setAllowLeadingWildcard(true);\nQuery luceneQuery = luceneParser.parse(searchQuery, "name");\n
1058	public static boolean isDocInterested(String query, String matchDoc)\n{\n    List&lt;String&gt; matchDocArr = new ArrayList&lt;String&gt;();\n    matchDocArr = Arrays.asList(matchDoc.split(" "));\n\n    List&lt;String&gt; queryArr = new ArrayList&lt;String&gt;();\n    queryArr = Arrays.asList(query.split(" "));\n\n    int matchCounter = 0;\n    for(int i=0; i&lt;matchDocArr.size(); i++)\n    {\n        if (queryArr.contains(matchDocArr.get(i)))\n            matchCounter++;\n    }\n\n    if (matchCounter == matchDocArr.size())\n        return true;\n\n    return false;\n}\n
1059	    QueryParser parser = new QueryParser("field", new StandardAnalyzer());\n    try {\n        Query query = parser.parse("query"); // changed this line\n    } catch (ParseException e) {\n        e.printStackTrace();\n    }\n
1060	BooleanQuery query = getParamMatches(dto, true);\nquery.setMinimumShouldMatch(2);\n
1061	BooleanJunction junction = builder.bool();\nmust(junction, createQuery(field1, term1));\nmust(junction, createQuery(field2, term2));\nquery = junction.createQuery();\n\nvoid must(BooleanJunction junction, Query query) {\n    if (query != null) {\n        junction.must(query);\n    }\n}\n\nQuery createQuery(String field, Object term) {\n     if(term != null) {\n          return builder.keyword().onField(field).matching(term).createQuery();\n     }\n     return null;\n}\n
1062	    @Bean\n    @StepScope\n    public ItemProcessor&lt;InputVO,OutputVO&gt; luceneIndexProcessor(@Value("#{stepExecutionContext[field1]}") String field1,@Value("#{stepExecutionContext[luceneObjects]}") SerializableLuceneObjects luceneObjects) throws Exception{\n        LuceneIndexProcessor indexProcessor =new LuceneIndexProcessor(luceneObjects);\n        return indexProcessor;\n    }\n
1063	public class CustomIndexWriter implements Serializable {\n\n\n    private static final long serialVersionUID = 1L;\n\n    private transient IndexWriter luceneIndexWriter;\n\n    public CustomIndexWriter(IndexWriter luceneIndexWriter) {\n         this.luceneIndexWriter=luceneIndexWriter;\n    }\n\n    public IndexWriter getLuceneIndexWriter() {\n        return luceneIndexWriter;\n    }\n\n    public void setLuceneIndexWriter(IndexWriter luceneIndexWriter) {\n        this.luceneIndexWriter = luceneIndexWriter;\n    }\n\n\n}\n
1064	public class SnowballAnalyzer extends Analyzer {\n    /**\n     * Creates a\n     * {@link org.apache.lucene.analysis.Analyzer.TokenStreamComponents} which\n     * tokenizes text when given a reader.\n     * \n     * @return A\n     *         {@link org.apache.lucene.analysis.Analyzer.TokenStreamComponents}\n     *         built from an {@link WhitespaceTokenizer} filtered with\n     *         {@link LowerCaseFilter} and English {@link SnowballFilter}.\n     */\n    @Override\n    protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer source = new WhitespaceTokenizer();\n        TokenStream filter = new LowerCaseFilter(source);\n        filter = new SnowballFilter(filter, "English");\n        return new TokenStreamComponents(source, filter);\n    }\n\n}\n
1065	@Field(store=Store.YES)\nprivate String name;\n
1066	&lt;lst name="default"&gt;\n    &lt;str name="config"&gt;data-config.xml&lt;/str&gt;\n&lt;/lst&gt;\n
1067	&lt;searchComponent name="suggest" class="solr.SuggestComponent"&gt;\n        &lt;lst name="suggester"&gt;\n              &lt;str name="name"&gt;suggestions&lt;/str&gt;\n              &lt;str name="lookupImpl"&gt;FuzzyLookupFactory&lt;/str&gt;\n              &lt;str name="dictionaryImpl"&gt;FileDictionaryFactory&lt;/str&gt;\n              &lt;str name="field"&gt;searchfield&lt;/str&gt;\n              &lt;str name="weightField"&gt;searchscore&lt;/str&gt;\n              &lt;str name="suggestAnalyzerFieldType"&gt;text_ngram&lt;/str&gt;\n              &lt;str name="buildOnStartup"&gt;false&lt;/str&gt;\n              &lt;str name="buildOnCommit"&gt;false&lt;/str&gt;\n              &lt;str name="sourceLocation"&gt;spellings.txt&lt;/str&gt;\n              &lt;str name="storeDir"&gt;autosuggest_dict&lt;/str&gt;\n        &lt;/lst&gt;\n &lt;/searchComponent&gt;\n\n  &lt;requestHandler name="/suggest" class="solr.SearchHandler" startup="lazy"&gt;\n        &lt;lst name="defaults"&gt;\n                &lt;str name="suggest"&gt;true&lt;/str&gt;\n                &lt;str name="suggest.count"&gt;10&lt;/str&gt;\n                &lt;str name="suggest.dictionary"&gt;suggestions&lt;/str&gt;\n                &lt;str name="suggest.dictionary"&gt;results&lt;/str&gt;\n        &lt;/lst&gt;\n        &lt;arr name="components"&gt;\n                &lt;str&gt;suggest&lt;/str&gt;\n        &lt;/arr&gt;\n  &lt;/requestHandler&gt;\n
1068	&lt;field name="title" type="string" indexed="true" stored="true"/&gt;\n
1069	&lt;!-- VB - Just like text_general, but supports $ currency matching and autoGeneratePhraseQueries --&gt;\n&lt;fieldType name="text_curr_3" class="solr.TextField" positionIncrementGap="100" autoGeneratePhraseQueries="true"&gt;\n  &lt;analyzer type="index"&gt;\n    &lt;charFilter class="solr.MappingCharFilterFactory" mapping="mapping.txt"/&gt;\n    &lt;charFilter class="solr.PatternReplaceCharFilterFactory" pattern="\$" replacement="xxdollarxx"/&gt;\n    &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;\n    &lt;filter class="solr.PatternReplaceFilterFactory" pattern="xxdollarxx" replacement="\$" replace="all"/&gt;\n    &lt;filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt" enablePositionIncrements="true"/&gt;\n    &lt;filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" catenateAll="0" splitOnCaseChange="1" types="word-delim-types.txt" /&gt;\n    &lt;filter class="solr.LowerCaseFilterFactory"/&gt;\n &lt;/analyzer&gt;\n  &lt;analyzer type="query"&gt;\n    &lt;charFilter class="solr.MappingCharFilterFactory" mapping="mapping.txt"/&gt;\n    &lt;charFilter class="solr.PatternReplaceCharFilterFactory" pattern="\$" replacement="xxdollarxx"/&gt;\n    &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;\n    &lt;filter class="solr.PatternReplaceFilterFactory" pattern="xxdollarxx" replacement="\$" replace="all"/&gt;\n    &lt;filter class="solr.SynonymFilterFactory" synonyms="synonyms.txt" ignoreCase="true" expand="true"/&gt;\n    &lt;filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt" enablePositionIncrements="true"/&gt;\n    &lt;filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="0" catenateNumbers="0" catenateAll="0" splitOnCaseChange="1"  types="word-delim-types.txt" /&gt;\n    &lt;filter class="solr.LowerCaseFilterFactory"/&gt;   \n  &lt;/analyzer&gt;\n&lt;/fieldType&gt;\n
1070	Query query = new PhraseQuery.Builder()\n                        .add(new Term("country", "iraq"))\n                        .add(new Term("country", "russia"))\n                        .setSlop(2)\n                        .build();\n
1071	        final TopDocs merge = TopDocs.merge(1000, new TopDocs[]{topDocs1, topDocs2});\n        Set&lt;ScoreDoc&gt; scoreDocs = new TreeSet&lt;&gt;(new Comparator&lt;ScoreDoc&gt;() {\n            @Override\n            public int compare(ScoreDoc o1, ScoreDoc o2) {\n                return Integer.compare(o1.doc, o2.doc);\n            }\n        });\n        float maxScore = Float.MIN_VALUE;\n        for (int i = 0; i &lt; merge.scoreDocs.length; ++i) {\n            final ScoreDoc[] scoreDocs1 = merge.scoreDocs;\n            scoreDocs.add(scoreDocs1[i]);\n            if (scoreDocs1[i].score &gt; maxScore) {\n                maxScore = scoreDocs1[i].score;\n            }\n        }\n        final TopDocs filteredTopDocs = new TopDocs(scoreDocs.size(), scoreDocs.toArray(new ScoreDoc[0]), maxScore);\n
1072	public static void Test()\n{\n    // Use an in-memory index.\n    RAMDirectory indexDirectory = new RAMDirectory();\n\n    // Make sure to use the same analyzer for indexing \n    Analyzer analyzer = new StandardAnalyzer(Lucene.Net.Util.Version.LUCENE_30);\n\n    // Add single document to the index.\n    using (IndexWriter writer = new IndexWriter(indexDirectory, analyzer, IndexWriter.MaxFieldLength.UNLIMITED))\n    {\n        Document document = new Document();\n        document.Add(new Field("content", "This is just some text", Field.Store.YES, Field.Index.ANALYZED));\n        document.Add(new Field("cataloguenumber", "DF-GH5", Field.Store.YES, Field.Index.ANALYZED));\n\n        writer.AddDocument(document);\n    }\n\n    var parser = new MultiFieldQueryParser(\n        Lucene.Net.Util.Version.LUCENE_30,\n        new[] { "cataloguenumber", "content" },\n        analyzer);\n\n    var searcher = new IndexSearcher(indexDirectory);\n\n    DoSearch("df-gh5", parser, searcher);\n    DoSearch("df-*", parser, searcher);\n}\n\nprivate static void DoSearch(string queryString, MultiFieldQueryParser parser, IndexSearcher searcher)\n{\n    var query = parser.Parse(queryString);\n\n    TopDocs docs = searcher.Search(query, 10);\n\n    foreach (ScoreDoc scoreDoc in docs.ScoreDocs)\n    {\n        Document searchHit = searcher.Doc(scoreDoc.Doc);\n        string cataloguenumber = searchHit.GetValues("cataloguenumber").FirstOrDefault();\n        string content = searchHit.GetValues("content").FirstOrDefault();\n        Console.WriteLine($"Found object: {cataloguenumber} {content}");\n    }\n}\n
1073	Analyzer analyzer = searchFactory.getAnalyzer( "ngram" );\nSystem.out.println( AnalyzerUtils.tokenizedTermValues( analyzer, "description", "4349" ) );\nSystem.out.println( AnalyzerUtils.tokenizedTermValues( analyzer, "description", "P 43" ) );\n
1074	&lt;dataConfig&gt;\n&lt;dataSource driver="org.hsqldb.jdbcDriver"  url="jdbc:hsqldb:/temp/example/ex" user="sa" /&gt;\n&lt;document&gt;\n&lt;entity name="city" query="select City.city_id ,City.city_name ,car.id ,car.name ,District.id  from Car join City on city.id = car.CITY_ID join District on car.DISTRICT_ID = District.id "&gt;\n    &lt;field column="cityid" name="City.city_id" /&gt;\n    &lt;field column="cityname" name="City.city_name" /&gt;\n    &lt;field column="carid" name="car.id" /&gt;\n    &lt;field column="carname" name="car.name" /&gt;\n    &lt;field column="Districtid" name="District.id" /&gt;\n    &lt;etc... any column you need&gt;\n\n\n&lt;/entity&gt;\n&lt;/document&gt;\n
1075	private int nTerm = 0; // field added by me\n\n@Override\nprotected TokenStreamComponents createComponents(String fieldName) {\n    final Tokenizer source = new StandardTokenizer();\n    TokenStream result = new StandardFilter(source);\n    result = new EnglishPossessiveFilter(result);\n    result = new LowerCaseFilter(result);\n    result = new StopFilter(result, stopwords);\n    if (!stemExclusionSet.isEmpty())\n        result = new SetKeywordMarkerFilter(result, stemExclusionSet);\n    result = new PorterStemFilter(result);\n\n    // my modification starts here:\n    class ExamineFilter extends FilteringTokenFilter {\n        private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n        public ExamineFilter( TokenStream in ) {\n                super(  in);\n          }         \n        @Override\n        protected boolean accept() throws IOException {\n            String term = new String( termAtt.buffer(), 0, termAtt.length() );\n            printOut( String.format( "# term %d |%s|", nTerm, term ));\n\n            // do all sorts of things with this term... \n\n            nTerm++;\n            return true;\n        }\n    }\n    class MyTokenStreamComponents extends TokenStreamComponents {\n        MyTokenStreamComponents( Tokenizer source, TokenStream result ){\n            super( source, result );\n        }\n        public TokenStream getTokenStream(){\n            // reset term count at start of each Document\n            nTerm = 0;\n            return super.getTokenStream();\n        }\n    }\n    result = new ExamineFilter( result );\n    return new MyTokenStreamComponents(source, result);\n    //\n}\n
1076	@Document(indexName = "myindex", type = "mytype")\n@Setting(settingPath = "/mysetting/mysetting.json")\npublic class Employee implements Serializable {\n\n    @Id\n    private String employeeId;\n\n    @Field(type = FieldType.String, analyzer = "synonym_analyzer")\n    private String transformedTitle ;\n
1077	java -Dtype=text/csv -Dc=collection1 -Drecursive -jar post.jar test\n
1078	dataDir=/ngs/app/ligerp/solr/zookeeper-3.4.6/zookeeperdata/1\nclientPort=2181\ninitLimit=100\nsyncLimit=2\nserver.1=10.136.145.38:2888:3888\nserver.2=10.136.145.39:2889:3889\nserver.3=10.136.145.40:2890:3890\n
1079	POST /twitter/tweet/_search?search_type=dfs_query_then_fetch\n{\n        "explain": true, \n        "query": {\n                "term" : {\n                        "title" : "d"\n                }\n        }\n}\n
1080	    {% assign curDate = site.time | date: '%s' %}\n    {% assign post_count = site.events | size %}\n    {% if post_count == 1 %}\n    {% assign css_class = "full" %}\n    {% elsif post_count == 2 %}\n    {% assign css_class = "one-half" %}\n    {% elsif post_count == 3 %}\n    {% assign css_class = "one-third" %}\n    {% else %}\n    {% assign css_class = "one-quarter" %}\n    {% endif %}\n\n    {% for event in site.events %}\n        {% assign postStartDate = event.date | date: '%s' %}\n         {% if postStartDate &gt; curDate %}\n    &lt;div class='{{css_class}}'&gt;{{event.title}}&lt;/div&gt; \n        {% endif %}\n    {% endfor %}\n
1081	import java.io.File;\nimport java.io.IOException;\n\nimport org.apache.lucene.analysis.core.SimpleAnalyzer;\nimport org.apache.lucene.index.IndexWriter;\nimport org.apache.lucene.index.IndexWriterConfig;\nimport org.apache.lucene.index.IndexWriterConfig.OpenMode;\nimport org.apache.lucene.store.FSDirectory;\n\npublic class IndexCreator {\n\n    public static IndexWriter getWriter() throws IOException{\n          File indexDir = new File("D:\\Experiment");\n          SimpleAnalyzer analyzer = new SimpleAnalyzer();\n          IndexWriterConfig indexWriterConfig = new IndexWriterConfig(analyzer);\n          indexWriterConfig.setOpenMode(OpenMode.CREATE_OR_APPEND);\n          IndexWriter indexWriter = new IndexWriter(FSDirectory.open(indexDir\n                .toPath()), indexWriterConfig);\n          indexWriter.commit();\n          return indexWriter;\n\n    }\n}\n
1082	localhost:8983/solr/select?q=job:programmer AND job:developer\n
1083	  final int endPos = pos + posLength;\n\n  if (!posToEndOffset.containsKey(endPos)) {\n    // First time we've seen a token arriving to this position:\n    posToEndOffset.put(endPos, endOffset);\n    //System.out.println("  + e " + endPos + " -&gt; " + endOffset);\n  } else {\n    // We've seen a token arriving to this position\n    // before; verify the endOffset is the same:\n    //System.out.println("  + ve " + endPos + " -&gt; " + endOffset);\n    assertEquals("inconsistent endOffset " + i + " pos=" + pos + " posLen=" + posLength + " token=" + termAtt, posToEndOffset.get(endPos).intValue(), endOffset);\n  }\n
1084	http://mysolrserver:8983/solr/MyCore/MyQueryHandler?q=Smith&amp;start=0&amp;rows=25&amp;sort=exists(query({!v=%27Id:(36+OR+76+OR+90+OR+224+OR+391)%27}))%20DESC,%20Name%20ASC\n
1085	final AnalyzingQueryParser analyzingQueryParser = new AnalyzingQueryParser("", analyzer);\n\n// TODO: The rewrite method can be overridden.\n// analyzingQueryParser.setMultiTermRewriteMethod(MultiTermQuery.CONSTANT_SCORE_BOOLEAN_REWRITE);\n\nQuery parsedQuery = analyzingQueryParser.parse(query);\n// Here parsedQuery is an instance of the org.apache.lucene.search.WildcardQuery class.\n\nparsedQuery = parsedQuery.rewrite(reader);\n// Here parsedQuery is an instance of the org.apache.lucene.search.MultiTermQueryConstantScoreWrapper class.\n\nfinal Weight weight = parsedQuery.createWeight(searcher, false);\nfinal Set&lt;Term&gt; terms = new HashSet&lt;&gt;();\nweight.extractTerms(terms);\n
1086	q: (id:'23' OR id:'24')^=2\n
1087	float payloadValue = ByteBuffer.wrap(bytesRef.bytes).order(ByteOrder.BIG_ENDIAN).getFloat();\n
1088	Document doc = new Document();\n\n        doc.add(new TextField("contents", "Homer January, Lenny February",Store.YES));\n        doc.add(new TextField("title", "2017 on call schedule.xls", Store.YES));\n\n        iwriter.addDocument(doc);\n\n        doc = new Document();\n        doc.add(new TextField("contents", "Carl January, Frank February",Store.YES));\n        doc.add(new TextField("title", "2018 on call schedule.xls", Store.YES));\n\n        iwriter.addDocument(doc);\n\n        iwriter.commit(); \n
1089	def simplistic_plural(word, text):\n    word_or_plural = re.escape(word) + 's?'\n    return re.match(word_or_plural, text)\n
1090	Query q = new QueryParser("W", analyzer).parse(query);\n
1091	public SentenceTokenizer(Reader reader,SentenceDetector sentenceDetector) {\n    super();\n    this.input =reader;\n    this.sentenceDetector = sentenceDetector;\n}\n
1092	Query categoryQuery = queryBuilder.keyword().onField( "category.__Id" ).matching( &lt;some ID&gt; ).createQuery()\nQuery booleanJunction = queryBuilder.bool()\n    .must( categoryQuery )\n    .must( &lt;some other query&gt; )\n    .must( &lt;some other query&gt; )\n    .createQuery()\nFullTextQuery fullTextQuery = fullTextEntityManager.createQuery( booleanJunction, Ad.class );\n
1093	curl -X GET "localhost:9200/users/_search?pretty=true" -H 'Content-Type: application/json' -d'\n{\n"query": {\n    "query_string": {\n    "query" : "firstName.keyword:\"Daulet\""\n}\n}\n}'\n
1094	Properties jobProperties = MassIndexingJob.parameters()\n        .forEntity( MyClass.class )\n        .restrictedBy( Restrictions.ge( "lastChangeDateTime", LocalDate.now().minus( 26, ChronoUnit.HOURS ) ) // 26 to account for DST switch and other slight delays\n        .build();\nlong executionId = BatchRuntime.getJobOperator()\n        .start( MassIndexingJob.NAME, jobProperties );\n
1095	import requests\n\nurl = 'http://localhost:8983/solr/collection/update' # update endpoint of the collection\n\nids_to_delete = ['a', 'b&lt;c', 'd:e']\nrequests.post(url, json={ 'delete': ids_to_delete })\n
1096	&lt;configuration xmlns:patch="http://www.sitecore.net/xmlconfig/"&gt;\n  &lt;sitecore&gt;\n    &lt;contentSearch&gt;\n      &lt;configuration type="Sitecore.ContentSearch.LuceneProvider.LuceneSearchConfiguration, Sitecore.ContentSearch.LuceneProvider"&gt;\n        &lt;indexes hint="list:AddIndex"&gt;\n          &lt;index id="demo_master" type="Sitecore.ContentSearch.LuceneProvider.LuceneIndex, Sitecore.ContentSearch.LuceneProvider"&gt;\n            &lt;param desc="name"&gt;$(id)&lt;/param&gt;\n            &lt;param desc="folder"&gt;$(id)&lt;/param&gt;\n            &lt;!-- This initializes index property store. Id has to be set to the index id --&gt;\n            &lt;param desc="propertyStore" ref="contentSearch/databasePropertyStore" param1="$(id)" /&gt;\n            &lt;configuration ref="contentSearch/indexConfigurations/LuceneCustomSearchConfiguration" /&gt;\n            &lt;strategies hint="list:AddStrategy"&gt;\n              &lt;!-- NOTE: order of these is controls the execution order --&gt;\n              &lt;strategy ref="contentSearch/indexUpdateStrategies/syncMaster" /&gt;\n            &lt;/strategies&gt;\n            &lt;commitPolicyExecutor type="Sitecore.ContentSearch.CommitPolicyExecutor, Sitecore.ContentSearch"&gt;\n              &lt;policies hint="list:AddCommitPolicy"&gt;\n                &lt;policy type="Sitecore.ContentSearch.TimeIntervalCommitPolicy, Sitecore.ContentSearch" /&gt;\n              &lt;/policies&gt;\n            &lt;/commitPolicyExecutor&gt;\n            &lt;locations hint="list:AddCrawler"&gt;\n              &lt;crawler type="Sitecore.ContentSearch.SitecoreItemCrawler, Sitecore.ContentSearch"&gt;\n                &lt;Database&gt;master&lt;/Database&gt;\n                &lt;Root&gt;/sitecore/content/Sample&lt;/Root&gt;\n              &lt;/crawler&gt;\n            &lt;/locations&gt;\n          &lt;/index&gt;\n        &lt;/indexes&gt;\n      &lt;/configuration&gt;\n      &lt;indexConfigurations&gt;\n        &lt;LuceneCustomSearchConfiguration type="Sitecore.ContentSearch.ContentSearchConfiguration, Sitecore.ContentSearch"&gt;\n          &lt;indexAllFields&gt;true&lt;/indexAllFields&gt;\n           &lt;initializeOnAdd&gt;true&lt;/initializeOnAdd&gt;\n          &lt;analyzer ref="contentSearch/configuration/defaultIndexConfiguration/analyzer" /&gt;\n          &lt;fieldMap type="Sitecore.ContentSearch.FieldMap, Sitecore.ContentSearch"&gt;\n            &lt;fieldNames hint="raw:AddFieldByFieldName"&gt;\n              &lt;field fieldName="_uniqueid"            storageType="YES" indexType="TOKENIZED"    vectorType="NO" boost="1f" type="System.String" settingType="Sitecore.ContentSearch.LuceneProvider.LuceneSearchFieldConfiguration, Sitecore.ContentSearch.LuceneProvider"&gt;\n                &lt;analyzer type="Sitecore.ContentSearch.LuceneProvider.Analyzers.LowerCaseKeywordAnalyzer, Sitecore.ContentSearch.LuceneProvider" /&gt;\n              &lt;/field&gt;\n              &lt;/fieldNames&gt;\n          &lt;/fieldMap&gt;\n          &lt;fields hint="raw:AddComputedIndexField"&gt;\n            &lt;field fieldName="imageurl"&gt;Lucene.CustomSearch.BusinessLayer.ComputedFields.ImageUrlComputedField,Lucene.CustomSearch.BusinessLayer&lt;/field&gt;\n            &lt;/fields&gt;\n          &lt;include hint="list:IncludeTemplate"&gt;\n            &lt;Laptop&gt;{3AF80D80-404F-411F-9E72-F43A7D92AEC8}&lt;/Laptop&gt;\n          &lt;/include&gt;\n          &lt;fieldReaders ref="contentSearch/configuration/defaultIndexConfiguration/fieldReaders"/&gt;\n          &lt;indexFieldStorageValueFormatter ref="contentSearch/configuration/defaultIndexConfiguration/indexFieldStorageValueFormatter"/&gt;\n          &lt;indexDocumentPropertyMapper ref="contentSearch/configuration/defaultIndexConfiguration/indexDocumentPropertyMapper"/&gt;\n        &lt;/LuceneCustomSearchConfiguration&gt;\n      &lt;/indexConfigurations&gt;\n    &lt;/contentSearch&gt;\n  &lt;/sitecore&gt;\n&lt;/configuration&gt;\n
1097	@AnalyzerDef(name = "yearanalyzer",\n        // Split input into tokens according to tokenizer\n        // Split input into tokens according to tokenizer\n        tokenizer = @TokenizerDef(factory = KeywordTokenizerFactory.class),\n        filters = {\n            @TokenFilterDef(factory = PatternReplaceFilterFactory.class, params = {\n                @Parameter(name = "pattern", value = "^\\d{2}(\\d{2})$"),\n                @Parameter(name = "replacement", value = "$1"),\n                @Parameter(name = "replace", value = "all")}),\n        })\n
1098	Explanation explain = searcher.explain(myQuery, resultDocNo);\nSystem.out.print(explain.ToString());\n
1099	Bits liveDocs = MultiFields.getLiveDocs(reader);\nif (!liveDocs.get(docID)) ...\n
1100	Query query = new SpanNearQuery(new SpanQuery[] {\n         new SpanTermQuery(new Term("title", "convert")),\n         new SpanMultiTermQueryWrapper(new PrefixQuery(new Term("title", "int"))),\n         new SpanTermQuery(new Term("title", "string"))\n     },\n     50,\n     true);\nreturn query;\n
1101	&lt;prop key="hibernate.search.default.directory_provider"&gt;filesystem&lt;/prop&gt;\n&lt;prop key="hibernate.search.default.indexBase"&gt;/var/lucene/indexes&lt;/prop&gt;\n
1102	&lt;fieldType name="text_start_end" class="solr.TextField" omitNorms="false"&gt;\n  &lt;analyzer&gt;\n    &lt;tokenizer class="solr.ClassicTokenizerFactory"/&gt;\n    &lt;filter class="solr.PositionFilterFactory"/&gt;\n    &lt;filter class="solr.LowerCaseFilterFactory"/&gt;\n    &lt;filter class="solr.EdgeNGramFilterFactory" minGramSize="1" maxGramSize="20" side="front" /&gt;\n  &lt;/analyzer&gt;\n&lt;/fieldType&gt;\n
1103	public class Members_BySearchTermAndGroup : AbstractIndexCreationTask {\n    public override IndexDefinition CreateIndexDefinition(){\n        return new IndexDefinition {\n            Map = "from g in docs.Groups\n                   from member in g.Members\n                   select new {\n                   GroupdId = g.Id,\n                   Name = member.Name,\n                   Bio = member.Bio,\n                   Content = new [] {member.Name, member.Bio },\n                  }",\n            Indexes = { \n                          { "GroupId", FieldIndexing.Default }, \n                          { "Content", FieldIndexing.Analyzed } \n                      },\n            Stores = {\n                          { "Name", FieldStorage.Yes },\n                          { "Bio", FieldStorage.Yes }\n                     }\n        }\n    }\n}\n
1104	&lt;form action="search2.php" method="post"&gt;    \n
1105	from doc in docs\nfrom text1 in ((IEnumerable&lt;dynamic&gt;)doc.Texts1)\nfrom text2 in ((IEnumerable&lt;dynamic&gt;)doc.Texts2)\nfrom text3 in ((IEnumerable&lt;dynamic&gt;)doc.Texts3)\n\nselect new\n{\n    Text1 = text1,\n    Text2 = text2,\n    Text3 = text3\n}\n
1106	Directory dir = new RAMDirectory();\nAnalyzer analyzer = null;\nIndexWriterConfig iwc = new IndexWriterConfig(Version.LUCENE_4_10_1, analyzer);\nIndexWriter indexWriter = new IndexWriter(dir, iwc);\n\nDocument doc1 = new Document();\ndoc1.add(new StringField("title", "aa", Field.Store.YES));\nindexWriter.addDocument(doc1);\nindexWriter.commit();\n\nIndexReader reader = IndexReader.open(dir);\nIndexSearcher searcher = new IndexSearcher(reader);\n// print results using MatchAllDocsQuery\n
1107	facet.range={!ex=dt}id&amp;..&amp;fq={!tag=dt}id:[0 TO 100000]\n
1108	string s = NumberTools.longToString(longval);\n//index the string...\n
1109	&lt;fieldType name="text_delimeter" class="solr.TextField" positionIncrementGap="100"&gt;\n&lt;analyzer type="index"&gt;\n&lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;\n&lt;filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" preserveOriginal="1" catenateAll="1" splitOnCaseChange="0"/&gt;\n&lt;filter class="solr.LowerCaseFilterFactory"/&gt;\n&lt;/analyzer&gt;\n&lt;analyzer type="query"&gt;\n&lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;\n&lt;!--\n    &lt;filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="0" catenateNumbers="0" preserveOriginal="1" catenateAll="0" splitOnCaseChange="0"/&gt; \n--&gt;\n&lt;filter class="solr.LowerCaseFilterFactory"/&gt;\n&lt;/analyzer&gt;\n&lt;/fieldType&gt;\n
1110	using (\n    IndexWriter writer = new IndexWriter(FSDirectory.Open("index"), new SimpleAnalyzer(),\n                                        true, IndexWriter.MaxFieldLength.LIMITED))\n
1111	Query query = NumericRangeQuery.newIntRange(Constants.INDEX_ID_FIELD, docIdx, docIdx, true, true);\n
1112	FullTextEntityManager fullTextEntityManager = org.hibernate.search.jpa.Search.getFullTextEntityManager(entityManager);\n    try {\n        fullTextEntityManager.createIndexer(standardEntity.class).batchSizeToLoadObjects( 1000 )\n        .cacheMode( CacheMode.IGNORE ).threadsToLoadObjects( 20 ).threadsForSubsequentFetching( 50 )\n        .startAndWait();        \n    } catch (InterruptedException e) {\n        e.printStackTrace();\n    }\n
1113	MATCH (n:Person) \nWHERE NOT HAS(n.migrated)\nSET n.name = n.name, n.migrated=true\nRETURN count(n) LIMIT 50000\n
1114	&lt;field name="field_name" type="text_general" indexed="true" stored="true"/&gt;\n
1115	abhijit@abhijit:~/Downloads/solr-5.0.0$ bin/solr start -e cloud \n\nWelcome to the SolrCloud example!\n\n\nThis interactive session will help you launch a SolrCloud cluster on your local workstation.\n\nTo begin, how many Solr nodes would you like to run in your local cluster? (specify 1-4 nodes) [2] 2\nOk, let's start up 2 Solr nodes for your example SolrCloud cluster.\n\nPlease enter the port for node1 [8983] 8983\nOops! Looks like port 8983 is already being used by another process. Please choose a different port. 8985\n8985\nPlease enter the port for node2 [7574] 7576\n7576\n\nStarting up SolrCloud node1 on port 8985 using command:\n\nsolr start -cloud -s example/cloud/node1/solr -p 8985   \n\n\nWaiting to see Solr listening on port 8985 [|]  \nStarted Solr server on port 8985 (pid=7092). Happy searching!\n\n\n\nStarting node2 on port 7576 using command:\n\nsolr start -cloud -s example/cloud/node2/solr -p 7576 -z localhost:9985   \n\n\nWaiting to see Solr listening on port 7576 [\]  \nStarted Solr server on port 7576 (pid=7461). Happy searching!\n\n\nNow let's create a new collection for indexing documents in your 2-node cluster.\n\nPlease provide a name for your new collection: [gettingstarted] testCore\ntestCore\nHow many shards would you like to split testCore into? [2] 2\n2\nHow many replicas per shard would you like to create? [2] 2\n2\nPlease choose a configuration for the testCore collection, available options are:\nbasic_configs, data_driven_schema_configs, or sample_techproducts_configs [data_driven_schema_configs] data_driven_schema_configs\nConnecting to ZooKeeper at localhost:9985\nUploading /home/abhijit/Downloads/solr-5.0.0/server/solr/configsets/data_driven_schema_configs/conf for config testCore to ZooKeeper at localhost:9985\n\nCreating new collection 'testCore' using command:\nhttp://127.0.1.1:7576/solr/admin/collections?action=CREATE&amp;name=testCore&amp;numShards=2&amp;replicationFactor=2&amp;maxShardsPerNode=2&amp;collection.configName=testCore\n\n{\n  "responseHeader":{\n    "status":0,\n    "QTime":7468},\n  "success":{"":{\n      "responseHeader":{\n        "status":0,\n        "QTime":7057},\n      "core":"testCore_shard1_replica2"}}}\n\n\n\nSolrCloud example running, please visit http://localhost:8985/solr \n
1116	public class PorterAnalyzer extends Analyzer {\n  @Override\n  @SuppressWarnings("resource")\n  protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n    final StandardTokenizer src = new StandardTokenizer(reader);\n    TokenStream tok = new StandardFilter(src);\n    tok = new LowerCaseFilter(tok);\n    tok = new StopFilter(tok, StandardAnalyzer.STOP_WORDS_SET);\n    tok = new PorterStemFilter(tok);\n    return new TokenStreamComponents(src, tok);\n  }\n}\n
1117	TermFirstPassGroupingCollector firstPassCollector = new TermFirstPassGroupingCollector(\n            "&lt;grouping field name, e.g. id&gt;",\n            Sort.INDEXORDER,\n            x);\n\nsearcher.search(query, firstPassCollector);\n\nCollection&lt;SearchGroup&lt;String&gt;&gt; firstPassResult = firstPassCollector.getTopGroups(0, false)\n
1118	&lt;searchComponent class="solr.SuggestComponent" name="suggest"&gt;\n  &lt;lst name="suggester"&gt;\n    &lt;str name="name"&gt;mySuggester&lt;/str&gt;\n    &lt;str name="lookupImpl"&gt;AnalyzingInfixLookupFactory&lt;/str&gt;\n    &lt;str name="dictionaryImpl"&gt;DocumentDictionaryFactory&lt;/str&gt;\n    &lt;str name="field"&gt;suggest_Manu&lt;/str&gt;  &lt;!-- the indexed field to derive suggestions from --&gt;\n    &lt;str name="weightField"&gt;productId_meter&lt;/str&gt;\n    &lt;str name="suggestAnalyzerFieldType"&gt;suggestAnalyzer&lt;/str&gt;\n    &lt;str name="buildOnCommit"&gt;true&lt;/str&gt;\n\n  &lt;/lst&gt;\n\n\n  &lt;lst name="suggester"&gt;\n\n    &lt;str name="name"&gt;mySuggester2&lt;/str&gt;\n    &lt;str name="lookupImpl"&gt;AnalyzingInfixLookupFactory&lt;/str&gt;\n    &lt;str name="dictionaryImpl"&gt;DocumentDictionaryFactory&lt;/str&gt;\n    &lt;str name="field"&gt;suggest_Name&lt;/str&gt;  &lt;!-- the indexed field to derive suggestions from --&gt;\n    &lt;str name="weightExpression"&gt;productId_meter&lt;/str&gt;\n    &lt;str name="suggestAnalyzerFieldType"&gt;suggestAnalyzer&lt;/str&gt;\n    &lt;str name="indexPath"&gt;path-of-system-dir/newSuggester2&lt;/str&gt;\n    &lt;str name="buildOnCommit"&gt;true&lt;/str&gt;\n\n  &lt;/lst&gt;\n\n&lt;/searchComponent&gt;\n
1119	FieldType ft = new FieldType();\nft.setIndexed(true);         \nft.setIndexOptionsFieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\nft.setStored(true);\nft.setStoreTermVectors(true);\nft.setStoreTermVectorOffsets(true);\nft.setTokenized(true);\nft.stored();\n\nQueryScorer qs = new QueryScorer(q);\nHighlighter h = new Highlighter(qs);\nhighlighter.setTextFragmenter(new SimpleFragmenter(300));           \nString highlighted = h.getBestFragment(new StandardAnalyzer(),f,Text);\n
1120	 Expression&lt;Func&lt;SearchResultItem, bool&gt;&gt; predicate = PredicateBuilder.True&lt;SearchResultItem&gt;();\n            predicate = predicate.Or(p =&gt; p.TemplateName.Equals("News"));\n            predicate = predicate.Or(p =&gt; p.TemplateName.Equals("Page"));\n\n            IEnumerable&lt;SearchResultItem&gt; results = _searchContext\n                .GetQueryable&lt;SearchResultItem&gt;()\n                .Where(predicate);\n
1121	q=(Cartridge)&amp;fq={!tag=Deals_EN_ss}Deals_EN_ss:"Clearance%20Items"%20OR%20Deals_EN_ss:"On%20Promotion"&amp;defType=edismax&amp;facet=true&amp;facet.field={!ex=Deals_EN_ss}Deals_EN_ss&amp;facet.field={!key=Brand_EN_ss}Brand_EN_ss&amp;facet.field={!key=Rating_EN_ss}Rating_EN_ss&amp;facet.field={!key=CA_Default_Price_EN_p}CA_Default_Price_EN_p\n
1122	  &lt;add name="InternalIndexer" type="UmbracoExamine.UmbracoContentIndexer, UmbracoExamine"\n       supportUnpublished="true"\n       supportProtected="true" \n       analyzer="Lucene.Net.Analysis.WhitespaceAnalyzer, Lucene.Net"\n       useTempStorage="LocalOnly"/&gt;\n\n  &lt;add name="InternalSearcher" type="UmbracoExamine.UmbracoExamineSearcher, UmbracoExamine"\n       analyzer="Lucene.Net.Analysis.WhitespaceAnalyzer, Lucene.Net" \n       useTempStorage="LocalOnly"/&gt;\n
1123	SynonymMap.Builder sb = new SynonymMap.Builder ( true );\nsb.add ( new CharsRef("DNA"), new CharsRef("DNS"), true );\n// ...add here others synonyms\nSynonymMap synonymMap = sb.build();\n
1124	    public class MyComparer : IComparer&lt;string&gt;\n{\n    int IComparer&lt;string&gt;.Compare(string x, string y)\n    {\n        if (x == y)\n        {\n            return 0;\n        }\n        if (x.Contains(y))\n        {\n            return -1;\n        }\n        else\n        {\n            return 1;\n        }\n    }\n}\n
1125	&lt;queryParser name="collapse" class="org.apache.solr.search.CollapsingQParserPlugin"/&gt;\n
1126	Query q = multipleQueryParser.parse("\"" + searchQuery + "\"");\n
1127	    static TreeMap&lt;Integer, Integer&gt; Total_Hits = new TreeMap&lt;Integer, Integer&gt;();\n
1128	...       \n\n    //REMOVE AND INSTANTIATE IN THE CYCLE!      TopScoreDocCollector collector = TopScoreDocCollector.create ( 5, true );\n\n                    s = "";\n                    while ( !s.equalsIgnoreCase ( "q" ) ) {\n                        try {\n                            System.out.println ( "Enter the search query (q=quit):" );\n                            s = br.readLine ();\n                            if ( s.equalsIgnoreCase ( "q" ) ) {\n                                break;\n                            }\n                            // INTANTIATE HERE!!!\n                            TopScoreDocCollector collector = TopScoreDocCollector.create ( 5, true );\n\n                            Query q = new QueryParser ( Version.LUCENE_40, "contents", analyzer ).parse ( s );\n                            searcher.search ( q, collector );\n\n...\n
1129	val filter =\n  new BooleanQuery.Builder().add(\n    new BooleanQuery.Builder()\n      .add(new ConstantScoreQuery( new TermQuery( new Term("k", "v1") ) ), BooleanClause.Occur.MUST)\n      .add(new ConstantScoreQuery( new TermQuery( new Term("k", "v2") ) ), BooleanClause.Occur.MUST_NOT)\n      .build()\n    ,\n    BooleanClause.Occur.MUST_NOT\n  ).add(new MatchAllDocsQuery(), BooleanClause.Occur.SHOULD).build()\n
1130	Query combinedQuery = querybuilder\n.bool()\n    .should( firstNameQuery )\n    .should( lastNameQuery )\n.createQuery();\n
1131	            QueryParser parser = new QueryParser(Version.LUCENE_31, "instanceId", new KeywordAnalyzer());\n            try {\n                Query lucene = parser.parse(keyword);\n            }\n
1132	from doc in docs.DocumentName\nselect new { Message = doc.Headers["Foo.Bar.Message"], Message2 = Headers.Foo2 };\n
1133	&lt;dynamicField name="addr*" type="text_general" indexed="true" stored="true"/&gt;\n
1134	var QParser = new QueryParser(Version.LUCENE_30, "Content", analyzer);\nQParser.AllowLeadingWildcard = true;\nvar Query = new QParser.Parse(searchQuery);\nQuery.Boost = 7.0f;\nreturn Query;\n
1135	public final class StandardAnalyzerV36 extends Analyzer {\n\n    public static final CharArraySet STOP_WORDS_SET = StopAnalyzer.ENGLISH_STOP_WORDS_SET;  \n\n    @Override\n    protected TokenStreamComponents createComponents(String fieldName) {\n\n        final ClassicTokenizer src = new ClassicTokenizer();\n        TokenStream tok = new StandardFilter(src);\n        tok = new StopFilter(new LowerCaseFilter(tok), STOP_WORDS_SET);\n\n        return new TokenStreamComponents(src, tok);\n    }\n\n    @Override\n    protected Reader initReader(String fieldName, Reader reader) {\n        return new HTMLStripCharFilter(reader);\n    }\n}\n\npublic class LuceneUtils {\n\n    public static List&lt;String&gt; tokenizeString(Analyzer analyzer, String string) {\n        List&lt;String&gt; result = new ArrayList&lt;String&gt;();\n        TokenStream stream = null;\n        try {\n            stream = analyzer.tokenStream(null, new StringReader(string));\n            stream.reset();\n            while (stream.incrementToken()) {\n                result.add(stream.getAttribute(CharTermAttribute.class).toString());\n            }\n        } catch (IOException e) {\n            // not thrown b/c we're using a string reader...\n            throw new RuntimeException(e);\n        } finally {\n            IOUtils.closeQuietly(stream);\n        }\n        return result;\n    }\n}\n
1136	curl -XPOST 'http://localhost:9200/_optimize?only_expunge_deletes=true'\n
1137	String categoriesForItem = getCategories();\n\nString [] categoriesForItems = categoriesForItem.split(","); \nfor(String cat : categoriesForItems) {\n    doc.add(new StringField("categories", cat , Field.Store.YES));\n}\n
1138	"original query" + _query_:"{!parent which="scope:product"} COLOR: Blue"\nchild.facet.field = SIZE\n
1139	hibernate.search.default.exclusive_index_use = false\n
1140	        &lt;plugin&gt;\n            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n            &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;\n            &lt;version&gt;2.4.3&lt;/version&gt;\n            &lt;executions&gt;\n                &lt;execution&gt;\n                    &lt;phase&gt;package&lt;/phase&gt;\n                    &lt;goals&gt;\n                        &lt;goal&gt;shade&lt;/goal&gt;\n                    &lt;/goals&gt;\n                    &lt;configuration&gt;\n                        &lt;transformers&gt;\n                            &lt;transformer\n                                    implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"&gt;\n                                &lt;mainClass&gt;main&lt;/mainClass&gt;\n                            &lt;/transformer&gt;\n                            &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/&gt;\n\n                        &lt;/transformers&gt;\n                    &lt;/configuration&gt;\n                &lt;/execution&gt;\n            &lt;/executions&gt;\n        &lt;/plugin&gt;\n
1141	Query query = new BooleanQuery.Builder()\n        .add(new BooleanClause(new MatchAllDocsQuery(), BooleanClause.Occur.MUST))\n        .add(new BooleanClause(myOldQuery, BooleanClause.Occur.SHOULD))\n        .build();\n
1142	POST /table/_search\n{\n  "query": {\n    "bool": {\n      "filter": {\n        "script": {\n          "script": "doc.id.value == doc.cityid.value"\n        }\n      }\n    }\n  }\n}\n
1143	public class SoundexFilter : TokenFilter\n{\n    private readonly ITermAttribute _termAttr;\n\n    public SoundexFilter(TokenStream input)\n        : base(input)\n    {\n        _termAttr = AddAttribute&lt;ITermAttribute&gt;();\n    }\n\n    public override bool IncrementToken()\n    {\n        if (input.IncrementToken())\n        {\n            string currentTerm = _termAttr.Term;\n            // Any phonetic hash calculation will work here.\n            var hash = Soundex.For(currentTerm);\n            _termAttr.SetTermBuffer(hash);\n            return true;\n        }\n\n        return false;\n    }\n}\n
1144	final SentenceSimilarityAssessor s=new SentenceSimilarityAssessor();\ns.getSearchEngineHungarianSentenceSimilarity(s1, s2, SimilarityConstants.GOOGLE, SimilarityConstants.NGD_MEASURE, SimilarityConstants.TURNEY_SCORE_1);\n
1145	BooleanQuery bq = new BooleanQuery();\nBooleanQuery entryBQ = new BooleanQuery();\nBooleanQuery descriptionBQ = new BooleanQuery();\nString[] tokens = String.valueOf(s).split("[^a-zA-Z0-9]");\nfor (String token : tokens) {\n    if (token.isEmpty()) continue;\n    entryBQ.add(new WildcardQuery(new Term("entry", token + "*")), BooleanClause.Occur.MUST);\n    descriptionBQ.add(new WildcardQuery(new Term("description", token + "*")), BooleanClause.Occur.MUST);\n}\nbq.add(entryBQ, BooleanClause.Occur.SHOULD);\nbq.add(descriptionBQ, BooleanClause.Occur.SHOULD);\nTopScoreDocCollector collector = TopScoreDocCollector.create(10, true);\nis.search(bq, collector);\n
1146	Directory directory = FSDirectory.open(FileUtils.toFile(new URL("file:lucene/indexes/")));\nthis.spellChecker = new SpellChecker(directory);\nIndexReader indexReader = IndexReader.open(directory, true);\nLuceneDictionary dictionary = new LuceneDictionary(indexReader, "field");\nIndexWriterConfig indexWriterConfig = new IndexWriterConfig(Version.LUCENE_35, new StandardAnalyzer(Version.LUCENE_35));\nthis.spellChecker.indexDictionary(dictionary, indexWriterConfig, true);\n
1147	&lt;property name="jpaProperties"&gt;\n    &lt;props&gt;\n       &lt;prop key="hibernate.search.default.directory_provider"&gt;\n       uk.company.package.JdbcDirectoryProvider\n     &lt;/prop&gt;\n    &lt;/props&gt;\n&lt;/property&gt;\n
1148	WHERE m.name =~ '(?i)neo'\n
1149	public int IndexEntities&lt;TEntity&gt;(DirectoryInfo indexLocation, IEnumerable&lt;TEntity&gt; entities, Func&lt;TEntity, Document&gt; converter)\n{\n    using (var indexer = new IndexWriterWrapper(indexLocation)) {\n        int indexCount = 0;\n        foreach (TEntity entity in entities) {\n            indexer.Writer.AddDocument(converter(entity));\n            indexCount++;\n        }\n        return indexCount;\n    }\n}\n
1150	private class EnumChildren implements Enumeration&lt;Integer&gt; {\n    int category, current;\n    int[] next, first;\n\n    public EnumChildren(int category) {\n        ChildrenArrays childrenArrays = tr.getChildrenArrays();\n        first = childrenArrays.getYoungestChildArray(); \n        next = childrenArrays.getOlderSiblingArray();\n        current = first[category];\n    }\n\n    public boolean hasChildren() {\n        return (current != TaxonomyReader.INVALID_ORDINAL);\n    }\n\n    @Override\n    public boolean hasMoreElements() {  \n        current = next[current];\n        return (current != TaxonomyReader.INVALID_ORDINAL);\n    }\n\n    @Override\n    public Integer nextElement() {\n        return current;\n    }\n}\n
1151	q=field_one:value_one AND -(field_two:[* TO *] OR field_three:[* TO *])\n
1152	curl -XPUT 'http://127.0.0.1:9200/user/?pretty=1'  -d '\n{\n   "settings" : {\n      "number_of_shards" : 1\n   }\n}\n'\n
1153	class SimpleAnalyzer extends Analyzer {\n\n        protected NormalizeCharMap charConvertMap;\n\n        protected void setCharConvertMap() {\n\n            NormalizeCharMap.Builder builder = new NormalizeCharMap.Builder();\n            builder.add("&amp;","and");\n            charConvertMap = builder.build();\n        }\n\n        public SimpleAnalyzer() {\n            setCharConvertMap();\n        }\n\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n            Tokenizer source = new MusicbrainzTokenizer(Version.LUCENE_35,\n                    reader);\n            TokenStream filter = new LowerCaseFilter(Version.LUCENE_35,source);\n            return new TokenStreamComponents(source, filter);\n        }\n\n        @Override\n        protected Reader initReader(String fieldName,\n                                    Reader reader)\n        {\n            return new MappingCharFilter(charConvertMap, reader);\n        }\n    }\n
1154	curl -X POST "http://localhost:9200/index/document/1" -d '{"id":1,"states_ties":["CA"],"state_abbreviation":"CA","worked_in_states":["CA"],"training_in_states":["CA"]}'\ncurl -X POST "http://localhost:9200/index/document/2" -d '{"id":2,"states_ties":["CA","NY"],"state_abbreviation":"FL","worked_in_states":["NY","CA"],"training_in_states":["NY","CA"]}'\ncurl -X POST "http://localhost:9200/index/document/3" -d '{"id":3,"states_ties":["CA","NY","FL"],"state_abbreviation":"NY","worked_in_states":["NY","CA"],"training_in_states":["NY","FL"]}'\n\ncurl -X GET 'http://localhost:9200/index/_search?per_page=10&amp;pretty' -d '{\n  "query": {\n    "custom_filters_score": {\n      "query": {\n        "terms": {\n          "states_ties": [\n            "CA"\n          ]\n        }\n      },\n      "filters": [\n        {\n          "filter": {\n            "term": {\n              "state_abbreviation": "CA"\n            }\n          },\n          "boost": 1.03\n        },\n        {\n          "filter": {\n            "terms": {\n              "worked_in_states": [\n                "CA"\n              ]\n            }\n          },\n          "boost": 1.02\n        },\n        {\n          "filter": {\n            "terms": {\n              "training_in_states": [\n                "CA"\n              ]\n            }\n          },\n          "boost": 1.01\n        }\n      ],\n      "score_mode": "multiply"\n    }\n  },\n  "sort": [\n    {\n      "_score": "desc"\n    }\n  ]\n}'\n\nresults: id: score\n\n1: 0.75584483\n2: 0.73383\n3: 0.7265643\n
1155	Query query = parser.parse("MyIdField:ID1234");\n
1156	TopDocs hits = searcher.search(lucene_query, pageSize*(page));\n\nScoreDoc[] scoreDocs = hits.scoreDocs;\n\nint j = startIndex;\nint rem = 0;\n\nwhile (j &lt; scoreDocs.length &amp;&amp; (endIndex==0 || j&lt;endIndex)) {\n\n    ScoreDoc sd = scoreDocs[j];\n    Document d = searcher.doc(sd.doc);\n
1157	[...]\nwhile($result=mysql_fetch_assoc($query)){\n    $all_values=explode(',',$result["value"]);\n    //Create a Doc foreach value of $all_values and gave them the same category of the result of the query\n    foreach ($all_values as $value) {\n        //Build a doc\n        $doc = new Zend_Search_Lucene_Document();\n\n        [...]\n\n        //Added res_id just to help grouping the results\n        $doc-&gt;addField(Zend_Search_Lucene_Field::unIndexed('res_id', sanitize($result["id"])));\n\n        $doc-&gt;addField(Zend_Search_Lucene_Field::text('category', sanitize($result["category"])));\n        //Renamed the field to value\n        $doc-&gt;addField(Zend_Search_Lucene_Field::text('value', urlencode($value)));\n\n        [...]\n\n        //Add to doc\n        $index-&gt;addDocument($doc);\n    }\n}\n[...]\n
1158	public class MyCustomFieldType extends FieldType {\n    /**\n     * {@inheritDoc}\n     */\n    @Override\n    protected void init(final IndexSchema schema, final Map&lt;String, String&gt; args) {\n        trueProperties |= TOKENIZED;\n        super.init(schema, args);\n    }\n\n    /**\n     * {@inheritDoc}\n     */\n    @Override\n    public void write(final XMLWriter xmlWriter, final String name, final Fieldable fieldable)\n        throws IOException\n    {\n        xmlWriter.writeStr(name, fieldable.stringValue());\n    }\n\n    /**\n     * {@inheritDoc}\n     */\n    @Override\n    public void write(final TextResponseWriter writer, final String name, final Fieldable fieldable)\n        throws  IOException\n    {\n        writer.writeStr(name, fieldable.stringValue(), true);\n    }\n\n    /**\n     * {@inheritDoc}\n     */\n    @Override\n    public SortField getSortField(final SchemaField field, final boolean reverse) {\n        return getStringSort(field, reverse);\n    }\n\n    /**\n     * {@inheritDoc}\n     */\n    @Override\n    public void setAnalyzer(final Analyzer analyzer) {\n        this.analyzer = analyzer;\n    }\n\n    /**\n     * {@inheritDoc}\n     */\n    @Override\n    public void setQueryAnalyzer(final Analyzer queryAnalyzer) {\n        this.queryAnalyzer = queryAnalyzer;\n    }\n\n    /**\n     * {@inheritDoc}\n     */\n    @Override\n    public Query getFieldQuery(\n        final QParser parser, final SchemaField field, final String externalVal)\n    {\n        // Do some parsing of the user's input (if necessary) from the query string (externalVal)\n        final String parsedInput = ...\n\n        // Instantiate your custom filter, taking note to wrap it in a caching wrapper!\n        final Filter filter = new CachingWrapperFilter(\n            new MyCustomFilter(field, parsedValue));\n\n        // Return a query that runs your filter against all docs in the index\n        // NOTE: depending on your needs, you may be able to do a more fine grained query here\n        // instead of a MatchAllDocsQuery!!\n        return new FilteredQuery(new MatchAllDocsQuery(), filter);\n    }\n}\n
1159	BooleanQuery bq = new BooleanQuery();\n\nforeach (string field in fields)\n{\n    foreach (string tok in tokArr)\n    {\n        bq.Add(new WildcardQuery(new Term(field, " *" + tok + "* ")), BooleanClause.Occur.SHOULD);\n    }\n}\n\nreturn bq;\n
1160	TermEnum terms = reader.Terms(new Term("id"));\nwhile(terms.next()) {\n    Term currentTerm = terms.term();\n    if (!(currentTerm.field().equals())) {\n        break;\n    }\n    String value = currentTerm.text();\n    //Whatever else you need to do with the value\n}\n
1161	var escapedLowerCaseSearchPattern = QueryParser.Escape(searchPattern);\nvar prefixEscapedLowerCaseSearchPattern = string.Concat("\"",\n                                                        escapedLowerCaseSearchPattern,\n                                                        "*\"");\nvar queryParser = new QueryParser(/* my lucene version */,\n                                  fieldName,\n                                  /* a reference to a static instance of my LowerCaseKeywordAnalyzer */);\nvar query = queryParser.Parse(prefixEscapedLowerCaseSearchPattern);\n
1162	public void set(String name, Object value, Document document, LuceneOptions luceneOptions) {\n    if ( value == null ) {\n        throw new IllegalArgumentException( "null cannot be passed to Tika bridge" );\n    }\n    InputStream in = null;\n    try {\n        in = getInputStreamForData( value );\n\n        Metadata metadata = metadataProcessor.prepareMetadata();\n        ParseContext parseContext = parseContextProvider.getParseContext( name, value );\n\n        StringWriter writer = new StringWriter();\n        WriteOutContentHandler contentHandler = new WriteOutContentHandler( writer );\n\n        Parser parser = new AutoDetectParser();\n        parser.parse( in, contentHandler, metadata, parseContext );\n        luceneOptions.addFieldToDocument( name, writer.toString(), document );\n\n        // allow for optional indexing of metadata by the user\n        metadataProcessor.set( name, value, document, luceneOptions, metadata );\n    }\n    catch ( Exception e ) {\n        throw propagate( e );\n    }\n    finally {\n        closeQuietly( in );\n    }\n}\n
1163	q=1234&amp;qf=ngram_text_field,simple_text_field^2&amp;defType=edismax\n
1164	http://localhost:8080/solr/collection1/dataimport?command=full-import&amp;entity=Source1\n
1165	public MetaphoneDistance() {\n    Metaphone metaphone = new Metaphone();\n}\n\n//I'm not really familiar with the library you mentioned, but I assume generateKeys performs a double metaphone?\npublic float getDistance(String str1, ,String str2) {\n    String[] keys1 = metaphone.getKeys(str1);  \n    String[] keys2 = metaphone.getKeys(str2);\n    float result = 0;\n    if (key1[0] == key2[0] || key1[0] == key2[1]) result += .5\n    if (key1[1] == key2[0] || key1[1] == key2[1]) result += .5\n    return result;\n}\n
1166	class MyStemmerAnalyzer extends Analyzer {\n    @override\n    public TokenStream tokenStream(String fieldName, Reader reader) {\n        TokenStream stream = new StandardTokenizer(reader);\n        stream = new StandardFilter(stream);\n        stream = new LowerCaseFilter(stream);\n        stream = new PorterStemFilter(stream);\n        stream = new StopFilter(stream, StopAnalyzer.ENGLISH_STOP_WORDS_SET);\n        return stream;\n    }\n}\n
1167	    public static List&lt;String&gt; tokenizeString(String string) throws IOException {\n        List&lt;String&gt; result = new ArrayList&lt;String&gt;();\n        Tokenizer source = new StandardTokenizer(version, new StringReader(string));\n        TokenStream stream = new StandardFilter(version, source);\n        stream = new GermanNormalizationFilter(stream);\n        CharTermAttribute cattr = stream.addAttribute(CharTermAttribute.class);\n        stream.reset();\n        while (stream.incrementToken()) {\n            result.add(cattr.toString());\n        }\n        return result;\n    }\n
1168	private int DocBase { get; set; }\n\npublic MyScoreProvider(IndexReader reader, IndexSearcher searcher) {\n   DocBase = GetDocBaseForIndexReader(reader, searcher);\n}\n\nprivate static int GetDocBaseForIndexReader(IndexReader reader, IndexSearcher searcher) {\n    // get all segment readers for the searcher\n    IndexReader rootReader = searcher.GetIndexReader();\n    var subReaders = new List&lt;IndexReader&gt;();\n    ReaderUtil.GatherSubReaders(subReaders, rootReader);\n\n    // sequentially loop through the subreaders until we find the specified reader, adjusting our offset along the way\n    int docBase = 0;\n    for (int i = 0; i &lt; subReaders.Count; i++)\n    {\n        if (subReaders[i] == reader)\n            break;\n        docBase += subReaders[i].MaxDoc();\n    }\n\n    return docBase;\n}\n\npublic override float CustomScore(int doc, float subQueryScore, float valSrcScore) {\n   float baseScore = subQueryScore * valSrcScore;\n   float boostedScore = baseScore * MyBoostCache.GetBoostForDocId(doc + DocBase);\n   return boostedScore;\n}\n
1169	Query query = Query.parse(searchQuery, conf);\nQueryParams queryParams = new QueryParams();\nqueryParams.setMaxHitsPerDup(100);\nqueryParams.setNumHits(100);\nquery.setParams(queryParams);\nHits hits = bean.search(query);\nlong allResultsCount =**hits.getTotal());**\n
1170	int docCount = indexReader.numDocs();\nIndexSearcher searcher = new IndexSearcher(indexReader);\n\nQuery queryI = new TermQuery(new Term(fieldName, termI));\nQuery queryJ = new TermQuery(new Term(fieldName, termJ));\n\nQuery queryIJ = new BooleanQuery();\nqueryIJ.add(new BooleanClause(queryI, BooleanClause.Occur.SHOULD));\nqueryIJ.add(new BooleanClause(queryJ, BooleanClause.Occur.SHOULD));\n\nint countI = searcher.search(nqueryI, maxDocs).totalHits;\nint countIJ = searcher.search(, maxDocs).totalHits;\n\nfloat confidence = (float)countIJ / (float)countI;\nfloat support = (float)countIJ / (float)docCount;\n
1171	IndexWriterConfig config = new IndexWriterConfig(Version.LUCENE_34, analyzer);\nconfig.setOpenMode(IndexWriterConfig.OpenMode.CREATE);\nwriter = new IndexWriter(dir, config);\n
1172	&lt;field name="id" type="uuid" indexed="true" stored="true" required="true"/&gt;\n
1173	HashSet&lt;String&gt; impKeywords = new HashSet&lt;String&gt;(new String[] {"Java", "Lucene"});\n
1174	/**\n * \n * \n * @param reader\n * @param fromDateTime\n *            - yyyymmddhhmmss \n * @param toDateTime\n *            - yyyymmddhhmmss \n * @return \n */\nstatic public String top10(IndexSearcher searcher, String fromDateTime,\n        String toDateTime) {\n    String top10Query = "";\n    try {\n        Query query = new TermRangeQuery("tweetDate", new BytesRef(\n                fromDateTime), new BytesRef(toDateTime), true, false);\n        final BitSet bits = new BitSet(searcher.getIndexReader().maxDoc());\n        searcher.search(query, new Collector() {\n\n            private int docBase;\n\n            @Override\n            public void setScorer(Scorer scorer) throws IOException {\n            }\n\n            @Override\n            public void setNextReader(AtomicReaderContext context)\n                    throws IOException {\n                this.docBase = context.docBase;\n            }\n\n            @Override\n            public void collect(int doc) throws IOException {\n                bits.set(doc + docBase);\n            }\n\n            @Override\n            public boolean acceptsDocsOutOfOrder() {\n                return false;\n            }\n        });\n\n        //\n        Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_43,\n                EnglishStopWords.getEnglishStopWords());\n\n        //\n        HashMap&lt;String, Long&gt; wordFrequency = new HashMap&lt;&gt;();\n        for (int wx = 0; wx &lt; bits.length(); ++wx) {\n            if (bits.get(wx)) {\n                Document wd = searcher.doc(wx);\n                //\n                TokenStream tokenStream = analyzer.tokenStream("temp",\n                        new StringReader(wd.get("content")));\n                // OffsetAttribute offsetAttribute = tokenStream\n                // .addAttribute(OffsetAttribute.class);\n                CharTermAttribute charTermAttribute = tokenStream\n                        .addAttribute(CharTermAttribute.class);\n                tokenStream.reset();\n                while (tokenStream.incrementToken()) {\n                    // int startOffset = offsetAttribute.startOffset();\n                    // int endOffset = offsetAttribute.endOffset();\n                    String term = charTermAttribute.toString();\n                    if (term.length() &lt; 2)\n                        continue;\n                    Long wl;\n                    if ((wl = wordFrequency.get(term)) == null)\n                        wordFrequency.put(term, 1L);\n                    else {\n                        wl += 1;\n                        wordFrequency.put(term, wl);\n                    }\n                }\n                tokenStream.end();\n                tokenStream.close();\n            }\n        }\n        analyzer.close();\n\n        // sort\n        List&lt;String&gt; occurterm = new ArrayList&lt;String&gt;();\n        for (String ws : wordFrequency.keySet()) {\n            occurterm.add(String.format("%06d\t%s", wordFrequency.get(ws),\n                    ws));\n        }\n        Collections.sort(occurterm, Collections.reverseOrder());\n\n        // make query string by top 10 words\n        int topCount = 10;\n        for (String ws : occurterm) {\n            if (topCount-- == 0)\n                break;\n            String[] tks = ws.split("\\t");\n            top10Query += tks[1] + " ";\n        }\n        top10Query.trim();\n    } catch (IOException e) {\n        e.printStackTrace();\n    } finally {\n    }\n    // return top10 word string\n    return top10Query;\n}\n
1175	var terms = indexReader.ClosestTerms(field, "dig")\n                       .OrderBy(t =&gt; t.Item2)\n                       .Take(10)\n                       .ToArray();\n
1176	GENERATE=$(JCC) $(foreach jar,$(JARS),--jar $(jar)) \\n           $(JCCFLAGS) --use_full_names \\n           --package java.lang java.lang.System \\n                               java.lang.Runtime \\n           --package java.util java.util.Arrays \\n                               java.util.Collections \\n                               java.util.HashMap \\n                               java.util.HashSet \\n                               java.util.TreeSet \\n                               java.lang.IllegalStateException \\n                               java.lang.IndexOutOfBoundsException \\n                               java.util.NoSuchElementException \\n                     java.text.SimpleDateFormat \\n                     java.text.DecimalFormat \\n                     java.text.Collator \\n           --package java.util.concurrent java.util.concurrent.Executors \\n           --package java.util.regex \\n           --package java.io java.io.StringReader \\n                             java.io.InputStreamReader \\n                             java.io.FileInputStream \\n                             java.io.BufferedReader \\n                             java.io.FileReader \\n           --exclude org.apache.lucene.sandbox.queries.regex.JakartaRegexpCapabilities \\n           --exclude org.apache.regexp.RegexpTunnel \\n           --python lucene \\n           --mapping org.apache.lucene.document.Document 'get:(Ljava/lang/String;)Ljava/lang/String;' \\n           --mapping java.util.Properties 'getProperty:(Ljava/lang/String;)Ljava/lang/String;' \\n           --sequence java.util.AbstractList 'size:()I' 'get:(I)Ljava/lang/Object;' \\n           org.apache.lucene.index.IndexWriter:getReader \\n           --version $(LUCENE_VER) \\n           --module python/collections.py \\n           --module python/ICUNormalizer2Filter.py \\n           --module python/ICUFoldingFilter.py \\n           --module python/ICUTransformFilter.py \\n           $(RESOURCES) \\n           --files $(NUM_FILES)\n
1177	StandardQueryParser parser = new StandardQueryParser(analyzer);\nQuery query = parse.parse("f:\"Forward Markets\"")\nQuery Scorer scorer = new QueryScorer(query);\n//.......\n
1178	      Term firstTerm = new Term("jobTitle", "entry");\n    **  Term secondTerm = firstTerm.createTerm("ar");\n\n      Term[] tTerms = new Term[] { firstTerm, secondTerm };\n      MultiPhraseQuery multiPhrasequery = new MultiPhraseQuery();\n\n      try {\n         File index = new File("I:/com.rhc.rayport.model.JobReq");\n         Directory indexDirectory = FSDirectory.open(index); \n         System.out.println(indexDirectory);\n         PrefixTermEnum reader = new PrefixTermEnum(IndexReader.open(indexDirectory), secondTerm);\n         System.out.println(reader.toString());\n\n         List&lt;Term&gt; termList = new LinkedList&lt;Term&gt;();\n         while (reader.docFreq() != -1) {\n               Term t = reader.term();\n               System.out.println(t);\n            if (t.field().equals("jobTitle") ||  t.text().startsWith(secondTerm.text())) {\n                termList.add(t);\n            }\n           reader.next();              \n         }\n\n         Term[] terms = termList.toArray(new Term[0]);\n         multiPhrasequery.add(firstTerm);\n       **  multiPhrasequery.add(terms);\n
1179	&lt;xsl:apply-templates select="$result-set/result" /&gt;\n\n&lt;xsl:template match="result"&gt;\n&lt;article&gt;\n  &lt;h1&gt;&lt;xsl:value-of select="title" /&gt;&lt;/h1&gt;\n&lt;/article&gt;\n&lt;/xsl:template&gt;\n
1180	SearchFactory searchFactory = fullTextSession.getSearchFactory();\norg.apache.lucene.queryParser.QueryParser parser = new QueryParser(defaultField, searchFactory.getAnalyzer(JobReq.class) );\n/* \n create your BooleanQuery, loop, whatever else\n*/\n    org.apache.lucene.search.Query query = parser.parse( allFields[i].getName().toString() + ":" + QueryParser.escape(termToFind) + "*" );\n    bq.add(new BooleanClause(tempQ, BooleanClause.Occur.SHOULD));\n\nhibQuery = fullTextSession.createFullTextQuery(bq).setSort(sort);\nresults = hibQuery.list();\n
1181	&lt;arr name="parsed_filter_queries"&gt;\n    &lt;str&gt;text:solr100&lt;/str&gt;\n&lt;/arr&gt;\n
1182	PATH=/usr/kerberos/sbin:/usr/kerberos/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin\n\nLANG=en_US.UTF-8\n
1183	Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_43);\nQueryParser parser = new QueryParser(Version.LUCENE_43, "myText", analyzer);\nQuery myQuery = parser.parse("My well-formed query");\n
1184	// create the filter\n// to cache the results, add .cache(true);\nBoolFilterBuilder filterBuilder = FilterBuilders.boolFilter();\n\n// did you mean to skip the first one?\nfor (int i = 0; i &lt; codes_vec.length; ++i)\n    filterBuilder.must(FilterBuilders.termFilter("code", codes_vec[i]));\n\n// add the filter to your search\nSearchResponse response = client.prepareSearch("index1")\n            .setFilter(filterBuilder)\n            .setFrom(0).setSize(10)\n            .execute()\n            .actionGet();\n
1185	String targetFileStr = IOUtils.toString(new FileInputStream(f), "UTF-8");\n
1186	SolrInputDocument updateDocument = new SolrInputDocument();\n//here you must add the id field with the desired value, corresponding to the doc you want to update:\nupdateDocument.addField("id", 2312312);\n//tell it to add the new value to the existing ones, rather then replace them with it:\nupdateDocument.addField("stuffedAnimals", new HashMap(){{put("add","pink fluffy flamingo");}});\n
1187	private final Pattern nonAlpha = Pattern.compile(".*[^a-z].*");\n@Override\nprotected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n    Tokenizer source = new StandardTokenizer(version, reader);\n    TokenStream filter = new StandardFilter(version, source);\n    filter = new LowerCaseFilter(version, filter);\n    filter = new StopFilter(version, filter, StandardAnalyzer.STOP_WORDS_SET);\n    filter = new PatternReplaceCharFilter(nonAlpha, "", filter);\n    return new TokenStreamComponents(source, filter);\n}\n
1188	String startDateString = DateTools.dateToString(startDate, DateTools.Resolution.DAY);\nString endDateString = DateTools.dateToString(endDate, DateTools.Resolution.DAY);\nTermRangeQuery query = TermRangeQuery.newStringRange("recordCreatedTime", startDateString, endDateString, true, false);\nSortField field = new SortField('recordCreatedTime', SortField.Type.STRING);\nSort sort = new Sort(field);\nTopDocs results = searcher.search(query, numDocs, sort);\n
1189	finally {\n\n      if (null != portfolioSolrReq) portfolioSolrReq.close();\n\n      if (null != portfolioIndexCore) {\n        RefCounted&lt;SolrIndexSearcher&gt; searcher = portfolioIndexCore.getNewestSearcher(false);\n        if (searcher.get() != portfolioIndexSearcher) {\n          log.warn("Current Searcher for the Core " + portfolioIndexCore\n              + " has changed. Old Searcher=[" + portfolioIndexSearcher + "], new Searcher=["\n              + searcher.get() + "]");\n          portfolioIndexSearcher.close();\n          portfolioIndexSearcher = null;\n        }else{\n          portfolioIndexSearcher.decref()\n          }\n        searcher.decref();\n        portfolioIndexCore.close();\n      }\n    }\n
1190	package com.mycompany.solr;\n\nimport java.io.IOException;\nimport org.apache.solr.common.SolrInputDocument;\nimport org.apache.solr.request.SolrQueryRequest;\nimport org.apache.solr.response.SolrQueryResponse;\nimport org.apache.solr.update.AddUpdateCommand;\nimport org.apache.solr.update.processor.UpdateRequestProcessor;\nimport org.apache.solr.update.processor.UpdateRequestProcessorFactory;\n\npublic class LastModifiedMergeProcessorFactory \n   extends UpdateRequestProcessorFactory {\n\n  @Override\n  public UpdateRequestProcessor getInstance(SolrQueryRequest req, \n       SolrQueryResponse rsp, UpdateRequestProcessor next) {\n    return new LastModifiedMergeProcessor(next);\n  }\n} \n\nclass LastModifiedMergeProcessor extends UpdateRequestProcessor {\n\n  public LastModifiedMergeProcessor(UpdateRequestProcessor next) {\n    super(next);\n  }\n\n  @Override\n  public void processAdd(AddUpdateCommand cmd) throws IOException {\n    SolrInputDocument doc = cmd.getSolrInputDocument();\n\n    Object metaDate = doc.getFieldValue( "last_modified" );\n    Object fileDate = doc.getFieldValue( "file_date" );\n    if( metaDate == null &amp;&amp; fileDate != null) {\n        doc.addField( "last_modified", fileDate );\n    }\n\n      // pass it up the chain\n      super.processAdd(cmd);\n    }   \n  }\n
1191	&lt;entity name="distinctWeeksVolumeEntity" dataSource="mysqlLocal" rootEntity="false" query="select week as distinctWeeksVolume, count(`distinct` tweetid) as weeklyTweetVolume from twitter.tweetbisect tb group by week"&gt;\n&lt;/entity&gt;\n
1192	     @Override\n     protected boolean lessThan(final Entry hitA, final Entry hitB) {\n\n         assert hitA != hitB;\n         assert hitA.slot != hitB.slot;\n\n         int numComparators = comparators.length;\n         for (int i = 0; i &lt; numComparators; ++i) {\n             final int c = reverseMul[i] * comparators[i].compare(hitA.slot,\n         hitB.slot);\n         if (c != 0) {\n         // Short circuit\n         return c &gt; 0;\n         }\n     }\n\n     // avoid random sort order that could lead to duplicates (bug\n     #31241):\n     return hitA.doc &gt; hitB.doc;\n    }\n
1193	for (termString &lt;- splitted) {\n  pq.add(new Term(IndexProperties.textField, termString))\n}\npq.setSlop(0)\n\nval collector = TopScoreDocCollector.create(5000, true)\nsearcher.search(pq, collector)\n
1194	 229   &lt;!-- The default high-performance update handler --&gt;\n 230   &lt;updateHandler class="solr.DirectUpdateHandler2"&gt;\n 231     &lt;!-- AutoCommit\n 232 \n 233          Perform a hard commit automatically under certain conditions.\n 234          Instead of enabling autoCommit, consider using "commitWithin"\n 235          when adding documents.\n 236 \n 237          http://wiki.apache.org/solr/UpdateXmlMessages\n 238 \n 239          maxDocs - Maximum number of documents to add since the last\n 240                    commit before automatically triggering a new commit.\n 241 \n 242          maxTime - Maximum amount of time in ms that is allowed to pass\n 243                    since a document was added before automaticly\n 244                    triggering a new commit.\n 245          openSearcher - if false, the commit causes recent index changes\n 246          to be flushed to stable storage, but does not cause a new\n 247          searcher to be opened to make those changes visible.\n 248       --&gt;\n 249      &lt;autoCommit&gt;\n 250        &lt;maxTime&gt;15000&lt;/maxTime&gt;\n 251        &lt;openSearcher&gt;false&lt;/openSearcher&gt;\n 252      &lt;/autoCommit&gt;\n 253 \n 254     &lt;!-- softAutoCommit is like autoCommit except it causes a\n 255          'soft' commit which only ensures that changes are visible\n 256          but does not ensure that data is synced to disk.  This is\n 257          faster and more near-realtime friendly than a hard commit.\n 258       --&gt;\n 259 \n 260     &lt;autoSoftCommit&gt;\n 261       &lt;maxTime&gt;1000&lt;/maxTime&gt;\n 262     &lt;/autoSoftCommit&gt;\n
1195	//The form you use for the submission of your search query \n&lt;form action="search.php" method="post"&gt;\n\n//Here are your form elements below are the hidden fields. \n\n&lt;input type="hidden" name="long" value=""/&gt;\n&lt;input type="hidden" name="lat" value=""/&gt;\n\n&lt;/form&gt; \n\n&lt;script type="text/javascript"&gt;\n\n//Getting the location once the page finished loading\n$( document ).ready(\nfunction()\n{\n  if (navigator.geolocation)\n  {\n     navigator.geolocation.getCurrentPosition(showPosition);\n  }\n});\n\n//Location callback. We set the long and lat values of the hidden fields. \nfunction showPosition(position)\n{\n    $('[name="lat"]').val(position.coords.latitude);\n    $('[name="long"]').val(position.coords.longitude);  \n}\n\n&lt;/script&gt;\n
1196	public boolean areEqualsOrderNotImportant(Query q1, Query q2) {\n    if((q1 instanceof BooleanQuery) &amp;&amp; (q2 instanceof BooleanQuery)) {\n        BooleanQuery bq1 = (BooleanQuery)q1;\n        BooleanQuery bq2 = (BooleanQuery)q2;\n        if(bq1.getClauses().length!=bq2.getClauses().length) {\n            return false;\n        }\n        for(BooleanClause clause: bq1.getClauses()) { //multiple occurence of same clause not handled\n            if(!contains(bq2.getClauses(), clause)){\n                return false;\n            }\n        }\n        return true;\n    }else {\n        return q1.equals(q2);\n    }\n}\n
1197	compassSettings = [\n  'compass.engine.optimizer.schedule.period': '300',\n  'compass.engine.mergeFactor':'1000',\n  'compass.engine.maxBufferedDocs':'1000',\n  'compass.engine.ramBufferSize': '128',\n  'compass.engine.useCompoundFile': 'false',\n  'compass.transaction.processor': 'read_committed',\n  'compass.transaction.processor.read_committed.concurrentOperations': 'false',\n  'compass.transaction.lockTimeout': '30',\n  'compass.transaction.lockPollInterval': '500',\n  'compass.transaction.readCommitted.translog.connection': 'ram://'\n]\n
1198	def markupDetails = [:]\n
1199	&lt;entity name="car" query="select id as id_ss, concat('Car ', id) as id from cars"&gt;\n  &lt;field column="id" name="id" template="Car ${car.id}"/&gt;\n  &lt;field column="id_ss" name="id_ss" /&gt;\n
1200	int docNumbersWithTerm = reader.docFreq(new Term(termsEnum.term(), field));\nSystem.out.println(docNumbersWithTerm);\n
1201	&lt;?php\n$path = 'G:\xampp\htdocs\includes\Zend';\nset_include_path(get_include_path() . PATH_SEPARATOR . $path);\n
1202	    public override float CustomScore(int doc, float subQueryScore, float valSrcScore)\n    {\n        float contentScore = subQueryScore;\n\n        double start = 1262307661d; //2010\n\n        if (_dateVsContentModifier == 0)\n        {\n            return base.CustomScore(doc, subQueryScore, valSrcScore);\n        }\n\n        long epoch = (long)(DateTime.Now - new DateTime(1970, 1, 1, 0, 0, 0, DateTimeKind.Utc)).TotalSeconds;\n        long docSinceStartHours = (long)Math.Ceiling((valSrcScore - start) / 3600);\n        long nowSinceStartHours = (long)Math.Ceiling((epoch - start) / 3600);\n\n        float ratio = (float)docSinceStartHours / (float)nowSinceStartHours; // Get a fraction where a document that was created this hour has a value of 1\n        float ageScore = (ratio * _dateVsContentModifier) + 1; // We add 1 because we dont want the bit where we square it bellow to make the value smaller\n\n        float ageScoreAdjustedSoNewerIsBetter = 1;\n\n        if (_newerContentModifier &gt; 0)\n        {\n            // Here we square it, multiuply it and then get the square root. This serves to make newer content have an exponentially higher score than old content instead of it just being linear\n            ageScoreAdjustedSoNewerIsBetter =  (float)Math.Sqrt((ageScore * ageScore) * _newerContentModifier);\n        }\n\n        return ageScoreAdjustedSoNewerIsBetter * contentScore;\n    }\n
1203	MATCH (p:Person) WHERE p.name =~ ".*test.*" RETURN p\n
1204	// inside iterator\nTopDocs docs;\nif (lastScore == null) {\n    docs = searcher.search(query, filter, limit, Sort.INDEXORDER, false, false);\n} else {\n    docs = searcher.searchAfter(lastScore, query, filter, limit, Sort.INDEXORDER, false, false);\n}\nlastScore = docs.scoreDocs[docs.scoreDocs.length - 1];\nfor (ScoreDoc scoreDoc : docs.scoreDocs) {\n    Document = searcher.doc(scoreDoc.doc, fields));\n}\n
1205	Term term = new Term("content", "update");\n\ndocument.removeField("path");\ndocument.add("path", "qqqq");\n\nwriter.updateDocument(term, document);\n
1206	org.hibernate.search.jpa.FullTextEntityManager fem = org.hibernate.search.jpa.Search.getFullTextEntityManager(entityManager);\n\n    Mongo mongo = new Mongo("127.0.0.1", 27017);\n    DB db = mongo.getDB("mainBase");\n    DBCollection dbCollection = db.getCollection("Persons");\n    DBCursor cursor = dbCollection.find();\n    Collection&lt;String&gt; ids = new ArrayList&lt;String&gt;();\n    String id = "";\n    while (cursor.hasNext()) {\n        id = cursor.next().get("_id").toString();\n        System.out.println(id);\n        ids.add(id);\n    }\n\n    System.out.println("&gt;"+ids.size());\n\n    Person pes;\n    for(String p : ids){\n        pes = new Person();\n        pes.setId(p);\n        pes = find(pes);\n        System.out.println("indexing: "+pes.getId());\n        fem.index( pes );//index each element\n        fem.flushToIndexes();//apply changes to indexes\n        fem.clear();//free memory since the queue is processed\n    }\n
1207	try {\n    shiFilter.reset();\n    while( shiFilter.incrementToken() ) {\n        token = cta.toString();\n        System.out.println( token );\n    }\n    shiFilter.end();\n    shiFilter.close();\n} \ncatch ( IOException ioe ) {\n    ioe.printStackTrace();\n}\n
1208	@Override\n  public void init(SolrParams args) {\n    super.init(args);\n    similarity = new PerFieldSimilarityWrapper() {\n      @Override\n      public Similarity get(String name) {\n        FieldType fieldType = core.getLatestSchema().getFieldTypeNoEx(name);\n        if (fieldType == null) {\n          return defaultSimilarity;\n        } else {\n          Similarity similarity = fieldType.getSimilarity();\n          return similarity == null ? defaultSimilarity : similarity;\n        }\n      }\n    };\n  }\n
1209	    if (_termAtt.Term.Contains("/"))\n    {\n        var tempArray = _termAtt.Term.Split('/');\n        foreach (var item in tempArray)\n        {\n            _terms.Enqueue(item.ToCharArray());\n        }\n    }\n    else\n    {\n        _terms.Enqueue(_termAtt.Term.ToCharArray());\n    }\n\n    return true;\n
1210	START user=node:node_auto_index(name="Siddarth")\nMATCH (user)-[:KNOWS]-&gt;(friend)\nRETURN friend\n
1211	FileInputStream fis;\ntry {\n  fis = new FileInputStream(file);\n} catch (FileNotFoundException fnfe) {\n  return;\n}\n\ntry {\n\n  Document doc = new Document();\n\n  String line = null;\n  try (BufferedReader reader = new BufferedReader(new InputStreamReader(fis, StandardCharsets.UTF_8))) {\n    line = reader.readLine();\n    Field headerField = new TextField("header", line, Field.Store.YES);\n    headerField.setBoost(2.0F);\n    doc.add(headerField);\n    while ((line = reader.readLine()) != null ) {\n        doc.add(new TextField("contents", line, Field.Store.YES));\n        }\n    } catch (IOException e) {\n        System.err.println(e);\n    }\n\n} finally {\n  fis.close();\n}\n
1212	fq =&gt; "(product_taxonomy_name:Necklaces AND product_collection_value:Chunky AND product_type_value:Beaded) \n    OR (product_taxonomy_name:Earrings AND product_collection_value:Chunky AND product_type_value:Danglers) \n    OR (...) \n    OR (...)"\n
1213	&lt;fields&gt;\n    &lt;field name="key" type="uuid" indexed="true" required="true"/&gt;\n    &lt;field name="tags" type="string" indexed="true" multiValued="true" required="false"/&gt;\n&lt;/fields&gt;\n
1214	IndexSearcher searcher = new IndexSearcher(directory);\n    IndexReader reader = searcher.getIndexReader();\n    RegexTermEnum regexTermEnum = new RegexTermEnum(reader, new Term(\n            "field", "d.*"), new JavaUtilRegexCapabilities());\n\n    do {\n        System.out.println("Next:");\n        System.out.println("\tDoc Freq: " + regexTermEnum.docFreq());\n        if (regexTermEnum.term() != null) {             \n            System.out.println("\t"+regexTermEnum.term());\n            TermDocs td = reader.termDocs(regexTermEnum.term());\n            while(td.next()){\n                System.out.println("Found "+ td.freq()+" matches in document " + reader.document(td.doc()).get("name"));\n            }\n        }\n    } while (regexTermEnum.next());\n    System.out.println("End.");\n
1215	BooleanQuery fieldTextSubQuery = new BooleanQuery.Builder()\n    .add(new WildcardQuery(new Term(fieldText, str), Occur.SHOULD)\n    .add(new FuzzyQuery(new Term(fieldText, strTemp), fuzzy), Occur.SHOULD)\n    .add(new FuzzyQuery(new Term(fieldText, mergedKeyword), fuzzy), Occur.SHOULD)\n    .build();\n\nBooleanQuery finalQuery = new BooleanQuery.Builder()\n    .add(new FuzzyQuery(new Term(fieldCity, city), 0), Occur.FILTER)\n    .add(textSubQuery, Occur.MUST)\n    .build();\n
1216	http://localhost:8983/solr/core1/select?q=*:*\nhttp://localhost:8983/solr/core2/select?q=*:*\n
1217	    public void doSearch(String querystr) throws IOException, ParseException {      \n\n\n   StandardAnalyzer analyzer = new StandardAnalyzer(Version.LUCENE_36);  \n\n   Directory index = FSDirectory.open(new File(indexDir));  \n\n   // 2. query  \n\n   Query q = new QueryParser(Version.LUCENE_36, LuceneConstants.Term_Vector_Position, analyzer).parse(querystr);  \n\n\n   // 3. search  \n   int hitsPerPage = 10;  \n   IndexSearcher searcher = new IndexSearcher(index, true);  \n   IndexReader reader = IndexReader.open(index, true);  \n   searcher.setDefaultFieldSortScoring(true, true);  \n   TopScoreDocCollector collector = TopScoreDocCollector.create(hitsPerPage, true);  \n   searcher.search(q, collector);  \n   ScoreDoc[] hits = collector.topDocs().scoreDocs;  \n\n   // 4. display term positions, and term indexes   \n   System.out.println("Found " + hits.length + " hits.");  \n //  System.out.println("Found " + hits.clone().length+ " hits.");\n   for(int i=0;i&lt;hits.clone().length;++i) {  \n\n       int docId = hits[i].doc;  \n       System.out.println("docId:" + docId);\n       TermFreqVector tfvector = reader.getTermFreqVector(docId, "TVP");  \n       TermPositionVector tpvector = (TermPositionVector)tfvector; \n       System.out.println("tfvector " + tfvector + " tpvector" + tpvector);\n       int termidx = tfvector.indexOf(querystr);  \n       System.out.println("termidx " + termidx );\n       int[] termposx = tpvector.getTermPositions(termidx);\n       //System.out.println("termposx " + termposx.length);\n       TermVectorOffsetInfo[] tvoffsetinfo = tpvector.getOffsets(termidx);\n\n     for (int j=0;j&lt;termposx.length;j++) {  \n          System.out.println("termpos at j :"+j + ": " +termposx[j]);  \n      } \n\n      System.out.println("tvoffsetinfo " + tvoffsetinfo.length);\n\n      for (int j=0;j&lt;tvoffsetinfo.length;j++) {  \n          int offsetStart = tvoffsetinfo[j].getStartOffset();  \n          int offsetEnd = tvoffsetinfo[j].getEndOffset();  \n          System.out.println("offsets : "+offsetStart+" "+offsetEnd);  \n      }  \n\n      Document d = searcher.doc(docId);  \n      System.out.println((i + 1) + ". " + d.get("filepath"));\n\n   }\n\n    // searcher can only be closed when there  \n   // is no need to access the do***ents any more.   \n   searcher.close();  \n
1218	qf=name address city\n
1219	IndexReader indexReaderUpdate = DirectoryReader.open(indexDir);\n\n// Use indexReaderUpdate here, instead of indexReader\nIndexSearcher searcherUpdate = new IndexSearcher(indexReader);\n\n// Use searcherUpdate here, instead of searcher\nTopDocs docsUpdate = searcher.search(new TermQuery(new Term("int1", "hello")), 10);\n
1220	&lt;fieldType name="text_general" class="solr.TextField" positionIncrementGap="100"&gt;\n  &lt;analyzer class="org.apache.lucene.analysis.standard.StandardAnalyzer"/&gt; \n&lt;/fieldtype&gt;\n
1221	for(ScoreDoc ScoreDoc: scoreDocs){\nTermFreqVector[] termsV = reader.getTermFreqVectors(ScoreDoc.doc);\n            int termFreq = 0;\n            for (int xy = 0; xy &lt; termsV.length; xy++) {  \n                String[] terms = termsV[xy].getTerms(); \n                int[] termFreqs = termsV[xy].getTermFrequencies();\n                int termcount = 0;\n                int count=0;\n                for(String str : terms){\n                    if(str.equalsIgnoreCase(queryString)){\n                        termcount = count;\n                    }\n                    count++;\n                }\n                termFreq = termFreqs[termcount];\n            }\n}\n
1222	import java.io.IOException;\nimport java.io.Reader;\n\nimport org.apache.lucene.analysis.Analyzer;\nimport org.apache.lucene.analysis.Tokenizer;\nimport org.apache.lucene.analysis.core.KeywordTokenizer;\n\n\n  ...\n  Analyzer a = new Analyzer() {\n    @Override\n    public TokenStreamComponents createComponents(\n               String fieldName, Reader reader) {\n      Tokenizer tokenizer = new KeywordTokenizer(reader);\n      return new TokenStreamComponents(tokenizer, \n                 new IndonesianStemFilter(tokenizer));\n    }\n  };\n
1223	&lt;field name="message" type="text_en" indexed="true" stored="true" \nmultiValued="true"/&gt;\n\n&lt;fieldType name="text_en" class="solr.TextField" positionIncrementGap="100"&gt;\n  &lt;analyzer type="index"&gt;\n    &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;\n    &lt;filter class="solr.LowerCaseFilterFactory"/&gt;\n    &lt;filter class="solr.EnglishPossessiveFilterFactory"/&gt;\n    &lt;filter class="solr.KeywordMarkerFilterFactory" \n            protected="protwords.txt"/&gt;\n    &lt;filter class="solr.PorterStemFilterFactory"/&gt;\n    &lt;filter class="solr.StopFilterFactory" ignoreCase="true" \n            words="lang/stopwords_en.txt"/&gt;\n  &lt;/analyzer&gt;\n  &lt;analyzer type="query"&gt;\n    &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;\n    &lt;filter class="solr.SynonymFilterFactory" synonyms="synonyms.txt"       \n            ignoreCase="true" expand="true"/&gt;\n    &lt;filter class="solr.LowerCaseFilterFactory"/&gt;\n    &lt;filter class="solr.EnglishPossessiveFilterFactory"/&gt;\n    &lt;filter class="solr.KeywordMarkerFilterFactory" \n            protected="protwords.txt"/&gt;\n    &lt;filter class="solr.PorterStemFilterFactory"/&gt;\n    &lt;filter class="solr.StopFilterFactory" ignoreCase="true"   \n            words="lang/stopwords_en.txt"/&gt;\n  &lt;/analyzer&gt;\n&lt;/fieldType&gt;\n
1224	*t* =&gt; (D1,1) (D1,3) (D2, 2) (D2, 5)\n
1225	 &lt;fieldType name="text_color" class="solr.TextField" positionIncrementGap="100"&gt;\n   &lt;analyzer&gt;\n     &lt;tokenizer class="solr.PatternTokenizerFactory" pattern="," /&gt;\n   &lt;/analyzer&gt;\n &lt;/fieldType&gt;\n\n&lt;field name="color" type="text_color" indexed="true" stored="true" /&gt;\n
1226	&lt;!-- hibernate search configuration --&gt;\n            &lt;!-- index root folder location --&gt;\n            &lt;prop key="hibernate.search.default.indexBase"&gt;${env.ftindex.location.root}/${datasource.database}/${ftindex.folder}&lt;/prop&gt;\n            &lt;!-- global analyzer --&gt;\n            &lt;prop key="hibernate.search.analyzer"&gt;custom_analyzer&lt;/prop&gt;\n            &lt;!-- asynchronous indexing for performance considerations --&gt;\n            &lt;prop key="org.hibernate.worker.execution"&gt;async&lt;/prop&gt;\n            &lt;!-- max number of indexing operation to be processed asynchronously (before session flush) to avoid OutOfMemoryException --&gt;\n            &lt;prop key="hibernate.search.worker.buffer_queue.max"&gt;100&lt;/prop&gt;\n
1227	final Query query = new QueryParser(Version.LUCENE_33, "keywords", new StandardAnalyzer(Version.LUCENE_33)).parse("+cats AND +movies");\n
1228	hit.Fields.ReadAll();\nforeach (Field field in hit.Fields)\n{\n    hitParagraph = hitParagraph + field.Key + ": " + item[field.Key] + "\n";\n}\n
1229	/select?q=fieldvalue:(a OR b OR c OR d)\n
1230	&lt;analyzerFilter name="lower" type="EdgeNGramTokenFilterProvider"/&gt;\n
1231	&lt;class name="com.package.ClassA" alias="classA"&gt;\n     &lt;component name="bs" ref-alias="classB" /&gt;\n&lt;/class&gt;\n&lt;class name="com.package.ClassB" alias="classB"&gt;\n&lt;/class&gt;\n
1232	&lt;comp:searchEngine useCompoundFile="false" cacheInvalidationInterval="-1"&gt;\n        &lt;comp:allProperty enable="false" /&gt;\n        &lt;!--\n            By Default, compass uses StandardAnalyzer for indexing and searching. StandardAnalyzer\n            will use certain stop words (stop words are not indexed and hence not searcheable) which are\n            valid search terms in the DataSource World. For e.g. 'in' for Indiana state, 'or' for Oregon etc.\n            So we need to provide our own Analyzer.\n        --&gt;\n        &lt;comp:analyzer name="default" type="CustomAnalyzer"\n            analyzerClass="com.ICStandardAnalyzer" /&gt;\n        &lt;comp:analyzer name="search" type="CustomAnalyzer"\n            analyzerClass="com.ICStandardAnalyzer" /&gt;\n        &lt;!--\n            Disable the optimizer as we will optimize the index as a separate batch job\n\n            Also, the merge factor is set to 1000, so that merging doesnt happen during the commit time.\n            Merging is a time consuming process and will be done by the batched optimizer\n        --&gt;\n        &lt;comp:optimizer schedule="false" mergeFactor="1000"/&gt;\n    &lt;/comp:searchEngine&gt;\n
1233	public static IEnumerable&lt;T&gt; Randomize&lt;T&gt;(this IEnumerable&lt;T&gt; source)\n{\n   Random rnd = new Random();\n   return source.OrderBy&lt;T, int&gt;((item) =&gt; rnd.Next());\n}\n
1234	int Id = hits.id(c);\nTermFreqVector TermFreqVector = IndexReader.GetTermFreqVector(Id, "content");\n// etc.\n
1235	            DefaultCompassQuery query = (DefaultCompassQuery) compassBuilderQuery.toQuery();\n            query.setTypes( types.toArray( new Class[types.size()] ) );\n            LuceneSearchEngineQuery searchEngineQuery = (LuceneSearchEngineQuery) query.getSearchEngineQuery();\n            final SimpleDateFormat sdf = new SimpleDateFormat( "yyyyMMddHHmmss" );\n            final long timeInMillis = Calendar.getInstance().getTimeInMillis();\n            ValueSourceQuery valSrcQuery = new ValueSourceQuery( new ValueSource() {\n\n                private static final long serialVersionUID = 1L;\n\n                @Override\n                public int hashCode() {\n                    return System.identityHashCode( this );\n                }\n\n                @Override\n                public DocValues getValues( final IndexReader reader ) throws IOException {\n                    return new DocValues() {\n\n                        @Override\n                        public float floatVal( int doc ) {\n                            try {\n                                Document document = reader.document( doc );\n                                Field field = document.getField( "date" );\n                                if (null != field) {\n                                    Date parse = sdf.parse( field.stringValue() );\n\n                                    long t = timeInMillis - parse.getTime();\n\n                                    float f = (1.0f / (t * (1.0f / TimeUnit.DAYS.toMillis( 30 )) + 1.0f));\n                                    if (logger.isDebugEnabled()) {\n                                        logger.debug( "Date match: " + parse.toString() );\n                                        logger.debug( "Calculated date boost as: " + f + " for doc id: " + doc );\n                                    }\n                                    return f;\n                                }\n                            } catch (CorruptIndexException e) {\n                                e.printStackTrace();\n                            } catch (IOException e) {\n                                e.printStackTrace();\n                            } catch (ParseException e) {\n                                e.printStackTrace();\n                            }\n                            return 1.0f;\n                        }\n\n                        @Override\n                        public String toString( int doc ) {\n                            return description() + "=" + strVal( doc );\n                        }\n\n                    };\n                }\n\n                @Override\n                public boolean equals( Object o ) {\n                    return this == o;\n                }\n\n                @Override\n                public String description() {\n                    return "[boost: date]";\n                }\n            } );\n            CustomScoreQuery sq = new CustomScoreQuery( searchEngineQuery.getQuery(), valSrcQuery );\n            searchEngineQuery.setQuery( sq );\n
1236	public class CustomMappingManager : IReadOnlyMappingManager\n{\n\npublic ICollection&lt;SolrFieldModel&gt; GetFields(Type type)\n    {\n        IEnumerable&lt;KeyValuePair&lt;PropertyInfo, IndexFieldAttribute[]&gt;&gt; mappedProperties = this.GetPropertiesWithAttribute&lt;IndexFieldAttribute&gt;(type);\n\n        IEnumerable&lt;SolrFieldModel&gt; fields = from mapping in mappedProperties\n                                             select new SolrFieldModel()\n                                             {\n                                                 Property = mapping.Key,\n                                                 FieldName = mapping.Value[0].FieldName ?? mapping.Key.Name\n                                             };\n\n        return new List&lt;SolrFieldModel&gt;(fields);\n    }\n\npublic SolrFieldModel GetUniqueKey(Type type)\n    {\n        SolrFieldModel uniqueKey;\n\n        IEnumerable&lt;KeyValuePair&lt;PropertyInfo, IndexUniqueKeyAttribute[]&gt;&gt; mappedProperties = this.GetPropertiesWithAttribute&lt;IndexUniqueKeyAttribute&gt;(type);\n\n        IEnumerable&lt;SolrFieldModel&gt; fields = from mapping in mappedProperties\n                                             select new SolrFieldModel()\n                                             {\n                                                 Property = mapping.Key,\n                                                 FieldName = mapping.Value[0].FieldName ?? mapping.Key.Name\n                                             };\n\n        uniqueKey = fields.FirstOrDefault();\n\n        if (uniqueKey == null)\n        {\n            throw new Exception("Index document has no unique key attribute");\n        }\n\n        return uniqueKey;\n    }\n}\n
1237	QueryParser qp = ...\nFilter filter = new QueryWrapperFilter(qp.parse("field2:abc"));\n// pass filter to searcher.search()\n
1238	&lt;fieldType name="text" class="solr.TextField" positionIncrementGap="100"&gt;\n      &lt;analyzer type="index"&gt;\n        &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;\n        &lt;!-- in this example, we will only use synonyms at query time\n        &lt;filter class="solr.SynonymFilterFactory" synonyms="index_synonyms.txt" ignoreCase="true" expand="false"/&gt;\n        --&gt;\n        &lt;!-- Case insensitive stop word removal.\n          add enablePositionIncrements=true in both the index and query\n          analyzers to leave a 'gap' for more accurate phrase queries.\n        --&gt;\n
1239	facet' =&gt; 'true'\n'facet.field' =&gt; 'PRODUCT'\n'facet.method' =&gt; 'enum'\n'facet.limit'=&gt;200\n'facet.mincount'=&gt;4\n'fq' =&gt; 'PRODUCT:(computer OR tv OR mp3-player)'\n
1240	&lt;add&gt;\n&lt;doc&gt;\n  &lt;field name="Name"&gt;John Smith&lt;/field&gt;\n  &lt;field name="Product"&gt;Chicken Sandwich&lt;/field&gt;\n  &lt;field name="Price"&gt;10&lt;/field&gt;\n&lt;/doc&gt;\n&lt;doc&gt;\n  &lt;field name="Name"&gt;John Smith&lt;/field&gt;\n  &lt;field name="Product"&gt;Dodge Challenger&lt;/field&gt;\n  &lt;field name="Price"&gt;35000&lt;/field&gt;\n&lt;/doc&gt;\n&lt;doc&gt;\n  &lt;field name="Name"&gt;John Q. Public&lt;/field&gt;\n
1241	TokenStream stream = new StandardTokenizer(Version.LUCENE_32, reader);\n
1242	var url = new java.net.URL("http://***********/GenerateThumbnail?url=http://money.cnn.com/2011/07/20/news/economy/debt_ceiling_deal/index.htm?cnn=yes"); \nvar connection = url.openConnection();\nconnection.connect();\nconnection.getContent();\nreturn row;\n
1243	&lt;field name="id" type="string" indexed="true" stored="true" required="true" /&gt; \n&lt;field name="title" type="text" indexed="true" stored="true" multiValued="true"/&gt;\n&lt;field name="text" type="text" indexed="true" stored="true" multiValued="true"/&gt;\n&lt;dynamicField name="*_i"  type="int"    indexed="true"  stored="true"/&gt;\n&lt;dynamicField name="*_ms"  type="string"  indexed="true"  stored="true" multiValued="true"/&gt;\n&lt;dynamicField name="*_dt" type="date"    indexed="true"  stored="true"/&gt;\n
1244	StandardAnalyzer standardAnalyzer = new StandardAnalyzer(new string[]{"an", "a", /*other stopwords*/});\n
1245	Directory index = FSDirectory.open(new File("/path/to/index")); \nIndexSearcher searcher = new IndexSearcher(index, true);\nTopScoreDocCollector collector = TopScoreDocCollector.create(10, true);\nsearcher.search(q, collector);\nScoreDoc[] hits = collector.topDocs().scoreDocs;\n
1246	$query-&gt;addFilterQuery("{!join from=id to=med_id }{!type=dismax qf='$qf' mm='1' q.alt='*:*'}".$qs);\n$query-&gt;setQuery("*:*");\n
1247	        tempPreviousToken.attSource.copyTo(this);\n        previousTokenFlag = false;\n        this.incrementToken();\n        return false;\n
1248	Query queryTerm = new TermQuery(new Term("word", searchedWord));\nQuery query1 = new TermQuery(new Term("category", NumericUtils.IntToPrefixCoded(0)));\nQuery query2 = new TermQuery(new Term("category", NumericUtils.IntToPrefixCoded(1));\n\nBooleanQuery innerOrQuery = new BooleanQuery();\ninnerOrQuery.Add(query1, BooleanClause.Occur.SHOULD);\ninnerOrQuery.Add(query2, BooleanClause.Occur.SHOULD);\n\nBooleanQuery mainQuery = new BooleanQuery();\nmainQuery.Add(queryTerm, BooleanClause.Occur.MUST);\nmainQuery.Add(innerOrQuery, BooleanClause.Occur.MUST);\n\nTopDocs topDocs = _indexSearcher.Search(mainQuery, Settings.Current.MaximumTopScoreCount);\n
1249	&lt;html&gt;&lt;body&gt;&lt;form&gt;\n  &lt;select&gt;\n    &lt;option value="1"&gt;&lt;li&gt;&lt;/option&gt;\n  &lt;/select&gt;\n&lt;/form&gt;&lt;/body&gt;&lt;/html&gt;\n
1250	  SpanNearQuery spanNearQuery = new SpanNearQuery(new[] { new SpanTermQuery(new Term(field, "A")), new SpanTermQuery(new Term(field,"B")) },0, true);\n  SpanFirstQuery spanFirstQuery = new SpanFirstQuery(spanNearQuery, 2);\n
1251	byte[] normsDocLengthArrField1 = indexReader.norms("filed1");\nbyte[] normsDocLengthArrField2 = indexReader.norms("filed2");\nbyte[] normsDocLengthArrField3 = indexReader.norms("filed3");\n\n double sumLength = 0;\n        for (int i = 0; i &lt; normsDocLengthArrField1.length; i++) {\n            double encodeLengthFOne = DefaultSimilarity.decodeNorm(normsDocLengthArrField1[i]);\n            double encodeLengthFTwo = DefaultSimilarity.decodeNorm(normsDocLengthArrField2[i]);\n            double encodeLengthFThree = DefaultSimilarity.decodeNorm(normsDocLengthArrField3[i]);\n\n        //decodeNorm -Decodes a normalization factor stored in an index.\n        double lengthFieldOne = 1 / (encodeLengthFOne * encodeLengthFOne);\n        double lengthFieldTwo = 1 / (encodeLengthFTwo * encodeLengthFTwo);\n        double lengthFieldThree = 1 / (encodeLengthFThree * encodeLengthFThree);\n        sumLength += lengthFieldOne + lengthFieldTwo + lengthFieldThree;\n\n    }\n    this.avgDocLength = sumLength / (normsDocLengthArrField1.length);\n
1252	for (int i = 0; i &lt; searcher.MaxDoc(); i++)\n{\n    string searchExplanation = searcher.Explain(k, i).ToString();\n    int strtIdx = searchExplanation.IndexOf("field=");\n    string[] fieldName;\n    if (strtIdx != -1)\n    {\n        fieldName = searchExplanation.Substring(strtIdx).Split(',');\n        for (int j = 0; j &lt; fieldName.GetLength(0) - 1; j++)\n        {\n            if (fieldNames.IndexOf(fieldName[j].Substring(6)) == -1)\n            {\n                 fieldNames.Add(fieldName[j].Substring(6));\n            }\n        }\n    }\n}\n
1253	  // mysql expert\n  Field mysqlf = new Field("skill", "mysql", \n                                    Field.Store.YES, \n                                    Field.Index.ANALYZED); \n  mysqlf.setBoost(10.0F); \n  // mysql begginer\n  mysqlf = new Field("skill", "mysql", \n                                    Field.Store.YES, \n                                    Field.Index.ANALYZED); \n  mysqlf.setBoost(1.0F); \n
1254	private void updateIndex2DataForId(String id) throws ParseException, IOException {\n    // Get all terms containing the node id\n    TermDocs termDocs = index1Reader.termDocs(new Term("id", id));\n    // Iterate\n    Document doc = new Document();\n    doc.add(new Field("id", id, Store.YES, Index.NOT_ANALYZED));\n    int docId = -1;\n    StringBuffer buffer = new StringBuffer();\n    while (termDocs.next()) {\n        docId = termDocs.doc();\n        buffer .append(keys[docId] + " "); // keys[] is pre-populated using FieldCache                 \n    }\n    doc.add(new Field("id", buffer.trim().toString(), Store.YES, Index.ANALYZED));   \n    index2Writer.addDocument(doc);\n}\n\nString[] keys = FieldCache.DEFAULT.getStrings(index1Reader, "keywords");\n
1255	NumericField num = (NumericField) theFieldable;\n